---
keywords: fastai
description: a flexible package to combine tabular data with text and images using wide and deep models.
title: "pytorch-widedeep, deep learning for tabular data II: advanced use"
author: Javier Rodriguez
toc: true 
badges: true
comments: true
nb_path: _notebooks/2020-12-11-pytorch-widedeep_ii.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-11-pytorch-widedeep_ii.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the second of a series of posts introducing <a href="https://github.com/jrzaurin/pytorch-widedeep">pytorch-widedeep</a>, a flexible package to combine tabular data with text and images (that could also be used for "standard" tabular data alone).</p>
<p>In the first post I described <code>pytorch-widedeep</code>'s data preprocessing utilities, the main components of a <code>WideDeep</code> model and a quick example to illustrate the basic use of the library. In this post I will use a series of examples to dig deeper into the many options <code>pytorch-widedeep</code> offers as we build wide and deep models.</p>
<h2 id="1.-Binary-classification-with-varying-parameters">1. Binary classification with varying parameters<a class="anchor-link" href="#1.-Binary-classification-with-varying-parameters"> </a></h2><p>Let's start by using again the <a href="http://archive.ics.uci.edu/ml/datasets/Adult">adult census</a> dataset.</p>
<p>Before moving any further, let me emphasize that, as we go through the examples, one should not pay excessive (or any) attention to the loss or the metrics in the sense that the input parameters are not selected to obtain "state of the art", but to illustrate usability.</p>
<p>A proper benchmarking exercise will be carried out in a future post. Having said that, and without further ado, let's start.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">adult</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/adult/adult.csv.zip&quot;</span><span class="p">)</span>
<span class="n">adult</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">adult</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">adult</span><span class="p">[</span><span class="s2">&quot;income_label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">adult</span><span class="p">[</span><span class="s2">&quot;income&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;&gt;50K&quot;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">adult</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;income&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">adult</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">adult</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s1">&#39;O&#39;</span><span class="p">:</span>
        <span class="n">adult</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">adult</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;unknown&quot;</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s2">&quot;?&quot;</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">adult</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">adult</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">adult</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>fnlwgt</th>
      <th>education</th>
      <th>educational_num</th>
      <th>marital_status</th>
      <th>occupation</th>
      <th>relationship</th>
      <th>race</th>
      <th>gender</th>
      <th>capital_gain</th>
      <th>capital_loss</th>
      <th>hours_per_week</th>
      <th>native_country</th>
      <th>income_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25</td>
      <td>private</td>
      <td>226802</td>
      <td>11th</td>
      <td>7</td>
      <td>never-married</td>
      <td>machine-op-inspct</td>
      <td>own-child</td>
      <td>black</td>
      <td>male</td>
      <td>0</td>
      <td>0</td>
      <td>40</td>
      <td>united-states</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>38</td>
      <td>private</td>
      <td>89814</td>
      <td>hs-grad</td>
      <td>9</td>
      <td>married-civ-spouse</td>
      <td>farming-fishing</td>
      <td>husband</td>
      <td>white</td>
      <td>male</td>
      <td>0</td>
      <td>0</td>
      <td>50</td>
      <td>united-states</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>28</td>
      <td>local-gov</td>
      <td>336951</td>
      <td>assoc-acdm</td>
      <td>12</td>
      <td>married-civ-spouse</td>
      <td>protective-serv</td>
      <td>husband</td>
      <td>white</td>
      <td>male</td>
      <td>0</td>
      <td>0</td>
      <td>40</td>
      <td>united-states</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>44</td>
      <td>private</td>
      <td>160323</td>
      <td>some-college</td>
      <td>10</td>
      <td>married-civ-spouse</td>
      <td>machine-op-inspct</td>
      <td>husband</td>
      <td>black</td>
      <td>male</td>
      <td>7688</td>
      <td>0</td>
      <td>40</td>
      <td>united-states</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>18</td>
      <td>unknown</td>
      <td>103497</td>
      <td>some-college</td>
      <td>10</td>
      <td>never-married</td>
      <td>unknown</td>
      <td>own-child</td>
      <td>white</td>
      <td>female</td>
      <td>0</td>
      <td>0</td>
      <td>30</td>
      <td>united-states</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>if you read the first post you will be familiar with the code below:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">pytorch_widedeep</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">WidePreprocessor</span><span class="p">,</span> <span class="n">TabPreprocessor</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Wide</span><span class="p">,</span> <span class="n">TabMlp</span><span class="p">,</span> <span class="n">TabResnet</span><span class="p">,</span> <span class="n">WideDeep</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.metrics</span> <span class="kn">import</span> <span class="n">Accuracy</span><span class="p">,</span> <span class="n">Recall</span>

<span class="n">wide_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;education&#39;</span><span class="p">,</span> <span class="s1">&#39;relationship&#39;</span><span class="p">,</span><span class="s1">&#39;workclass&#39;</span><span class="p">,</span><span class="s1">&#39;occupation&#39;</span><span class="p">,</span><span class="s1">&#39;native_country&#39;</span><span class="p">,</span><span class="s1">&#39;gender&#39;</span><span class="p">]</span>
<span class="n">crossed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;education&#39;</span><span class="p">,</span> <span class="s1">&#39;occupation&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;native_country&#39;</span><span class="p">,</span> <span class="s1">&#39;occupation&#39;</span><span class="p">)]</span>
<span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;education&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;relationship&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;workclass&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;occupation&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),(</span><span class="s1">&#39;native_country&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">)]</span>
<span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">,</span><span class="s2">&quot;hours_per_week&quot;</span><span class="p">]</span>
<span class="n">target_col</span> <span class="o">=</span> <span class="s1">&#39;income_label&#39;</span>

<span class="c1"># TARGET</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">adult</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># WIDE</span>
<span class="n">wide_preprocessor</span> <span class="o">=</span> <span class="n">WidePreprocessor</span><span class="p">(</span><span class="n">wide_cols</span><span class="o">=</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="n">crossed_cols</span><span class="p">)</span>
<span class="n">X_wide</span> <span class="o">=</span> <span class="n">wide_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">adult</span><span class="p">)</span>

<span class="c1"># DEEP</span>
<span class="n">tab_preprocessor</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span><span class="n">embed_cols</span><span class="o">=</span><span class="n">cat_embed_cols</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="n">X_tab</span> <span class="o">=</span> <span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">adult</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">wide_dim</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X_wide</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># We can add dropout and batchnorm to the dense layers, as well as chose the order of the operations</span>
<span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabMlp</span><span class="p">(</span><span class="n">column_idx</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">column_idx</span><span class="p">,</span>
                   <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">32</span><span class="p">],</span> 
                   <span class="n">mlp_dropout</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> 
                   <span class="n">mlp_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                   <span class="n">mlp_linear_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                   <span class="n">embed_input</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">embeddings_input</span><span class="p">,</span>
                   <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's have a look to the model that we will be running:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>WideDeep(
  (wide): Wide(
    (wide_linear): Embedding(797, 1, padding_idx=0)
  )
  (deeptabular): Sequential(
    (0): TabMlp(
      (embed_layers): ModuleDict(
        (emb_layer_education): Embedding(17, 32, padding_idx=0)
        (emb_layer_native_country): Embedding(43, 32, padding_idx=0)
        (emb_layer_occupation): Embedding(16, 32, padding_idx=0)
        (emb_layer_relationship): Embedding(7, 32, padding_idx=0)
        (emb_layer_workclass): Embedding(10, 32, padding_idx=0)
      )
      (embedding_dropout): Dropout(p=0.1, inplace=False)
      (tab_mlp): MLP(
        (mlp): Sequential(
          (dense_layer_0): Sequential(
            (0): Linear(in_features=162, out_features=64, bias=False)
            (1): ReLU(inplace=True)
            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): Dropout(p=0.5, inplace=False)
          )
          (dense_layer_1): Sequential(
            (0): Linear(in_features=64, out_features=32, bias=True)
            (1): ReLU(inplace=True)
            (2): Dropout(p=0.5, inplace=False)
          )
        )
      )
    )
    (1): Linear(in_features=32, out_features=1, bias=True)
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we will define the set up for each model component, including optimizers, learning rate schedulers and initializers:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_widedeep.initializers</span> <span class="kn">import</span> <span class="n">KaimingNormal</span><span class="p">,</span> <span class="n">XavierNormal</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">LRHistory</span><span class="p">,</span> <span class="n">EarlyStopping</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.optim</span> <span class="kn">import</span> <span class="n">RAdam</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Optimizers</span>
<span class="n">wide_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wide</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>
<span class="n">deep_opt</span> <span class="o">=</span> <span class="n">RAdam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">deeptabular</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># LR Schedulers</span>
<span class="n">wide_sch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">wide_opt</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">deep_sch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">deep_opt</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Component-dependent settings as Dict</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;wide&#39;</span><span class="p">:</span> <span class="n">wide_opt</span><span class="p">,</span> <span class="s1">&#39;deeptabular&#39;</span><span class="p">:</span><span class="n">deep_opt</span><span class="p">}</span>
<span class="n">schedulers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;wide&#39;</span><span class="p">:</span> <span class="n">wide_sch</span><span class="p">,</span> <span class="s1">&#39;deeptabular&#39;</span><span class="p">:</span><span class="n">deep_sch</span><span class="p">}</span>
<span class="n">initializers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;wide&#39;</span><span class="p">:</span> <span class="n">KaimingNormal</span><span class="p">,</span> <span class="s1">&#39;deeptabular&#39;</span><span class="p">:</span><span class="n">XavierNormal</span><span class="p">}</span>

<span class="c1"># General settings as List</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">LRHistory</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">&#39;model_weights/wd_out&#39;</span><span class="p">)]</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">Accuracy</span><span class="p">,</span> <span class="n">Recall</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Build the trainer and fit!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
                  <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> 
                  <span class="n">optimizers</span><span class="o">=</span><span class="n">optimizers</span><span class="p">,</span> 
                  <span class="n">lr_schedulers</span><span class="o">=</span><span class="n">schedulers</span><span class="p">,</span>
                  <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
                  <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span> 
                  <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wide</span><span class="o">=</span><span class="n">X_wide</span><span class="p">,</span> <span class="n">X_tab</span><span class="o">=</span><span class="n">X_tab</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">val_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/javier/.pyenv/versions/3.7.9/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
  return f(*args, **kwds)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;n epochs&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;train_acc&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_acc&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;n epochs&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_history</span><span class="p">[</span><span class="s1">&#39;lr_wide_0&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;wide&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">lr_history</span><span class="p">[</span><span class="s1">&#39;lr_deeptabular_0&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;deeptabular&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;n epochs&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;learning rate&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Text(0, 0.5, &#39;learning rate&#39;)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see from the plots, the learning rate effectively decreases by a factor of 0.1 (the default) after the corresponding <code>step_size</code> for each component. Note that the keys in the <code>model.lr_history</code> dictionary have a suffix <code>_0</code>. This is because if you pass different parameter groups to the torch optimizers, these will also be recorded. We'll see this in the regression example later in the post.</p>
<p>Before I move to the next section let me just mention that the <code>WideDeep</code> class comes with a useful method to "rescue" the learned embeddings, very creatively called <code>get_embeddings</code>. For example, let's say I want to use the embeddings learned for the different levels of the categorical feature <code>education</code>. These can be access via:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">education_embed</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">get_embeddings</span><span class="p">(</span>
    <span class="n">col_name</span><span class="o">=</span><span class="s1">&#39;education&#39;</span><span class="p">,</span> 
    <span class="n">cat_encoding_dict</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">encoding_dict</span>
<span class="p">)</span>
<span class="n">education_embed</span><span class="p">[</span><span class="s1">&#39;doctorate&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([ 0.41479743,  0.08521606,  0.2710749 , -0.17924106, -0.07241581,
       -0.2514616 , -0.24809864, -0.20624267, -0.12701468, -0.00737057,
       -0.17397854,  0.03000254, -0.06039784,  0.28008303, -0.35625017,
        0.00706905,  0.18486224, -0.05701892, -0.05574326, -0.08269893,
       -0.15482767,  0.30681178, -0.23743518,  0.08368678,  0.20123835,
        0.30058601, -0.15073103, -0.08352864,  0.07049613, -0.28594372,
       -0.05307232, -0.17094977], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Using-the-Focal-Loss">2. Using the Focal Loss<a class="anchor-link" href="#2.-Using-the-Focal-Loss"> </a></h2><p>The Focal loss (hereafter FL) was introduced by Tsung-Yi Lin et al., in their <a href="https://arxiv.org/pdf/1708.02002.pdf">2018 paper</a> Focal Loss for Dense Object Detection [1]. It is designed to address scenarios with extreme imbalanced classes, such as one-stage object detection where the imbalance between foreground and background classes can be, for example, 1:1000.</p>
<p>The adult census dataset is not really imbalanced, therefore is not the best dataset to test the performance of the FL. Nonetheless, let me illustrate how easy is to use the FL with <code>pytorch-widedeep</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;binary_focal_loss&quot;</span><span class="p">,</span>
    <span class="n">optimizers</span><span class="o">=</span><span class="n">optimizers</span><span class="p">,</span> 
    <span class="n">lr_schedulers</span><span class="o">=</span><span class="n">schedulers</span><span class="p">,</span> 
    <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="c1"># the alpha parameter of the focal loss</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="c1"># the gamma parameter of the focal loss</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wide</span><span class="o">=</span><span class="n">X_wide</span><span class="p">,</span> <span class="n">X_tab</span><span class="o">=</span><span class="n">X_tab</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">val_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To learn more about the losses available at <code>pytorch-widedeep</code> have a look at the <code>losses</code> module in the library or the <a href="https://pytorch-widedeep.readthedocs.io/en/latest/losses.html">docs</a>.</p>
<h2 id="3.-Regression-combining-tabular-data,-text-and-images">3. Regression combining tabular data, text and images<a class="anchor-link" href="#3.-Regression-combining-tabular-data,-text-and-images"> </a></h2><p>For this example we will use a small sample (so you can run it locally in a laptop) of the <a href="http://insideairbnb.com/get-the-data.html">Airbnb listings dataset</a> in London.</p>
<p>In case you are interested in all details, I did prepared the original dataset for this post, and all the code can be found at the <code>airbnb_data_preprocessing.py</code>, <a href="`https://github.com/jrzaurin/pytorch-widedeep/blob/master/examples/airbnb_data_preprocessing.py`">here</a>. After such preprocessing the data looks like this:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">airbnb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/airbnb/airbnb_sample.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">airbnb</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>host_id</th>
      <th>description</th>
      <th>host_listings_count</th>
      <th>host_identity_verified</th>
      <th>neighbourhood_cleansed</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>is_location_exact</th>
      <th>property_type</th>
      <th>room_type</th>
      <th>accommodates</th>
      <th>bathrooms</th>
      <th>bedrooms</th>
      <th>beds</th>
      <th>guests_included</th>
      <th>minimum_nights</th>
      <th>instant_bookable</th>
      <th>cancellation_policy</th>
      <th>has_house_rules</th>
      <th>host_gender</th>
      <th>accommodates_catg</th>
      <th>guests_included_catg</th>
      <th>minimum_nights_catg</th>
      <th>host_listings_count_catg</th>
      <th>bathrooms_catg</th>
      <th>bedrooms_catg</th>
      <th>beds_catg</th>
      <th>amenity_24-hour_check-in</th>
      <th>amenity__toilet</th>
      <th>amenity_accessible-height_bed</th>
      <th>amenity_accessible-height_toilet</th>
      <th>amenity_air_conditioning</th>
      <th>amenity_air_purifier</th>
      <th>amenity_alfresco_bathtub</th>
      <th>amenity_amazon_echo</th>
      <th>amenity_baby_bath</th>
      <th>amenity_baby_monitor</th>
      <th>amenity_babysitter_recommendations</th>
      <th>amenity_balcony</th>
      <th>amenity_bath_towel</th>
      <th>amenity_bathroom_essentials</th>
      <th>amenity_bathtub</th>
      <th>amenity_bathtub_with_bath_chair</th>
      <th>amenity_bbq_grill</th>
      <th>amenity_beach_essentials</th>
      <th>amenity_beach_view</th>
      <th>amenity_beachfront</th>
      <th>amenity_bed_linens</th>
      <th>amenity_bedroom_comforts</th>
      <th>...</th>
      <th>amenity_roll-in_shower</th>
      <th>amenity_room-darkening_shades</th>
      <th>amenity_safety_card</th>
      <th>amenity_sauna</th>
      <th>amenity_self_check-in</th>
      <th>amenity_shampoo</th>
      <th>amenity_shared_gym</th>
      <th>amenity_shared_hot_tub</th>
      <th>amenity_shared_pool</th>
      <th>amenity_shower_chair</th>
      <th>amenity_single_level_home</th>
      <th>amenity_ski-in_ski-out</th>
      <th>amenity_smart_lock</th>
      <th>amenity_smart_tv</th>
      <th>amenity_smoke_detector</th>
      <th>amenity_smoking_allowed</th>
      <th>amenity_soaking_tub</th>
      <th>amenity_sound_system</th>
      <th>amenity_stair_gates</th>
      <th>amenity_stand_alone_steam_shower</th>
      <th>amenity_standing_valet</th>
      <th>amenity_steam_oven</th>
      <th>amenity_stove</th>
      <th>amenity_suitable_for_events</th>
      <th>amenity_sun_loungers</th>
      <th>amenity_table_corner_guards</th>
      <th>amenity_tennis_court</th>
      <th>amenity_terrace</th>
      <th>amenity_toilet_paper</th>
      <th>amenity_touchless_faucets</th>
      <th>amenity_tv</th>
      <th>amenity_walk-in_shower</th>
      <th>amenity_warming_drawer</th>
      <th>amenity_washer</th>
      <th>amenity_washer_dryer</th>
      <th>amenity_waterfront</th>
      <th>amenity_well-lit_path_to_entrance</th>
      <th>amenity_wheelchair_accessible</th>
      <th>amenity_wide_clearance_to_shower</th>
      <th>amenity_wide_doorway_to_guest_bathroom</th>
      <th>amenity_wide_entrance</th>
      <th>amenity_wide_entrance_for_guests</th>
      <th>amenity_wide_entryway</th>
      <th>amenity_wide_hallways</th>
      <th>amenity_wifi</th>
      <th>amenity_window_guards</th>
      <th>amenity_wine_cooler</th>
      <th>security_deposit</th>
      <th>extra_people</th>
      <th>yield</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>13913.jpg</td>
      <td>54730</td>
      <td>My bright double bedroom with a large window has a relaxed feeling! It comfortably fits one or t...</td>
      <td>4.0</td>
      <td>f</td>
      <td>Islington</td>
      <td>51.56802</td>
      <td>-0.11121</td>
      <td>t</td>
      <td>apartment</td>
      <td>private_room</td>
      <td>2</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>1</td>
      <td>f</td>
      <td>moderate</td>
      <td>1</td>
      <td>female</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>100.0</td>
      <td>15.0</td>
      <td>12.0</td>
    </tr>
  </tbody>
</table>
<p>1 rows  223 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's define what will go through the wide and deep components</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># There are a number of columns that are already binary. Therefore, no need to one hot encode them</span>
<span class="n">crossed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;property_type&#39;</span><span class="p">,</span> <span class="s1">&#39;room_type&#39;</span><span class="p">)]</span>
<span class="n">already_dummies</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">airbnb</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s1">&#39;amenity&#39;</span> <span class="ow">in</span> <span class="n">c</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;has_house_rules&#39;</span><span class="p">]</span>
<span class="n">wide_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;is_location_exact&#39;</span><span class="p">,</span> <span class="s1">&#39;property_type&#39;</span><span class="p">,</span> <span class="s1">&#39;room_type&#39;</span><span class="p">,</span> <span class="s1">&#39;host_gender&#39;</span><span class="p">,</span>
<span class="s1">&#39;instant_bookable&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">already_dummies</span>
<span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">airbnb</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s1">&#39;catg&#39;</span> <span class="ow">in</span> <span class="n">c</span><span class="p">]</span> <span class="o">+</span> \
    <span class="p">[(</span><span class="s1">&#39;neighbourhood_cleansed&#39;</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;cancellation_policy&#39;</span><span class="p">,</span> <span class="mi">16</span><span class="p">)]</span>
<span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span> <span class="s1">&#39;longitude&#39;</span><span class="p">,</span> <span class="s1">&#39;security_deposit&#39;</span><span class="p">,</span> <span class="s1">&#39;extra_people&#39;</span><span class="p">]</span>
<span class="c1"># it does not make sense to standarised Latitude and Longitude. Here I am going to &quot;pass&quot; but you </span>
<span class="c1"># might want to check the LatLongScalarEnc available in the autogluon tabular library.</span>
<span class="n">already_standard</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span> <span class="s1">&#39;longitude&#39;</span><span class="p">]</span>
<span class="c1"># text and image colnames</span>
<span class="n">text_col</span> <span class="o">=</span> <span class="s1">&#39;description&#39;</span>
<span class="n">img_col</span> <span class="o">=</span> <span class="s1">&#39;id&#39;</span>
<span class="c1"># path to pretrained word embeddings and the images</span>
<span class="n">word_vectors_path</span> <span class="o">=</span> <span class="s1">&#39;data/glove.6B/glove.6B.100d.txt&#39;</span>
<span class="n">img_path</span> <span class="o">=</span> <span class="s1">&#39;data/airbnb/property_picture&#39;</span>
<span class="c1"># target</span>
<span class="n">target_col</span> <span class="o">=</span> <span class="s1">&#39;yield&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note the following: columns that are already dummies (defined as <code>already_dummies</code>), are treated as any other wide column. Internally, nothing will really happen to them. They will just add one entry to the embedding lookup table.</p>
<p>On the other hand, you will see that among the columns that will be passed through the <code>deeptabular</code> component we have <code>already_standard</code> columns, which are longitude and latitude in this case. These are columns for which it makes no sense to standardize them via <code>sklearn</code>'s <code>StandardScaler</code>, which is what <code>TabPreprocessor</code> uses internally.  A solution would be to pre-process them before-hand (using for example the <a href="https://github.com/awslabs/autogluon/blob/master/tabular/src/autogluon/tabular/models/tab_transformer/tab_transformer_encoder.py">LatLongScalarEnc</a> available at the <code>autogluon</code> library) and then pass them to the <code>TabPreprocessor</code>.</p>
<p>Nonetheless, in this case I am going to "ignore" this issue and move on since I just want to illustrate the use of the package.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">pytorch_widedeep.preprocessing</span> <span class="kn">import</span> <span class="n">WidePreprocessor</span><span class="p">,</span> <span class="n">TabPreprocessor</span><span class="p">,</span> <span class="n">TextPreprocessor</span><span class="p">,</span> <span class="n">ImagePreprocessor</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.models</span> <span class="kn">import</span> <span class="n">Wide</span><span class="p">,</span> <span class="n">TabMlp</span><span class="p">,</span> <span class="n">DeepText</span><span class="p">,</span> <span class="n">DeepImage</span><span class="p">,</span> <span class="n">WideDeep</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.initializers</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pytorch_widedeep.callbacks</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="n">airbnb</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">wide_preprocessor</span> <span class="o">=</span> <span class="n">WidePreprocessor</span><span class="p">(</span><span class="n">wide_cols</span><span class="o">=</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="n">crossed_cols</span><span class="p">)</span>
<span class="n">X_wide</span> <span class="o">=</span> <span class="n">wide_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>

<span class="n">tab_preprocessor</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span><span class="n">embed_cols</span><span class="o">=</span><span class="n">cat_embed_cols</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="n">X_tab</span> <span class="o">=</span> <span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>

<span class="n">text_preprocessor</span> <span class="o">=</span> <span class="n">TextPreprocessor</span><span class="p">(</span><span class="n">word_vectors_path</span><span class="o">=</span><span class="n">word_vectors_path</span><span class="p">,</span> <span class="n">text_col</span><span class="o">=</span><span class="n">text_col</span><span class="p">)</span>
<span class="n">X_text</span> <span class="o">=</span> <span class="n">text_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>

<span class="n">image_processor</span> <span class="o">=</span> <span class="n">ImagePreprocessor</span><span class="p">(</span><span class="n">img_col</span> <span class="o">=</span> <span class="n">img_col</span><span class="p">,</span> <span class="n">img_path</span> <span class="o">=</span> <span class="n">img_path</span><span class="p">)</span>
<span class="n">X_images</span> <span class="o">=</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The vocabulary contains 2192 tokens
Indexing word vectors...
Loaded 400000 word vectors
Preparing embeddings matrix...
2175 words in the vocabulary had data/glove.6B/glove.6B.100d.txt vectors and appear more than 5 times
Reading Images from data/airbnb/property_picture
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>  4%|         | 36/1001 [00:00&lt;00:02, 346.67it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Resizing
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|| 1001/1001 [00:02&lt;00:00, 372.15it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Computing normalisation metrics
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At this stage the data is ready to be passed through the model. However, instead of building a "simple" model that collects the <code>wide</code>, <code>deeptabular</code>, <code>deeptext</code> and <code>deepimage</code> component, I am going to use this opportunity to illustrate <code>pytorch-widedepp</code>'s flexibility to build wide and deep models. I like to call this, getting into <em>Kaggle mode</em>.</p>
<p>First we define the components of the model...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">wide_dim</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X_wide</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># deeptabular: 2 Dense layers</span>
<span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabMlp</span><span class="p">(</span>
    <span class="n">column_idx</span> <span class="o">=</span> <span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">column_idx</span><span class="p">,</span>
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">64</span><span class="p">],</span>
    <span class="n">mlp_dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">mlp_batchnorm</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">embed_input</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">embeddings_input</span><span class="p">,</span>
    <span class="n">embed_dropout</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">continuous_cols</span> <span class="o">=</span> <span class="n">continuous_cols</span><span class="p">,</span>
    <span class="n">batchnorm_cont</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
    
<span class="c1"># deeptext: a stack of 2 LSTMs</span>
<span class="n">deeptext</span> <span class="o">=</span> <span class="n">DeepText</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">text_preprocessor</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">),</span> 
    <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> 
    <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">rnn_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
    <span class="n">embed_matrix</span><span class="o">=</span><span class="n">text_preprocessor</span><span class="o">.</span><span class="n">embedding_matrix</span><span class="p">)</span>

<span class="c1"># Pretrained Resnet 18 (default is all but last 2 conv blocks frozen) plus a FC-Head 512-&gt;256-&gt;128</span>
<span class="n">deepimage</span> <span class="o">=</span> <span class="n">DeepImage</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">head_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>...and, as we build the model, add a fully connected <em>head</em> via the input parameters (could also be used via the additional component/parameter <code>deephead</code>)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span>
    <span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> 
    <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">,</span> 
    <span class="n">deeptext</span><span class="o">=</span><span class="n">deeptext</span><span class="p">,</span> 
    <span class="n">deepimage</span><span class="o">=</span><span class="n">deepimage</span><span class="p">,</span> 
    <span class="n">head_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's have a look to the model</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>WideDeep(
  (wide): Wide(
    (wide_linear): Embedding(357, 1, padding_idx=0)
  )
  (deeptabular): TabMlp(
    (embed_layers): ModuleDict(
      (emb_layer_accommodates_catg): Embedding(4, 16, padding_idx=0)
      (emb_layer_bathrooms_catg): Embedding(4, 16, padding_idx=0)
      (emb_layer_bedrooms_catg): Embedding(5, 16, padding_idx=0)
      (emb_layer_beds_catg): Embedding(5, 16, padding_idx=0)
      (emb_layer_cancellation_policy): Embedding(6, 16, padding_idx=0)
      (emb_layer_guests_included_catg): Embedding(4, 16, padding_idx=0)
      (emb_layer_host_listings_count_catg): Embedding(5, 16, padding_idx=0)
      (emb_layer_minimum_nights_catg): Embedding(4, 16, padding_idx=0)
      (emb_layer_neighbourhood_cleansed): Embedding(33, 64, padding_idx=0)
    )
    (embedding_dropout): Dropout(p=0.1, inplace=False)
    (norm): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (tab_mlp): MLP(
      (mlp): Sequential(
        (dense_layer_0): Sequential(
          (0): BatchNorm1d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Dropout(p=0.1, inplace=False)
          (2): Linear(in_features=196, out_features=128, bias=False)
          (3): ReLU(inplace=True)
        )
        (dense_layer_1): Sequential(
          (0): Dropout(p=0.1, inplace=False)
          (1): Linear(in_features=128, out_features=64, bias=True)
          (2): ReLU(inplace=True)
        )
      )
    )
  )
  (deeptext): DeepText(
    (word_embed): Embedding(2192, 100, padding_idx=1)
    (rnn): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.5)
  )
  (deepimage): DeepImage(
    (backbone): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): AdaptiveAvgPool2d(output_size=(1, 1))
    )
    (imagehead): MLP(
      (mlp): Sequential(
        (dense_layer_0): Sequential(
          (0): Dropout(p=0.1, inplace=False)
          (1): Linear(in_features=512, out_features=256, bias=True)
          (2): ReLU(inplace=True)
        )
        (dense_layer_1): Sequential(
          (0): Dropout(p=0.1, inplace=False)
          (1): Linear(in_features=256, out_features=128, bias=True)
          (2): ReLU(inplace=True)
        )
      )
    )
  )
  (deephead): MLP(
    (mlp): Sequential(
      (dense_layer_0): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=256, out_features=128, bias=True)
        (2): ReLU(inplace=True)
      )
      (dense_layer_1): Sequential(
        (0): Dropout(p=0.1, inplace=False)
        (1): Linear(in_features=128, out_features=64, bias=True)
        (2): ReLU(inplace=True)
      )
    )
    (head_out): Linear(in_features=64, out_features=1, bias=True)
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a big model, so let me go component by component.</p>
<ol>
<li><p><code>wide</code>: simple linear model implemented via an <code>Embedding</code> layer</p>
</li>
<li><p><code>deeptabular</code>: embeddings concatenated to categorical columns that are then passed through two dense layers with the following sizes [196 $\rightarrow$ 128 $\rightarrow$ 64].</p>
</li>
<li><p><code>deeptext</code>: two stacked LTSMs that will received the pre-trained glove wordvectors and output a last hidden state of dim 64 (this would be 128 if we had used <code>bidirectional = True</code>)</p>
</li>
<li><p><code>deepimage</code>: a pre-trained ResNet 18 model where only the last <code>Sequential</code> block (7) will be trained. The rest will remain "frozen". on top of it we have <code>imagehead</code> which is just a <code>Sequential</code> model comprised of two dense layers with the following sizes [512 $\rightarrow$ 256 $\rightarrow$ 128]</p>
</li>
<li><p><code>deephead</code>: on top of the 3 deep components we have a final component referred as <code>deephead</code>. This component will receive the concatenated output from all the deep components, and pass it through a further collection of dense layers. In this case the sizes are [256 $\rightarrow$ 64 $\rightarrow$ 1]. We input 256 because the output dim from <code>deeptabular</code> is 64, the  output dim from <code>deeptext</code> is 64 and the output dim from <code>deepimage</code> is 128. The final <code>deephead</code> output dim is 1 because we are performing a regression, i.e. one output neuron with no activation function.</p>
</li>
</ol>
<p>Let's go even a step further and use different optimizers, initializers and schedulers for different components. Moreover, let's use a different learning rate for different parameter groups in the case of the <code>deeptabular</code>, remember, this is <em>Kaggle mode</em>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Optimizers. Different parameter groups for the deeptabular component will use different lr</span>
<span class="n">tab_params</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">childname</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">childname</span> <span class="o">==</span> <span class="s1">&#39;deeptabular&#39;</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">n</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="n">child</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;emb_layer&quot;</span> <span class="ow">in</span> <span class="n">n</span><span class="p">:</span> <span class="n">tab_params</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">})</span>
            <span class="k">else</span><span class="p">:</span> <span class="n">tab_params</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">})</span>
                
<span class="n">wide_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wide</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>
<span class="n">tab_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">tab_params</span><span class="p">)</span>
<span class="n">text_opt</span> <span class="o">=</span> <span class="n">RAdam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">deeptext</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">img_opt</span>  <span class="o">=</span> <span class="n">RAdam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">deepimage</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">head_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">deephead</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;wide&#39;</span><span class="p">:</span> <span class="n">wide_opt</span><span class="p">,</span> <span class="s1">&#39;deeptabular&#39;</span><span class="p">:</span><span class="n">tab_opt</span><span class="p">,</span> <span class="s1">&#39;deeptext&#39;</span><span class="p">:</span><span class="n">text_opt</span><span class="p">,</span> <span class="s1">&#39;deepimage&#39;</span><span class="p">:</span> <span class="n">img_opt</span><span class="p">,</span> <span class="s1">&#39;deephead&#39;</span><span class="p">:</span> <span class="n">head_opt</span><span class="p">}</span>

<span class="c1"># schedulers</span>
<span class="n">wide_sch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">wide_opt</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">deep_sch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">tab_opt</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
<span class="n">text_sch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">text_opt</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">img_sch</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">tab_opt</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
<span class="n">head_sch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">head_opt</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">schedulers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;wide&#39;</span><span class="p">:</span> <span class="n">wide_sch</span><span class="p">,</span> <span class="s1">&#39;deeptabular&#39;</span><span class="p">:</span><span class="n">deep_sch</span><span class="p">,</span> <span class="s1">&#39;deeptext&#39;</span><span class="p">:</span><span class="n">text_sch</span><span class="p">,</span> <span class="s1">&#39;deepimage&#39;</span><span class="p">:</span> <span class="n">img_sch</span><span class="p">,</span> <span class="s1">&#39;deephead&#39;</span><span class="p">:</span> <span class="n">head_sch</span><span class="p">}</span>

<span class="c1"># initializers</span>
<span class="n">initializers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;wide&#39;</span><span class="p">:</span> <span class="n">KaimingNormal</span><span class="p">,</span> <span class="s1">&#39;deeptabular&#39;</span><span class="p">:</span><span class="n">KaimingNormal</span><span class="p">,</span> 
                <span class="s1">&#39;deeptext&#39;</span><span class="p">:</span><span class="n">KaimingNormal</span><span class="p">(</span><span class="n">pattern</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;^(?!.*word_embed).*$&quot;</span><span class="p">),</span> <span class="c1"># do not initialize the pre-trained word-vectors!</span>
                <span class="s1">&#39;deepimage&#39;</span><span class="p">:</span><span class="n">KaimingNormal</span><span class="p">}</span>

<span class="c1"># transforms and callbacks</span>
<span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.406</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.485</span><span class="p">]</span>  <span class="c1">#BGR</span>
<span class="n">std</span> <span class="o">=</span>  <span class="p">[</span><span class="mf">0.225</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.229</span><span class="p">]</span>  <span class="c1">#BGR</span>
<span class="n">transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">ToTensor</span><span class="p">,</span> <span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">)]</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">LRHistory</span><span class="p">(</span><span class="n">n_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="o">=</span><span class="s1">&#39;model_weights/wd_out&#39;</span><span class="p">)]</span>                
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that, since we will use pre-trained word embeddings, we do not want to initialize these embeddings. However you might still want to initialize the other layers in the <code>deeptext</code> component. This is not a problem, you can do that with the parameter <code>pattern</code> and your knowledge on regular  expressions. In the <code>deeptext</code> initializer definition above:</p>
<div class="highlight"><pre><span></span><span class="n">KaimingNormal</span><span class="p">(</span><span class="n">pattern</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;^(?!.*word_embed).*$&quot;</span><span class="p">)</span>
</pre></div>
<p>I am NOT initializing parameters whose name contains the string <code>word_embed</code>.</p>
<p>So...let's compile and run, which is as easy as:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;regression&quot;</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span> <span class="n">optimizers</span><span class="o">=</span><span class="n">optimizers</span><span class="p">,</span>
    <span class="n">lr_schedulers</span><span class="o">=</span><span class="n">schedulers</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="n">transforms</span><span class="o">=</span><span class="n">transforms</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wide</span><span class="o">=</span><span class="n">X_wide</span><span class="p">,</span> <span class="n">X_tab</span><span class="o">=</span><span class="n">X_tab</span><span class="p">,</span> <span class="n">X_text</span><span class="o">=</span><span class="n">X_text</span><span class="p">,</span> <span class="n">X_img</span><span class="o">=</span><span class="n">X_images</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">val_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 25/25 [02:11&lt;00:00,  5.28s/it, loss=1.27e+4]
valid: 100%|| 7/7 [00:15&lt;00:00,  2.25s/it, loss=9.2e+3] 
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As I mentioned early in the post, please, <strong>do not focus on the success metric/loss</strong> (<code>mse</code> in this case). I am just using a very small sample of the dataset and some "random" set up. I just want to illustrate usability. A benchmark post will come in the "no-so-distant future".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="4.-Finetune/Warmup-routines">4. Finetune/Warmup routines<a class="anchor-link" href="#4.-Finetune/Warmup-routines"> </a></h2><p>Let's place ourselves in two possible scenarios.</p>
<ol>
<li><p>Let's assume we have run a model and we want to just transfer the learnings (you know...transfer-learning) to another dataset, or simply we have received new data and we do not want to start the training of each component from scratch. Simply, we want to load the pre-trained weights and fine-tune.</p>
</li>
<li><p>Or, we just want to "warm up" individual model components individually before the joined training begins.</p>
</li>
</ol>
<p>This can be done with the <code>finetune</code> set of parameters (aliased all as <code>warmup</code> parameters if you wanted). There are 3 fine-tuning routines:</p>
<ol>
<li><p>Fine-tune all trainable layers at once with a triangular one-cycle learning rate (referred as slanted triangular learning rates in Howard &amp; Ruder 2018)</p>
</li>
<li><p>Gradual fine-tuning inspired by the work of Felbo et al., 2017 [2]</p>
</li>
<li><p>Gradual fine-tuning based on the work of Howard &amp; Ruder 2018 [3]</p>
</li>
</ol>
<p>Currently fine-tunning is only supported without a fully connected head, i.e. if <code>deephead=None</code>. In addition, Felbo and Howard routines apply only, of course, to the <code>deeptabular</code>, <code>deeptext</code> and <code>deepimage</code>models. The <code>wide</code> component can also be fine-tuned, but only in an "all at once" mode.</p>
<p>Let me briefly describe the "Felbo" and "Howard" routines before showing how to use them.</p>
<h3 id="4.1-The-Felbo-finetune-routine">4.1 The Felbo finetune routine<a class="anchor-link" href="#4.1-The-Felbo-finetune-routine"> </a></h3><p>The Felbo fine-tune routine can be illustrated by the following figure:</p>
<p>{% include image.html alt="resnet_block" max-width="500" file="/infinitoml/images/copied_from_nb/figures/pytorch-widedeep/felbo_routine.png" %}</p>
<p><strong>Figure 1</strong>. The figure can be described as follows: fine-tune (or train) the last layer for one epoch using a one cycle triangular learning rate. Then fine-tune the next deeper layer for one epoch, with a learning rate that is a factor of 2.5 lower than the previous learning rate (the 2.5 factor is fixed) while freezing the already warmed up layer(s). Repeat untill all individual layers are warmed. Then warm one last epoch with all warmed layers trainable. The vanishing color gradient in the figure attempts to illustrate the decreasing learning rate.</p>
<p>Note that this is not identical to the Fine-Tunning routine described in Felbo et al, 2017, this is why I used the word 'inspired'.</p>
<h3 id="4.2-The-Howard-finetune-routine">4.2 The Howard finetune routine<a class="anchor-link" href="#4.2-The-Howard-finetune-routine"> </a></h3><p>The Howard routine can be illustrated by the following figure:</p>
<p>{% include image.html alt="resnet_block" max-width="500" file="/infinitoml/images/copied_from_nb/figures/pytorch-widedeep/howard_routine.png" %}</p>
<p><strong>Figure 2</strong>. The figure can be described as follows: fine-tune (or train) the last layer for one epoch using a one cycle triangular learning rate. Then fine-tune the next deeper layer for one epoch, with a learning rate that is a factor of 2.5 lower than the previous learning rate (the 2.5 factor is fixed) while keeping the already warmed up layer(s) trainable. Repeat. The vanishing color gradient in the figure attempts to illustrate the decreasing learning rate.</p>
<p>Note that I write "fine-tune (or train) the last layer for one epoch [...]". However, in practice the user will have to specify the order of the layers to be fine-tuned. This is another reason why I wrote that the fine-tune routines I have implemented are inspired by the work of Felbo and Howard and not identical to their implemenations.</p>
<p>The felbo and howard routines can be accessed with via the <a href="https://pytorch-widedeep.readthedocs.io/en/latest/trainer.html">finetune parameters</a> (aliased as <code>warmup</code> parameters in case the user wants to use consistent naming). Let me go back to the adult dataset and let's have a look:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wide_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;education&#39;</span><span class="p">,</span> <span class="s1">&#39;relationship&#39;</span><span class="p">,</span><span class="s1">&#39;workclass&#39;</span><span class="p">,</span><span class="s1">&#39;occupation&#39;</span><span class="p">,</span><span class="s1">&#39;native_country&#39;</span><span class="p">,</span><span class="s1">&#39;gender&#39;</span><span class="p">]</span>
<span class="n">crossed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;education&#39;</span><span class="p">,</span> <span class="s1">&#39;occupation&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;native_country&#39;</span><span class="p">,</span> <span class="s1">&#39;occupation&#39;</span><span class="p">)]</span>
<span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;education&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;relationship&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;workclass&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;occupation&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),(</span><span class="s1">&#39;native_country&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">)]</span>
<span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">,</span><span class="s2">&quot;hours_per_week&quot;</span><span class="p">]</span>
<span class="n">target_col</span> <span class="o">=</span> <span class="s1">&#39;income_label&#39;</span>

<span class="c1"># TARGET</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">adult</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># WIDE</span>
<span class="n">wide_preprocessor</span> <span class="o">=</span> <span class="n">WidePreprocessor</span><span class="p">(</span><span class="n">wide_cols</span><span class="o">=</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="n">crossed_cols</span><span class="p">)</span>
<span class="n">X_wide</span> <span class="o">=</span> <span class="n">wide_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">adult</span><span class="p">)</span>

<span class="c1"># DEEP</span>
<span class="n">tab_preprocessor</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span><span class="n">embed_cols</span><span class="o">=</span><span class="n">cat_embed_cols</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="n">X_tab</span> <span class="o">=</span> <span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">adult</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">wide_dim</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X_wide</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabResnet</span><span class="p">(</span>
    <span class="n">blocks_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span> 
    <span class="n">column_idx</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">column_idx</span><span class="p">,</span>
    <span class="n">embed_input</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">embeddings_input</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>WideDeep(
  (wide): Wide(
    (wide_linear): Embedding(797, 1, padding_idx=0)
  )
  (deeptabular): Sequential(
    (0): TabResnet(
      (embed_layers): ModuleDict(
        (emb_layer_education): Embedding(17, 32, padding_idx=0)
        (emb_layer_native_country): Embedding(43, 32, padding_idx=0)
        (emb_layer_occupation): Embedding(16, 32, padding_idx=0)
        (emb_layer_relationship): Embedding(7, 32, padding_idx=0)
        (emb_layer_workclass): Embedding(10, 32, padding_idx=0)
      )
      (embedding_dropout): Dropout(p=0.1, inplace=False)
      (tab_resnet): DenseResnet(
        (dense_resnet): Sequential(
          (lin1): Linear(in_features=162, out_features=128, bias=True)
          (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (block_0): BasicBlock(
            (lin1): Linear(in_features=128, out_features=64, bias=True)
            (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)
            (dp): Dropout(p=0.1, inplace=False)
            (lin2): Linear(in_features=64, out_features=64, bias=True)
            (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (resize): Sequential(
              (0): Linear(in_features=128, out_features=64, bias=True)
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (block_1): BasicBlock(
            (lin1): Linear(in_features=64, out_features=32, bias=True)
            (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)
            (dp): Dropout(p=0.1, inplace=False)
            (lin2): Linear(in_features=32, out_features=32, bias=True)
            (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (resize): Sequential(
              (0): Linear(in_features=64, out_features=32, bias=True)
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
    )
    (1): Linear(in_features=32, out_features=1, bias=True)
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">Accuracy</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wide</span><span class="o">=</span><span class="n">X_wide</span><span class="p">,</span> <span class="n">X_tab</span><span class="o">=</span><span class="n">X_tab</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">val_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 172/172 [00:06&lt;00:00, 26.32it/s, loss=0.415, metrics={&#39;acc&#39;: 0.8016}]
valid: 100%|| 20/20 [00:00&lt;00:00, 74.72it/s, loss=0.364, metrics={&#39;acc&#39;: 0.8044}]
epoch 2: 100%|| 172/172 [00:06&lt;00:00, 26.31it/s, loss=0.372, metrics={&#39;acc&#39;: 0.8249}]
valid: 100%|| 20/20 [00:00&lt;00:00, 76.28it/s, loss=0.356, metrics={&#39;acc&#39;: 0.8256}]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s2">&quot;models_dir/model.t&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we are going to fine-tune the model components, and in the case of the <code>deeptabular</code> component, we will fine-tune the resnet-blocks and the linear layer but NOT the embeddings.</p>
<p>For this, we need to access the model component's children: <code>deeptabular</code> $\rightarrow$ <code>tab_resnet</code> $\rightarrow$ <code>dense_resnet</code> $\rightarrow$ <code>blocks</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># you can just load the model as any pytorch model or use the Trainer&#39;s staticmethod `load_model`</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Trainer</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;models_dir/model.t&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tab_lin_layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">deeptabular</span><span class="o">.</span><span class="n">children</span><span class="p">())[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tab_deep_layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
    <span class="nb">list</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">deeptabular</span><span class="o">.</span><span class="n">children</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">children</span><span class="p">())[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">children</span><span class="p">())[</span>
        <span class="mi">0</span>
    <span class="p">]</span><span class="o">.</span><span class="n">children</span><span class="p">()</span>
<span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tab_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">tab_lin_layers</span><span class="p">]</span> <span class="o">+</span> <span class="n">tab_deep_layers</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tab_layers</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[Linear(in_features=32, out_features=1, bias=True),
 BasicBlock(
   (lin1): Linear(in_features=64, out_features=32, bias=True)
   (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)
   (dp): Dropout(p=0.1, inplace=False)
   (lin2): Linear(in_features=32, out_features=32, bias=True)
   (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   (resize): Sequential(
     (0): Linear(in_features=64, out_features=32, bias=True)
     (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   )
 ),
 BasicBlock(
   (lin1): Linear(in_features=128, out_features=64, bias=True)
   (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)
   (dp): Dropout(p=0.1, inplace=False)
   (lin2): Linear(in_features=64, out_features=64, bias=True)
   (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   (resize): Sequential(
     (0): Linear(in_features=128, out_features=64, bias=True)
     (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
   )
 )]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">new_trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">Accuracy</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">new_trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_wide</span><span class="o">=</span><span class="n">X_wide</span><span class="p">,</span> 
    <span class="n">X_tab</span><span class="o">=</span><span class="n">X_tab</span><span class="p">,</span> 
    <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> 
    <span class="n">val_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
    <span class="n">finetune</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">finetune_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">finetune_deeptabular_gradual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">finetune_deeptabular_layers</span> <span class="o">=</span> <span class="n">tab_layers</span><span class="p">,</span>
    <span class="n">finetune_deeptabular_max_lr</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">n_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>  0%|          | 0/1374 [00:00&lt;?, ?it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training wide for 2 epochs
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 1374/1374 [00:09&lt;00:00, 150.31it/s, loss=0.421, metrics={&#39;acc&#39;: 0.7995}]
epoch 2: 100%|| 1374/1374 [00:08&lt;00:00, 160.97it/s, loss=0.361, metrics={&#39;acc&#39;: 0.8158}]
  0%|          | 0/1374 [00:00&lt;?, ?it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training deeptabular, layer 1 of 3
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 1374/1374 [00:23&lt;00:00, 58.62it/s, loss=0.385, metrics={&#39;acc&#39;: 0.8172}]
  0%|          | 0/1374 [00:00&lt;?, ?it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training deeptabular, layer 2 of 3
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 1374/1374 [00:26&lt;00:00, 51.08it/s, loss=0.373, metrics={&#39;acc&#39;: 0.8193}]
  0%|          | 0/1374 [00:00&lt;?, ?it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training deeptabular, layer 3 of 3
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 1374/1374 [00:24&lt;00:00, 55.97it/s, loss=0.368, metrics={&#39;acc&#39;: 0.8207}]
  0%|          | 0/1374 [00:00&lt;?, ?it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fine-tuning of individual components completed. Training the whole model for 2 epochs
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 1374/1374 [00:33&lt;00:00, 41.35it/s, loss=0.352, metrics={&#39;acc&#39;: 0.8373}]
valid: 100%|| 153/153 [00:01&lt;00:00, 113.01it/s, loss=0.35, metrics={&#39;acc&#39;: 0.8368}] 
epoch 2: 100%|| 1374/1374 [00:31&lt;00:00, 43.85it/s, loss=0.344, metrics={&#39;acc&#39;: 0.8398}]
valid: 100%|| 153/153 [00:01&lt;00:00, 129.62it/s, loss=0.348, metrics={&#39;acc&#39;: 0.8395}]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="5.-Custom-model">5. Custom model<a class="anchor-link" href="#5.-Custom-model"> </a></h2><p>So far we have used the components that come with <code>pytorch-widedee</code>. However, as I mentioned in the first post, it is very likely that the user wants to use custom models for the <code>deeptext</code> and <code>deepimage</code> components. This is easily attainable by...well...simply passing your own model.</p>
<p>You should just remember that the model must return the last layer of activations (and NOT the predictions) and must contained an attribute called <code>output_dim</code> with the output dimension of  that last layer.</p>
<p>For example, let's say we want to use as <code>deeptext</code> a <strong>very</strong> simple stack of 2 bidirectional GRUs. Let's see how to do such a thing with the airbnb dataset</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">crossed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;property_type&#39;</span><span class="p">,</span> <span class="s1">&#39;room_type&#39;</span><span class="p">)]</span>

<span class="n">already_dummies</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">airbnb</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s1">&#39;amenity&#39;</span> <span class="ow">in</span> <span class="n">c</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;has_house_rules&#39;</span><span class="p">]</span>

<span class="n">wide_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;is_location_exact&#39;</span><span class="p">,</span> <span class="s1">&#39;property_type&#39;</span><span class="p">,</span> <span class="s1">&#39;room_type&#39;</span><span class="p">,</span> <span class="s1">&#39;host_gender&#39;</span><span class="p">,</span>
<span class="s1">&#39;instant_bookable&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">already_dummies</span>

<span class="n">cat_embed_cols</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">airbnb</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s1">&#39;catg&#39;</span> <span class="ow">in</span> <span class="n">c</span><span class="p">]</span> <span class="o">+</span> \
    <span class="p">[(</span><span class="s1">&#39;neighbourhood_cleansed&#39;</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;cancellation_policy&#39;</span><span class="p">,</span> <span class="mi">16</span><span class="p">)]</span>

<span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span> <span class="s1">&#39;longitude&#39;</span><span class="p">,</span> <span class="s1">&#39;security_deposit&#39;</span><span class="p">,</span> <span class="s1">&#39;extra_people&#39;</span><span class="p">]</span>

<span class="n">already_standard</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span> <span class="s1">&#39;longitude&#39;</span><span class="p">]</span>

<span class="n">text_col</span> <span class="o">=</span> <span class="s1">&#39;description&#39;</span>

<span class="n">img_col</span> <span class="o">=</span> <span class="s1">&#39;id&#39;</span>
<span class="n">word_vectors_path</span> <span class="o">=</span> <span class="s1">&#39;data/glove.6B/glove.6B.100d.txt&#39;</span>

<span class="n">img_path</span> <span class="o">=</span> <span class="s1">&#39;data/airbnb/property_picture&#39;</span>

<span class="n">target_col</span> <span class="o">=</span> <span class="s1">&#39;yield&#39;</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">airbnb</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">wide_preprocessor</span> <span class="o">=</span> <span class="n">WidePreprocessor</span><span class="p">(</span><span class="n">wide_cols</span><span class="o">=</span><span class="n">wide_cols</span><span class="p">,</span> <span class="n">crossed_cols</span><span class="o">=</span><span class="n">crossed_cols</span><span class="p">)</span>
<span class="n">X_wide</span> <span class="o">=</span> <span class="n">wide_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>

<span class="n">tab_preprocessor</span> <span class="o">=</span> <span class="n">TabPreprocessor</span><span class="p">(</span><span class="n">embed_cols</span><span class="o">=</span><span class="n">cat_embed_cols</span><span class="p">,</span> <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span><span class="p">)</span>
<span class="n">X_tab</span> <span class="o">=</span> <span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>

<span class="n">text_preprocessor</span> <span class="o">=</span> <span class="n">TextPreprocessor</span><span class="p">(</span><span class="n">word_vectors_path</span><span class="o">=</span><span class="n">word_vectors_path</span><span class="p">,</span> <span class="n">text_col</span><span class="o">=</span><span class="n">text_col</span><span class="p">)</span>
<span class="n">X_text</span> <span class="o">=</span> <span class="n">text_preprocessor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>

<span class="n">image_processor</span> <span class="o">=</span> <span class="n">ImagePreprocessor</span><span class="p">(</span><span class="n">img_col</span> <span class="o">=</span> <span class="n">img_col</span><span class="p">,</span> <span class="n">img_path</span> <span class="o">=</span> <span class="n">img_path</span><span class="p">)</span>
<span class="n">X_images</span> <span class="o">=</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">airbnb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The vocabulary contains 2192 tokens
Indexing word vectors...
Loaded 400000 word vectors
Preparing embeddings matrix...
2175 words in the vocabulary had data/glove.6B/glove.6B.100d.txt vectors and appear more than 5 times
Reading Images from data/airbnb/property_picture
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>  4%|         | 39/1001 [00:00&lt;00:02, 389.27it/s]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Resizing
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>100%|| 1001/1001 [00:02&lt;00:00, 381.95it/s]
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Computing normalisation metrics
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">MyDeepText</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyDeepText</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># word/token embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span>
        <span class="p">)</span>

        <span class="c1"># stack of RNNs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Remember, this must be defined. If not WideDeep will through an error</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embed</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
        <span class="n">o</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">h</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And from here, "<em>proceed as usual</em>"</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wide</span> <span class="o">=</span> <span class="n">Wide</span><span class="p">(</span><span class="n">wide_dim</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X_wide</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pred_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">deeptabular</span> <span class="o">=</span> <span class="n">TabMlp</span><span class="p">(</span> 
    <span class="n">mlp_hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">32</span><span class="p">],</span>
    <span class="n">column_idx</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">column_idx</span><span class="p">,</span>
    <span class="n">embed_input</span><span class="o">=</span><span class="n">tab_preprocessor</span><span class="o">.</span><span class="n">embeddings_input</span><span class="p">,</span>
    <span class="n">continuous_cols</span><span class="o">=</span><span class="n">continuous_cols</span>
<span class="p">)</span>
<span class="n">mydeeptext</span> <span class="o">=</span> <span class="n">MyDeepText</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">text_preprocessor</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">itos</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">WideDeep</span><span class="p">(</span><span class="n">wide</span><span class="o">=</span><span class="n">wide</span><span class="p">,</span> <span class="n">deeptabular</span><span class="o">=</span><span class="n">deeptabular</span><span class="p">,</span> <span class="n">deeptext</span><span class="o">=</span><span class="n">mydeeptext</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>WideDeep(
  (wide): Wide(
    (wide_linear): Embedding(357, 1, padding_idx=0)
  )
  (deeptabular): Sequential(
    (0): TabMlp(
      (embed_layers): ModuleDict(
        (emb_layer_accommodates_catg): Embedding(4, 16, padding_idx=0)
        (emb_layer_bathrooms_catg): Embedding(4, 16, padding_idx=0)
        (emb_layer_bedrooms_catg): Embedding(5, 16, padding_idx=0)
        (emb_layer_beds_catg): Embedding(5, 16, padding_idx=0)
        (emb_layer_cancellation_policy): Embedding(6, 16, padding_idx=0)
        (emb_layer_guests_included_catg): Embedding(4, 16, padding_idx=0)
        (emb_layer_host_listings_count_catg): Embedding(5, 16, padding_idx=0)
        (emb_layer_minimum_nights_catg): Embedding(4, 16, padding_idx=0)
        (emb_layer_neighbourhood_cleansed): Embedding(33, 64, padding_idx=0)
      )
      (embedding_dropout): Dropout(p=0.1, inplace=False)
      (tab_mlp): MLP(
        (mlp): Sequential(
          (dense_layer_0): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=196, out_features=64, bias=True)
            (2): ReLU(inplace=True)
          )
          (dense_layer_1): Sequential(
            (0): Dropout(p=0.1, inplace=False)
            (1): Linear(in_features=64, out_features=32, bias=True)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (1): Linear(in_features=32, out_features=1, bias=True)
  )
  (deeptext): Sequential(
    (0): MyDeepText(
      (word_embed): Embedding(2192, 100, padding_idx=1)
      (rnn): GRU(100, 64, num_layers=2, batch_first=True, bidirectional=True)
    )
    (1): Linear(in_features=128, out_features=1, bias=True)
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;regression&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_wide</span><span class="o">=</span><span class="n">X_wide</span><span class="p">,</span> <span class="n">X_tab</span><span class="o">=</span><span class="n">X_tab</span><span class="p">,</span> <span class="n">X_text</span><span class="o">=</span><span class="n">X_text</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">val_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>epoch 1: 100%|| 13/13 [00:03&lt;00:00,  3.77it/s, loss=1.79e+4]
valid: 100%|| 4/4 [00:00&lt;00:00, 13.34it/s, loss=1.49e+4]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="6.-Conclusion">6. Conclusion<a class="anchor-link" href="#6.-Conclusion"> </a></h2><p>In this second post I tried to illustrate in detail the different functionalities of the <code>pytorch-widedeep</code> package, and how these can be used to customize each of the four potential components of the <code>WideDeep</code> model that can be built with <code>pytorch-widedeep</code>. I have also describe the warm-up routines that can be used to "warm-up" each individual component before the joined training and finally, how custom models, "external" to <code>pytorch-widedeep</code> can be used in combination with the package.</p>
<p>However, this is not the end of the journey. As you will have seen, there is an "<em>imbalance in the <code>pytorch-widedeep</code> force</em>", in the sense that while fully pre-trained models are incorporated for the <code>deepimage</code> component, this is not the case for the <code>deeptext</code> component, where only pre-trained word embeddings are considered. Of course, as illustrated in Section 4, you could build your own pre-trained <code>deeptext</code> component and pass it to the <code>WideDeep</code> constructor class, but eventually, I want to allow that option within the package.</p>
<p>This means that eventually I will need to integrate the library with some of the pre-trained Language models available or simply code a custom version for <code>pytorch-widedeep</code>.</p>
<p>One the other hand, I want to bring more DL models for the <code>deeptabular</code> components, such as <a href="https://arxiv.org/pdf/1908.07442.pdf">TabNet</a>. There is already a fantastic <a href="https://github.com/dreamquark-ai/tabnet"><code>Pytorch</code> implementation</a> which I highly recommend.</p>
<p>If you made it this far, thanks for reading! And if you use the package, let me know your thoughts!</p>
<h4 id="References">References<a class="anchor-link" href="#References"> </a></h4><p>[1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, et al., 2018: Focal Loss for Dense Object Detection. <a href="https://arxiv.org/pdf/1708.02002.pdf">arXiv:1708.02002v2</a></p>
<p>[3] Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm. Bjarke Felbo, Alan Mislove, Anders Sgaard, et al., 2017. <a href="https://arxiv.org/abs/1708.00524">arXiv:1708.00524</a></p>
<p>[3] Universal Language Model Fine-tuning for Text Classification. Jeremy Howard, Sebastian Ruder, 2018 <a href="https://arxiv.org/abs/1801.06146">arXiv:1801.06146v5</a></p>

</div>
</div>
</div>
</div>
 

