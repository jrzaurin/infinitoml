{
  
    
        "post0": {
            "title": "pytorch-widedeep, deep learning for tabular data III: the deeptabular component",
            "content": "This is the third of a series of posts introducing pytorch-widedeep, a flexible package to combine tabular data with text and images (that could also be used for &quot;standard&quot; tabular data alone). . While writing this post I will assume that the reader is not familiar with the previous two posts. Of course, reading them would help, but in order to understand the content of this post and then being able to use pytorch-widedeep on tabular data, is not a requirement. . As I mentioned earlier, pytorch-widedeep&#39;s main goal is to facilitate the combination of images and text with tabular data via wide and deep models. To that aim, wide and deep models can be built with up to four model components: wide, deeptabular, deeptext and deepimage, that will take care of the different types of input datasets (&quot;standard&quot; tabular, i.e. numerical and categorical features, text and images). This post focuses only on the so called deeptabular component, and the 3 different models available in this library that can be used to build that component. Nonetheless, and for completion, I will briefly describe the remaining components first. . The wide component of a wide and deep model is simply a liner model, and in pytorch-widedeep such model can be created via the Wide class. In the case of the deeptext component, pytorch-widedeep offers one model, available via the DeepText class. DeepText builds a simple stack of LSTMs, i.e. a standard DL text classifier or regressor, with flexibility regarding the use of pre-trained word embeddings, of a Fully Connected Head (FC-Head), etc. For the deepimage component, pytorch-widedeep includes two alternatives: a pre-trained Resnet model or a &quot;standard&quot; stack of CNNs to be trained from scratch. The two are available via the DeepImage class which, as in the case of DeepText, offers some flexibility when building the architecture. . To clarify the use of the term &quot;model&quot; and Wide and Deep &quot;model component&quot; (in case there is some confusion), let&#39;s have a look to the following code: . wide_model = Wide(...) text_model = DeepText(...) image_model = DeepImage(...) # we use the previous models as the wide and deep model components wdmodel = WideDeep(wide=wide_model, deeptext=text_model, deepimage=image_model) ... . Simply, a wide and deep model has model components that are (of course) models themselves. Note that any of the four wide and deep model components can be a custom model by the user. In fact, while I recommend using the models available in pytorch-widedeep for the wide and deeptabular model components, it is very likely that users will want to use their own models for the deeptext and deepimagecomponents. That is perfectly possible as long as the custom models have an attribute called output_dim with the size of the last layer of activations, so that WideDeep can be constructed. In addition, any of the four components can be used independently in isolation. For example, you might want to use just a wide component, which is simply a linear model. To that aim, simply: . wide_model = Wide(...) # this would not be a wide and deep model but just wide wdmodel = WideDeep(wide=wide_model) ... . If you want to learn more about different model components and the models available in pytorch-widedeep please, have a look to the Examples folder in the repo, the documentation or the companion posts. Let&#39;s now take a deep dive into the models available for the deeptabular component . 1. The deeptabular component . As I was developing the package I realised that perhaps one of the most interesting offerings in pytorch-widedeep was related to the models available for the deeptabular component. Remember that each component can be used independently in isolation. Building a WideDeep model comprised only by a deeptabular component would be what is normally referred as DL for tabular data. Of course, such model is not a wide and deep model, is &quot;just&quot; deep. . Currently, pytorch-widedeep offers three models that can be used as the deeptabular component. In order of complexity, these are: . TabMlp: this is very similar to the tabular model in the fantastic fastai library, and consists simply in embeddings representing the categorical features, concatenated with the continuous features, and passed then through a MLP. . | TabRenset: This is similar to the previous model but the embeddings are passed through a series of ResNet blocks built with dense layers. . | TabTransformer: Details on the TabTransformer can be found in: TabTransformer: Tabular Data Modeling Using Contextual Embeddings. Again, this is similar to the models before but the embeddings are passed through a series of Transformer (encoder) blocks. . | . A lot has been (and is being) written about the use of DL for tabular data, and certainly each of these models would deserve a post by themselves (the TabMlp is a section in the great fastai book and the TabTransformer was presented in a scientific publication). Here, I will try to describe them with some detail and illustrate their use within pytorch-widedeep. A proper benchmark exercise will be carried out in a not-so-distant future. . 1.1 TabMlp . The following figure illustrates the TabMlp model architecture. . . Fig 1. The TabMlp: this is the simples architecture and is very similar to the tabular model available in the fantastic fastai library. In fact, the implementation of the dense layers of the MLP is mostly identical to that in that library. . The dashed-border boxes indicate that these components are optional. For example, we could use TabMlp without categorical components, or without continuous components, if we wanted. . Let&#39;s have a look and see how this model is used . #collapse-hide import pandas as pd import numpy as np from sklearn.model_selection import train_test_split adult = pd.read_csv(&quot;data/adult/adult.csv.zip&quot;) adult.columns = [c.replace(&quot;-&quot;, &quot;_&quot;) for c in adult.columns] adult[&quot;income_label&quot;] = (adult[&quot;income&quot;].apply(lambda x: &quot;&gt;50K&quot; in x)).astype(int) adult.drop(&quot;income&quot;, axis=1, inplace=True) for c in adult.columns: if adult[c].dtype == &#39;O&#39;: adult[c] = adult[c].apply(lambda x: &quot;unknown&quot; if x == &quot;?&quot; else x) adult[c] = adult[c].str.lower() adult_train, adult_test = train_test_split(adult, test_size=0.2, stratify=adult.income_label) . . adult.head() . age workclass fnlwgt education educational_num marital_status occupation relationship race gender capital_gain capital_loss hours_per_week native_country income_label . 0 25 | private | 226802 | 11th | 7 | never-married | machine-op-inspct | own-child | black | male | 0 | 0 | 40 | united-states | 0 | . 1 38 | private | 89814 | hs-grad | 9 | married-civ-spouse | farming-fishing | husband | white | male | 0 | 0 | 50 | united-states | 0 | . 2 28 | local-gov | 336951 | assoc-acdm | 12 | married-civ-spouse | protective-serv | husband | white | male | 0 | 0 | 40 | united-states | 1 | . 3 44 | private | 160323 | some-college | 10 | married-civ-spouse | machine-op-inspct | husband | black | male | 7688 | 0 | 40 | united-states | 1 | . 4 18 | unknown | 103497 | some-college | 10 | never-married | unknown | own-child | white | female | 0 | 0 | 30 | united-states | 0 | . # define the embedding and continuous columns, and target embed_cols = [ (&#39;workclass&#39;, 6), (&#39;education&#39;, 8), (&#39;marital_status&#39;, 6), (&#39;occupation&#39;,8), (&#39;relationship&#39;, 6), (&#39;race&#39;, 6)] cont_cols = [&quot;age&quot;, &quot;hours_per_week&quot;, &quot;fnlwgt&quot;, &quot;educational_num&quot;] target = adult_train[&quot;income_label&quot;].values . # prepare deeptabular component from pytorch_widedeep.preprocessing import TabPreprocessor tab_preprocessor = TabPreprocessor(embed_cols=embed_cols, continuous_cols=cont_cols) X_tab = tab_preprocessor.fit_transform(adult_train) . Let&#39;s pause for a second, since the code up until here is going to be common to all models with some minor adaptations for the TabTransformer. So far, we have simply defined the columns that will be represented by embeddings and the numerical (aka continuous) columns. Once they are defined the dataset is prepared with the TabPreprocessor. Internally, the preprocessor label encodes the &quot;embedding columns&quot; and standardizes the numerical columns. Note that one could chose not to standardizes the numerical columns and then use a BatchNorm1D layer when building the model. That is also a valid approach. Alternatively, one could use both, as I will. . At this stage the data is prepared and we are ready to build the model . from pytorch_widedeep.models import TabMlp, WideDeep tabmlp = TabMlp( mlp_hidden_dims=[200, 100], column_idx=tab_preprocessor.column_idx, embed_input=tab_preprocessor.embeddings_input, continuous_cols=cont_cols, batchnorm_cont=True, ) . /Users/javier/.pyenv/versions/3.7.9/envs/wdposts/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . Let&#39;s have a look to the model we just built and how it relates to Fig 1 . tabmlp . TabMlp( (embed_layers): ModuleDict( (emb_layer_education): Embedding(17, 8, padding_idx=0) (emb_layer_marital_status): Embedding(8, 6, padding_idx=0) (emb_layer_occupation): Embedding(16, 8, padding_idx=0) (emb_layer_race): Embedding(6, 6, padding_idx=0) (emb_layer_relationship): Embedding(7, 6, padding_idx=0) (emb_layer_workclass): Embedding(10, 6, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (norm): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (tab_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=44, out_features=200, bias=True) (2): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=200, out_features=100, bias=True) (2): ReLU(inplace=True) ) ) ) ) . As we can see, we have a series of columns that would be represented as embeddings. The embeddings from all these columns are concatenated, to form a tensor of dim (bsz, 40) where bsz is batch size. Then, the &quot;batchnormed&quot; continuous columns are also concatenated, resulting in a tensor of dim (bsz, 44), that will be passed to the 2-layer MLP (200 -&gt; 100). . One important thing to mention, common to all model components, is that pytorch-widedeep models do not build the last connection, i.e. the connection with the output neuron or neurons depending whether this is a regression, binary or multi-class classification. Such connection is built by the WideDeep constructor class. This means that even if we wanted to use a single-component model, the model still needs to be built with the WideDeep class. . This is because the library is, a priori, intended to build WideDeep models (and hence its name). Once the model is built it is passed to the Trainer (as we will see now). The Trainer class is coded to receive a parent model of class WideDeep with children that are the model components. This is very convenient for a number of aspects in the library. . Effectively this simply requires one extra line of code. . model = WideDeep(deeptabular=tabmlp) . model . WideDeep( (deeptabular): Sequential( (0): TabMlp( (embed_layers): ModuleDict( (emb_layer_education): Embedding(17, 8, padding_idx=0) (emb_layer_marital_status): Embedding(8, 6, padding_idx=0) (emb_layer_occupation): Embedding(16, 8, padding_idx=0) (emb_layer_race): Embedding(6, 6, padding_idx=0) (emb_layer_relationship): Embedding(7, 6, padding_idx=0) (emb_layer_workclass): Embedding(10, 6, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (norm): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (tab_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=44, out_features=200, bias=True) (2): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=200, out_features=100, bias=True) (2): ReLU(inplace=True) ) ) ) ) (1): Linear(in_features=100, out_features=1, bias=True) ) ) . As we can see, our model has the final connection now and is a model of class WideDeep formed by one single component, deeptabular, which is a model of class TabMlp formed mainly by the embed_layers and an MLP very creatively called tab_mlp. . We are now ready to train it. The code below simply runs with defaults. one could use any torch optimizer, learning rate schedulers, etc. Just have a look to the docs or the Examples folder in the repo. . from pytorch_widedeep import Trainer from pytorch_widedeep.metrics import Accuracy trainer = Trainer(model, objective=&quot;binary&quot;, metrics=[(Accuracy)]) trainer.fit(X_tab=X_tab, target=target, n_epochs=5, batch_size=256, val_split=0.2) . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:03&lt;00:00, 37.56it/s, loss=0.403, metrics={&#39;acc&#39;: 0.8126}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 79.02it/s, loss=0.379, metrics={&#39;acc&#39;: 0.8129}] epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:02&lt;00:00, 46.57it/s, loss=0.364, metrics={&#39;acc&#39;: 0.8309}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 101.29it/s, loss=0.372, metrics={&#39;acc&#39;: 0.8286}] epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:02&lt;00:00, 53.31it/s, loss=0.359, metrics={&#39;acc&#39;: 0.832}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 115.28it/s, loss=0.363, metrics={&#39;acc&#39;: 0.8309}] epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:01&lt;00:00, 62.93it/s, loss=0.355, metrics={&#39;acc&#39;: 0.8335}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 120.26it/s, loss=0.361, metrics={&#39;acc&#39;: 0.8326}] epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:01&lt;00:00, 61.74it/s, loss=0.355, metrics={&#39;acc&#39;: 0.8353}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 117.20it/s, loss=0.358, metrics={&#39;acc&#39;: 0.8342}] . Once we understand what TabMlp does, TabResnet should be pretty straightforward . 1.2 TabResnet . The following figure illustrates the TabResnet model architecture. . . Fig 2. The TabResnet: this model is similar to the TabMlp, but the embeddings (or the concatenation of embeddings and continuous features, normalised or not) are passed through a series of Resnet blocks built with dense layers. The dashed-border boxes indicate that the component is optional and the dashed lines indicate the different paths or connections present depending on which components we decide to include. . This is probably the most flexible of the three models discussed in this post in the sense that there are many variants one can define via the parameters. For example, we could chose to concatenate the continuous features, normalized or not via a BatchNorm1d layer, with the embeddings and then pass the result of such a concatenation trough the series of Resnet blocks. Alternatively, we might prefer to concatenate the continuous features with the results of passing the embeddings through the Resnet blocks. Another optional component is the MLP before the output neuron(s). If not MLP is present, the output from the Resnet blocks or the results of concatenating that output with the continuous features (normalised or not) will be connected directly to the output neuron(s). . Each of the Resnet block is comprised by the following operations: . . Fig 3. &quot;Dense&quot; Resnet Block. b is the batch size and d the dimension of the embeddings. . Let&#39;s build a TabResnet model: . from pytorch_widedeep.models import TabResnet tabresnet = TabResnet( column_idx=tab_preprocessor.column_idx, embed_input=tab_preprocessor.embeddings_input, continuous_cols=cont_cols, batchnorm_cont=True, blocks_dims=[200, 100, 100], mlp_hidden_dims=[100, 50], ) model = WideDeep(deeptabular=tabresnet) model . /Users/javier/.pyenv/versions/3.7.9/envs/wdposts/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . WideDeep( (deeptabular): Sequential( (0): TabResnet( (embed_layers): ModuleDict( (emb_layer_education): Embedding(17, 8, padding_idx=0) (emb_layer_marital_status): Embedding(8, 6, padding_idx=0) (emb_layer_occupation): Embedding(16, 8, padding_idx=0) (emb_layer_race): Embedding(6, 6, padding_idx=0) (emb_layer_relationship): Embedding(7, 6, padding_idx=0) (emb_layer_workclass): Embedding(10, 6, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (norm): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (tab_resnet): DenseResnet( (dense_resnet): Sequential( (lin1): Linear(in_features=44, out_features=200, bias=True) (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (block_0): BasicBlock( (lin1): Linear(in_features=200, out_features=100, bias=True) (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True) (dp): Dropout(p=0.1, inplace=False) (lin2): Linear(in_features=100, out_features=100, bias=True) (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (resize): Sequential( (0): Linear(in_features=200, out_features=100, bias=True) (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (block_1): BasicBlock( (lin1): Linear(in_features=100, out_features=100, bias=True) (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True) (dp): Dropout(p=0.1, inplace=False) (lin2): Linear(in_features=100, out_features=100, bias=True) (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (tab_resnet_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=100, out_features=100, bias=True) (2): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=100, out_features=50, bias=True) (2): ReLU(inplace=True) ) ) ) ) (1): Linear(in_features=50, out_features=1, bias=True) ) ) . As we did previously with the TabMlp, let&#39;s &quot;walk through&quot; the model. As we can see, the object model is an instance of a WideDeep model formed by a single component, deeptabular that is a TabResnet model. The TabResnet model is formed by a series of embeddings that are concatenated themselves, and then further concatenated with the normalised continuous columns. The resulting tensor of dim (bsz, 44) is then passed through a tab_resnet component, which is comprised by two so-called &quot;dense&quot; Resnet blocks. The output of the Resnet blocks, of dim (bsz, 100) is passed through a 2-layer MLP, named tab_resnet_mlp and finally &quot;plugged&quot; into the output neuron. In summary: Embedding + DenseResnet + MLP. . To run it, the code is, as one might expect identical to the one shown before for the TabMlp. . trainer = Trainer(model, objective=&quot;binary&quot;, metrics=[(Accuracy)]) trainer.fit(X_tab=X_tab, target=target, n_epochs=5, batch_size=256, val_split=0.2) . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:04&lt;00:00, 29.04it/s, loss=0.382, metrics={&#39;acc&#39;: 0.8178}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 100.56it/s, loss=0.367, metrics={&#39;acc&#39;: 0.818}] epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:04&lt;00:00, 29.43it/s, loss=0.355, metrics={&#39;acc&#39;: 0.8345}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 82.47it/s, loss=0.355, metrics={&#39;acc&#39;: 0.8332}] epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:04&lt;00:00, 26.99it/s, loss=0.349, metrics={&#39;acc&#39;: 0.8369}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 107.97it/s, loss=0.354, metrics={&#39;acc&#39;: 0.8354}] epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:04&lt;00:00, 28.54it/s, loss=0.348, metrics={&#39;acc&#39;: 0.8386}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 103.24it/s, loss=0.352, metrics={&#39;acc&#39;: 0.8369}] epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:04&lt;00:00, 30.13it/s, loss=0.346, metrics={&#39;acc&#39;: 0.8384}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 84.18it/s, loss=0.352, metrics={&#39;acc&#39;: 0.8364}] . And now, last but not least, the last addition to the library, the TabTransformer. . &#160;1.3 TabTransformer . The TabTransformer is described in detail in TabTransformer: Tabular Data Modeling Using Contextual Embeddings [1], by the clever guys at Amazon. Is an entertaining paper that I, of course, strongly recommend if you are going to use this model on your tabular data (and also in general if you are interested in DL for tabular data). . My implementation is not the only one available. Given that the model was conceived by the researchers at Amazon, it is also available in their fantastic autogluon library (which you should definitely check). In addition, you can find another implementation here by Phil Wang, whose entire github is simply outstanding. My implementation is partially inspired by these but has some particularities and adaptations so that it works within the pytorch-widedeep package. . The following figure illustrates the TabTransformer model architecture. . . Fig 4. The TabTransfomer, described in [TabTransformer: Tabular Data Modeling Using Contextual Embeddings]. (https://arxiv.org/pdf/2012.06678.pdf). The dashed-border boxes indicate that the component is optional. . As in previous cases, there are a number of variants and details to consider as one builds the model. I will describe some here, but for a full view of all the possible parameters, please, have a look to the docs. . I don&#39;t want to go into the details of what is a Transformer [2] in this post. There is an overwhelming amount of literature if you wanted to learn about it, with the most popular being perhaps The Annotated Transformer. Also check this post and if you are a math &quot;maniac&quot; you might like this paper [3]. However, let me just briefly describe it so I can introduce the little math we will need for this post. In one sentence, a Transformer consists of a multi-head self-attention layer followed by feed-forward layer, with element-wise addition and layer-normalization being done after each layer. . As most of you will know, a self-attention layer comprises three matrices, Key, Query and Value. Each input categorical column, i.e. embedding, is projected onto these matrices (although see the fixed_attention option later in the post) to generate their corresponding key, query and value vectors. Formally, let $K in R^{e times d}$, $Q in R^{e times d}$ and $V in R^{e times d}$ be the Key, Query and Value matrices of the embeddings where $e$ is the embeddings dimension and $d$ is the dimension of all the Key, Query and Value matrices. Then every input categorical column, i.e embedding, attends to all other categorical columns through an attention head: . $$ Attention(K, Q, V ) = A cdot V, hspace{5cm}(1) $$where . $$ A = softmax( frac{QK^T}{ sqrt{d}} ), hspace{6cm}(2) $$And that is all the math we need. . As I was thinking in a figure to illustrate a transformer block, I realised that there is a chance that the reader has seen every possible representation/figure. Therefore, I decided to illustrate the transformer block in a way that relates directly to the way it is implemented. . . Fig 5. The Transfomer block. The letters in parenthesis indicate the dimension of the corresponding tensor after the operation indicated in the corresponding box. For example, the tensor attn_weights has dim (b, h, s, s). . As the figure shows, the input tensor ($X$) is projected onto its key, query and value matrices. These are then &quot;re-arranged into&quot; the multi-head self-attention layer where each head will attend to part of the embeddings. We then compute $A$ (Eq 2), which is then multiplied by $V$ to obtain what I refer as attn_score (Eq 1). attn_score is then re-arranged, so that we &quot;collect&quot; the attention scores from all the heads, and projected again to obtain the results (attn_out), that will be added to the input and normalised (Y). Finally Y goes through the Feed-Forward layer and a further Add + Norm. . Before moving to the code related to building the model itself, there are a couple of details in the implementation that are worth mentioning. Here we go: . FullEmbeddingDropout: when building a TabTransformer model, there is the possibility of dropping entirely the embedding corresponding to a categorical column. This is set by the parameter full_embed_dropout: bool, which points to the class FullEmbeddingDropout. | . SharedEmbeddings: when building a TabTransformer model, it is possible for all the embeddings that represent a categorical column to share a fraction of their embeddings, or define a common separated embedding per column that will be added to the column&#39;s embeddings. . The idea behind this so-called &quot;column embedding&quot; is to enable the model to distinguish the classes in one column from those in the other columns. In other words, we want the model to learn representations not only of the different categorical values in the column, but also of the column itself. This is attained by the shared_embed group of parameters: share_embed : bool, add_shared_embed: bool and frac_shared_embed: int. The first simply indicates if embeddings will be shared, the second sets the sharing strategy and the third one the fraction of the embeddings that will be shared, depending on the strategy. They all relate to the class SharedEmbeddings . For example, let&#39;s say that we have a categorical column with 5 different categories that will be encoded as embeddings of dim 8. This will result in a lookup table for that column of dim (5, 8). The two sharing strategies are illustrated in Fig 6. . . Fig 6. The two sharing embeddings strategies. Upper panel: the &quot;column embedding&quot; replaces embedding dim / frac_shared_embed (4 in this case) of the total embeddings that represent the different values of the categorical column. Lower panel: the &quot;column embedding&quot; is added (well, technically broadcasted and added) to the original embedding lookup table. Note that n_cat here refers to the number of different categories for this particular column. . | . fixed_attention: this in inspired by the implementation at the Autogluon library. When using &quot;fixed attention&quot;, the key and query matrices are not the result of any projection of the input tensor $X$, but learnable matrices (referred as fixed_key and fixed_query) defined separately, as you instantiate the model. fixed_attention does not affect how the Value matrix is computed. . Let me go through an example with numbers to clarify things. Let&#39;s assume we have a dataset with 5 categorical columns that will be encoded by embeddings of dim 4 and we use a batch size (bsz) of 6. Figure 7 shows how the key matrix will be computed for a given batch (same applies to the query matrix) with and without fixed attention. . . Fig 7. Key matrix computation for a given batch with and without fixed attention (same applies to the query matrix). The different color tones in the matrices are my attempt to illustrate that, while without fixed attention the key matrix can have different values anywhere in the matrix, with fixed attention the key matrix is the result of the repetition of the &quot;fixed-key&quot; bsz times. The input projected layer is, of course, broadcasted aong the bsz dimension in the upper panel. . As I mentioned, this implementation is inspired by that at the Autogluon library. Since the guys at Amazon are the ones that came up with the TabTransformer, is only logical to think that they might have found a purpose for this implementation of attention, at least in some cases. However, at the time of writing such purpose is not 100% clear to me. It is known that, in problems like machine translation, most attention heads learn redundant patterns (see e.g. Alessandro Raganato et al., 2020 [4] and references therein). Therefore, maybe the fixed attention mechanism discussed here helps reducing redundancy for problems involving tabular data. . Overall, the way I interpret it, in layman&#39;s terms, is the following: when using fixed attention, the Key and the Query matrices are defined as the model is instantiated, and do not know of the input until the attention weights (attn_weights) are multiplied by the value matrix to obtain what I refer as attn_score in figure 5. Those attention weights, which are in essence the result of a matrix multiplication between the key and the query matrices (plus softmax and normalization), are going to be the same for all the heads, for all samples in a given batch. Therefore, my interpretation is that when using fixed attention, we reduce the attention capabilities of the transformer, which will focus on less aspects of the inputs, reducing potential redundancies. . | . Anyway, enough speculation. Time to have a look to the code. Note that, since we are going to stack the embeddings (instead of concatenating them) they all must have the same dimensions. Such dimension is set as we build the model instead that at the pre-processing stage. To avoid input format conflicts we use the for_tabtransformer parameter at pre-processing time. . embed_cols = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital_status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;] tab_preprocessor = TabPreprocessor( embed_cols=embed_cols, continuous_cols=cont_cols, for_tabtransformer=True) X_tab = tab_preprocessor.fit_transform(adult_train) . /Users/javier/.pyenv/versions/3.7.9/envs/wdposts/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . from pytorch_widedeep.models import TabTransformer tabtransformer = TabTransformer( column_idx=tab_preprocessor.column_idx, embed_input=tab_preprocessor.embeddings_input, continuous_cols=cont_cols, shared_embed=True, num_blocks=3, ) model = WideDeep(deeptabular=tabtransformer) model . /Users/javier/.pyenv/versions/3.7.9/envs/wdposts/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . WideDeep( (deeptabular): Sequential( (0): TabTransformer( (embed_layers): ModuleDict( (emb_layer_education): SharedEmbeddings( (embed): Embedding(17, 32, padding_idx=0) (dropout): Dropout(p=0.1, inplace=False) ) (emb_layer_marital_status): SharedEmbeddings( (embed): Embedding(8, 32, padding_idx=0) (dropout): Dropout(p=0.1, inplace=False) ) (emb_layer_occupation): SharedEmbeddings( (embed): Embedding(16, 32, padding_idx=0) (dropout): Dropout(p=0.1, inplace=False) ) (emb_layer_race): SharedEmbeddings( (embed): Embedding(6, 32, padding_idx=0) (dropout): Dropout(p=0.1, inplace=False) ) (emb_layer_relationship): SharedEmbeddings( (embed): Embedding(7, 32, padding_idx=0) (dropout): Dropout(p=0.1, inplace=False) ) (emb_layer_workclass): SharedEmbeddings( (embed): Embedding(10, 32, padding_idx=0) (dropout): Dropout(p=0.1, inplace=False) ) ) (blks): Sequential( (block0): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) (block1): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) (block2): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) ) (tab_transformer_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Linear(in_features=196, out_features=784, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.1, inplace=False) ) (dense_layer_1): Sequential( (0): Linear(in_features=784, out_features=392, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.1, inplace=False) ) ) ) ) (1): Linear(in_features=392, out_features=1, bias=True) ) ) . Let&#39;s walk through the model. Initially, and as always, we have the embeddings that will represent the categorical columns. The only particular aspect in this model is that the embeddings are of class SharedEmbeddings, which I described before. These embeddings are stacked and passed through three transformer blocks. The output for all the categorical columns is concatenated, resulting in a tensor of dim (bsz, 192) where 192 is equal to the number of categorical columns (6) times the embedding dim (32). This tensor is then concatenated with the &quot;layernormed&quot; continuous columns, resulting in a tensor of dim (bsz, 196). As usual, this tensor goes through an MLP and &quot;off we go&quot;. . To run it, the code is, as one might expect identical to the one shown before for the TabMlp and TabRenset. . trainer = Trainer(model, objective=&quot;binary&quot;, metrics=[(Accuracy)]) trainer.fit(X_tab=X_tab, target=target, n_epochs=5, batch_size=256, val_split=0.2) . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:11&lt;00:00, 10.76it/s, loss=0.375, metrics={&#39;acc&#39;: 0.8216}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 36.87it/s, loss=0.358, metrics={&#39;acc&#39;: 0.8228}] epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:09&lt;00:00, 12.54it/s, loss=0.353, metrics={&#39;acc&#39;: 0.837}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 32.90it/s, loss=0.36, metrics={&#39;acc&#39;: 0.8347}] epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:10&lt;00:00, 11.74it/s, loss=0.349, metrics={&#39;acc&#39;: 0.8379}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 33.19it/s, loss=0.368, metrics={&#39;acc&#39;: 0.8343}] epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:09&lt;00:00, 12.61it/s, loss=0.346, metrics={&#39;acc&#39;: 0.8383}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 38.99it/s, loss=0.373, metrics={&#39;acc&#39;: 0.834}] epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:09&lt;00:00, 13.07it/s, loss=0.344, metrics={&#39;acc&#39;: 0.8395}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:00&lt;00:00, 37.73it/s, loss=0.373, metrics={&#39;acc&#39;: 0.8355}] . &#160;2. Conclusion and future work . In this post my intention was to illustrate how one can use pytorch-widedeep as a library for &quot;standard DL for tabular data&quot;, i.e. without building wide and deep models and for problems that do not involve text and/or images (if you wanted to learn more about the library please visit the repo, the documentation, or the previous posts). To that aim the only component that we need is the deeptabular component, for which pytorch-widedeep comes with 3 models implemented &quot;out of the box&quot;: TabMlp, TabResnet and TabTransformer. In this post I have explained their architecture in detail and how to use them within the library. In the no-so-distant future I intend to implement TabNet [5], as well as performing a proper benchmarking exercise so I can set robust defaults and then release version 1.0. Of course, you can help me by using the package in your datasets üôÇ. If you found this post useful and you like the library, please give a star to the repo. Other than that, happy coding. . &#160;3. References . [1] TabTransformer: Tabular Data Modeling Using Contextual Embeddings. Xin Huang, Ashish Khetan, Milan Cvitkovic, Zohar Karnin, 2020. arXiv:2012.06678v1 . [2] Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, et al., 2017. arXiv:1706.03762v5 . [3] A Mathematical Theory of Attention, James Vuckovic, Aristide Baratin, Remi Tachet des Combes, 2020. arXiv:2007.02876v2 . [4] Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation. Alessandro Raganato, Yves Scherrer, J√∂rg Tiedemann, 2020. arXiv:2002.10260v3 . [5] TabNet: Attentive Interpretable Tabular Learning, Sercan O. Arik, Tomas Pfister, arXiv:1908.07442v5 .",
            "url": "https://jrzaurin.github.io/infinitoml/2021/02/18/pytorch-widedeep_iii.html",
            "relUrl": "/2021/02/18/pytorch-widedeep_iii.html",
            "date": " ‚Ä¢ Feb 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "pytorch-widedeep, deep learning for tabular data II: advanced use",
            "content": "This is the second of a series of posts introducing pytorch-widedeep, a flexible package to combine tabular data with text and images (that could also be used for &quot;standard&quot; tabular data alone). . In the first post I described pytorch-widedeep&#39;s data preprocessing utilities, the main components of a WideDeep model and a quick example to illustrate the basic use of the library. In this post I will use a series of examples to dig deeper into the many options pytorch-widedeep offers as we build wide and deep models. . 1. Binary classification with varying parameters . Let&#39;s start by using again the adult census dataset. . Before moving any further, let me emphasize that, as we go through the examples, one should not pay excessive (or any) attention to the loss or the metrics in the sense that the input parameters are not selected to obtain &quot;state of the art&quot;, but to illustrate usability. . A proper benchmarking exercise will be carried out in a future post. Having said that, and without further ado, let&#39;s start. . #collapse-hide import pandas as pd import numpy as np adult = pd.read_csv(&quot;data/adult/adult.csv.zip&quot;) adult.columns = [c.replace(&quot;-&quot;, &quot;_&quot;) for c in adult.columns] adult[&quot;income_label&quot;] = (adult[&quot;income&quot;].apply(lambda x: &quot;&gt;50K&quot; in x)).astype(int) adult.drop(&quot;income&quot;, axis=1, inplace=True) for c in adult.columns: if adult[c].dtype == &#39;O&#39;: adult[c] = adult[c].apply(lambda x: &quot;unknown&quot; if x == &quot;?&quot; else x) adult[c] = adult[c].str.lower() . . adult.head() . age workclass fnlwgt education educational_num marital_status occupation relationship race gender capital_gain capital_loss hours_per_week native_country income_label . 0 25 | private | 226802 | 11th | 7 | never-married | machine-op-inspct | own-child | black | male | 0 | 0 | 40 | united-states | 0 | . 1 38 | private | 89814 | hs-grad | 9 | married-civ-spouse | farming-fishing | husband | white | male | 0 | 0 | 50 | united-states | 0 | . 2 28 | local-gov | 336951 | assoc-acdm | 12 | married-civ-spouse | protective-serv | husband | white | male | 0 | 0 | 40 | united-states | 1 | . 3 44 | private | 160323 | some-college | 10 | married-civ-spouse | machine-op-inspct | husband | black | male | 7688 | 0 | 40 | united-states | 1 | . 4 18 | unknown | 103497 | some-college | 10 | never-married | unknown | own-child | white | female | 0 | 0 | 30 | united-states | 0 | . if you read the first post you will be familiar with the code below: . import torch from pytorch_widedeep import Trainer from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor from pytorch_widedeep.models import Wide, TabMlp, TabResnet, WideDeep from pytorch_widedeep.metrics import Accuracy, Recall wide_cols = [&#39;education&#39;, &#39;relationship&#39;,&#39;workclass&#39;,&#39;occupation&#39;,&#39;native_country&#39;,&#39;gender&#39;] crossed_cols = [(&#39;education&#39;, &#39;occupation&#39;), (&#39;native_country&#39;, &#39;occupation&#39;)] cat_embed_cols = [(&#39;education&#39;,32), (&#39;relationship&#39;,32), (&#39;workclass&#39;,32), (&#39;occupation&#39;,32),(&#39;native_country&#39;,32)] continuous_cols = [&quot;age&quot;,&quot;hours_per_week&quot;] target_col = &#39;income_label&#39; # TARGET target = adult[target_col].values # WIDE wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols) X_wide = wide_preprocessor.fit_transform(adult) # DEEP tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols) X_tab = tab_preprocessor.fit_transform(adult) . wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1) # We can add dropout and batchnorm to the dense layers, as well as chose the order of the operations deeptabular = TabMlp(column_idx=tab_preprocessor.column_idx, mlp_hidden_dims=[64,32], mlp_dropout=[0.5, 0.5], mlp_batchnorm=True, mlp_linear_first = True, embed_input=tab_preprocessor.embeddings_input, continuous_cols=continuous_cols) model = WideDeep(wide=wide, deeptabular=deeptabular) . Let&#39;s have a look to the model that we will be running: . model . WideDeep( (wide): Wide( (wide_linear): Embedding(797, 1, padding_idx=0) ) (deeptabular): Sequential( (0): TabMlp( (embed_layers): ModuleDict( (emb_layer_education): Embedding(17, 32, padding_idx=0) (emb_layer_native_country): Embedding(43, 32, padding_idx=0) (emb_layer_occupation): Embedding(16, 32, padding_idx=0) (emb_layer_relationship): Embedding(7, 32, padding_idx=0) (emb_layer_workclass): Embedding(10, 32, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (tab_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Linear(in_features=162, out_features=64, bias=False) (1): ReLU(inplace=True) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) ) (dense_layer_1): Sequential( (0): Linear(in_features=64, out_features=32, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) ) ) ) ) (1): Linear(in_features=32, out_features=1, bias=True) ) ) . Now we will define the set up for each model component, including optimizers, learning rate schedulers and initializers: . from pytorch_widedeep.initializers import KaimingNormal, XavierNormal from pytorch_widedeep.callbacks import ModelCheckpoint, LRHistory, EarlyStopping from pytorch_widedeep.optim import RAdam . # Optimizers wide_opt = torch.optim.Adam(model.wide.parameters(), lr=0.03) deep_opt = RAdam(model.deeptabular.parameters(), lr=0.01) # LR Schedulers wide_sch = torch.optim.lr_scheduler.StepLR(wide_opt, step_size=3) deep_sch = torch.optim.lr_scheduler.StepLR(deep_opt, step_size=5) # Component-dependent settings as Dict optimizers = {&#39;wide&#39;: wide_opt, &#39;deeptabular&#39;:deep_opt} schedulers = {&#39;wide&#39;: wide_sch, &#39;deeptabular&#39;:deep_sch} initializers = {&#39;wide&#39;: KaimingNormal, &#39;deeptabular&#39;:XavierNormal} # General settings as List callbacks = [LRHistory(n_epochs=10), EarlyStopping, ModelCheckpoint(filepath=&#39;model_weights/wd_out&#39;)] metrics = [Accuracy, Recall] . Build the trainer and fit! . trainer = Trainer(model, objective=&#39;binary&#39;, optimizers=optimizers, lr_schedulers=schedulers, initializers=initializers, callbacks=callbacks, metrics=metrics, verbose=0, ) . trainer.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=10, batch_size=256, val_split=0.2) . #collapse-hide import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns sns.set() . . /Users/javier/.pyenv/versions/3.7.9/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject return f(*args, **kwds) . #collapse-hide plt.figure(figsize=(15,8)) plt.subplot(2,2,1) plt.plot(trainer.history[&#39;train_loss&#39;], label=&quot;train&quot;) plt.plot(trainer.history[&#39;val_loss&#39;], label=&quot;val&quot;) plt.legend(fontsize=13) plt.xlabel(&quot;n epochs&quot;, fontsize=13) plt.ylabel(&quot;Loss&quot;, fontsize=13) plt.subplot(2,2,2) plt.plot(trainer.history[&#39;train_acc&#39;], label=&quot;train&quot;) plt.plot(trainer.history[&#39;val_acc&#39;], label=&quot;val&quot;) plt.legend(fontsize=13) plt.xlabel(&quot;n epochs&quot;, fontsize=13) plt.ylabel(&quot;Accuracy&quot;, fontsize=13) plt.subplot(2,2,3) plt.plot(trainer.lr_history[&#39;lr_wide_0&#39;], label=&quot;wide&quot;) plt.plot(trainer.lr_history[&#39;lr_deeptabular_0&#39;], label=&quot;deeptabular&quot;) plt.legend(fontsize=13) plt.xlabel(&quot;n epochs&quot;, fontsize=13) plt.ylabel(&quot;learning rate&quot;, fontsize=13) . . Text(0, 0.5, &#39;learning rate&#39;) . As we can see from the plots, the learning rate effectively decreases by a factor of 0.1 (the default) after the corresponding step_size for each component. Note that the keys in the model.lr_history dictionary have a suffix _0. This is because if you pass different parameter groups to the torch optimizers, these will also be recorded. We&#39;ll see this in the regression example later in the post. . Before I move to the next section let me just mention that the WideDeep class comes with a useful method to &quot;rescue&quot; the learned embeddings, very creatively called get_embeddings. For example, let&#39;s say I want to use the embeddings learned for the different levels of the categorical feature education. These can be access via: . education_embed = trainer.get_embeddings( col_name=&#39;education&#39;, cat_encoding_dict=tab_preprocessor.label_encoder.encoding_dict ) education_embed[&#39;doctorate&#39;] . array([ 0.41479743, 0.08521606, 0.2710749 , -0.17924106, -0.07241581, -0.2514616 , -0.24809864, -0.20624267, -0.12701468, -0.00737057, -0.17397854, 0.03000254, -0.06039784, 0.28008303, -0.35625017, 0.00706905, 0.18486224, -0.05701892, -0.05574326, -0.08269893, -0.15482767, 0.30681178, -0.23743518, 0.08368678, 0.20123835, 0.30058601, -0.15073103, -0.08352864, 0.07049613, -0.28594372, -0.05307232, -0.17094977], dtype=float32) . 2. Using the Focal Loss . The Focal loss (hereafter FL) was introduced by Tsung-Yi Lin et al., in their 2018 paper ‚ÄúFocal Loss for Dense Object Detection‚Äù [1]. It is designed to address scenarios with extreme imbalanced classes, such as one-stage object detection where the imbalance between foreground and background classes can be, for example, 1:1000. . The adult census dataset is not really imbalanced, therefore is not the best dataset to test the performance of the FL. Nonetheless, let me illustrate how easy is to use the FL with pytorch-widedeep. . model = WideDeep(wide=wide, deeptabular=deeptabular) . trainer = Trainer( model, objective=&quot;binary_focal_loss&quot;, optimizers=optimizers, lr_schedulers=schedulers, initializers=initializers, callbacks=callbacks, metrics=metrics, alpha=0.2, # the alpha parameter of the focal loss gamma=1.0, # the gamma parameter of the focal loss verbose=False ) . trainer.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=2, batch_size=256, val_split=0.2) . To learn more about the losses available at pytorch-widedeep have a look at the losses module in the library or the docs. . 3. Regression combining tabular data, text and images . For this example we will use a small sample (so you can run it locally in a laptop) of the Airbnb listings dataset in London. . In case you are interested in all details, I did prepared the original dataset for this post, and all the code can be found at the airbnb_data_preprocessing.py, here. After such preprocessing the data looks like this: . #collapse-hide airbnb = pd.read_csv(&#39;data/airbnb/airbnb_sample.csv&#39;) . . airbnb.head(1) . id host_id description host_listings_count host_identity_verified neighbourhood_cleansed latitude longitude is_location_exact property_type room_type accommodates bathrooms bedrooms beds guests_included minimum_nights instant_bookable cancellation_policy has_house_rules host_gender accommodates_catg guests_included_catg minimum_nights_catg host_listings_count_catg bathrooms_catg bedrooms_catg beds_catg amenity_24-hour_check-in amenity__toilet amenity_accessible-height_bed amenity_accessible-height_toilet amenity_air_conditioning amenity_air_purifier amenity_alfresco_bathtub amenity_amazon_echo amenity_baby_bath amenity_baby_monitor amenity_babysitter_recommendations amenity_balcony amenity_bath_towel amenity_bathroom_essentials amenity_bathtub amenity_bathtub_with_bath_chair amenity_bbq_grill amenity_beach_essentials amenity_beach_view amenity_beachfront amenity_bed_linens amenity_bedroom_comforts ... amenity_roll-in_shower amenity_room-darkening_shades amenity_safety_card amenity_sauna amenity_self_check-in amenity_shampoo amenity_shared_gym amenity_shared_hot_tub amenity_shared_pool amenity_shower_chair amenity_single_level_home amenity_ski-in_ski-out amenity_smart_lock amenity_smart_tv amenity_smoke_detector amenity_smoking_allowed amenity_soaking_tub amenity_sound_system amenity_stair_gates amenity_stand_alone_steam_shower amenity_standing_valet amenity_steam_oven amenity_stove amenity_suitable_for_events amenity_sun_loungers amenity_table_corner_guards amenity_tennis_court amenity_terrace amenity_toilet_paper amenity_touchless_faucets amenity_tv amenity_walk-in_shower amenity_warming_drawer amenity_washer amenity_washer_dryer amenity_waterfront amenity_well-lit_path_to_entrance amenity_wheelchair_accessible amenity_wide_clearance_to_shower amenity_wide_doorway_to_guest_bathroom amenity_wide_entrance amenity_wide_entrance_for_guests amenity_wide_entryway amenity_wide_hallways amenity_wifi amenity_window_guards amenity_wine_cooler security_deposit extra_people yield . 0 13913.jpg | 54730 | My bright double bedroom with a large window has a relaxed feeling! It comfortably fits one or t... | 4.0 | f | Islington | 51.56802 | -0.11121 | t | apartment | private_room | 2 | 1.0 | 1.0 | 0.0 | 1 | 1 | f | moderate | 1 | female | 2 | 1 | 1 | 3 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 100.0 | 15.0 | 12.0 | . 1 rows √ó 223 columns . Let&#39;s define what will go through the wide and deep components . # There are a number of columns that are already binary. Therefore, no need to one hot encode them crossed_cols = [(&#39;property_type&#39;, &#39;room_type&#39;)] already_dummies = [c for c in airbnb.columns if &#39;amenity&#39; in c] + [&#39;has_house_rules&#39;] wide_cols = [&#39;is_location_exact&#39;, &#39;property_type&#39;, &#39;room_type&#39;, &#39;host_gender&#39;, &#39;instant_bookable&#39;] + already_dummies cat_embed_cols = [(c, 16) for c in airbnb.columns if &#39;catg&#39; in c] + [(&#39;neighbourhood_cleansed&#39;, 64), (&#39;cancellation_policy&#39;, 16)] continuous_cols = [&#39;latitude&#39;, &#39;longitude&#39;, &#39;security_deposit&#39;, &#39;extra_people&#39;] # it does not make sense to standarised Latitude and Longitude. Here I am going to &quot;pass&quot; but you # might want to check the LatLongScalarEnc available in the autogluon tabular library. already_standard = [&#39;latitude&#39;, &#39;longitude&#39;] # text and image colnames text_col = &#39;description&#39; img_col = &#39;id&#39; # path to pretrained word embeddings and the images word_vectors_path = &#39;data/glove.6B/glove.6B.100d.txt&#39; img_path = &#39;data/airbnb/property_picture&#39; # target target_col = &#39;yield&#39; . Note the following: columns that are already dummies (defined as already_dummies), are treated as any other wide column. Internally, nothing will really happen to them. They will just add one entry to the embedding lookup table. . On the other hand, you will see that among the columns that will be passed through the deeptabular component we have already_standard columns, which are longitude and latitude in this case. These are columns for which it makes no sense to standardize them via sklearn&#39;s StandardScaler, which is what TabPreprocessor uses internally. A solution would be to pre-process them before-hand (using for example the LatLongScalarEnc available at the autogluon library) and then pass them to the TabPreprocessor. . Nonetheless, in this case I am going to &quot;ignore&quot; this issue and move on since I just want to illustrate the use of the package. . import os import torch from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor, TextPreprocessor, ImagePreprocessor from pytorch_widedeep.models import Wide, TabMlp, DeepText, DeepImage, WideDeep from pytorch_widedeep.initializers import * from pytorch_widedeep.callbacks import * . target = airbnb[target_col].values wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols) X_wide = wide_preprocessor.fit_transform(airbnb) tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols) X_tab = tab_preprocessor.fit_transform(airbnb) text_preprocessor = TextPreprocessor(word_vectors_path=word_vectors_path, text_col=text_col) X_text = text_preprocessor.fit_transform(airbnb) image_processor = ImagePreprocessor(img_col = img_col, img_path = img_path) X_images = image_processor.fit_transform(airbnb) . The vocabulary contains 2192 tokens Indexing word vectors... Loaded 400000 word vectors Preparing embeddings matrix... 2175 words in the vocabulary had data/glove.6B/glove.6B.100d.txt vectors and appear more than 5 times Reading Images from data/airbnb/property_picture . 4%|‚ñé | 36/1001 [00:00&lt;00:02, 346.67it/s] . Resizing . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1001/1001 [00:02&lt;00:00, 372.15it/s] . Computing normalisation metrics . At this stage the data is ready to be passed through the model. However, instead of building a &quot;simple&quot; model that collects the wide, deeptabular, deeptext and deepimage component, I am going to use this opportunity to illustrate pytorch-widedepp&#39;s flexibility to build wide and deep models. I like to call this, getting into Kaggle mode. . First we define the components of the model... . wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1) # deeptabular: 2 Dense layers deeptabular = TabMlp( column_idx = tab_preprocessor.column_idx, mlp_hidden_dims=[128,64], mlp_dropout = 0.1, mlp_batchnorm = True, embed_input=tab_preprocessor.embeddings_input, embed_dropout = 0.1, continuous_cols = continuous_cols, batchnorm_cont = True ) # deeptext: a stack of 2 LSTMs deeptext = DeepText( vocab_size=len(text_preprocessor.vocab.itos), hidden_dim=64, n_layers=2, rnn_dropout=0.5, embed_matrix=text_preprocessor.embedding_matrix) # Pretrained Resnet 18 (default is all but last 2 conv blocks frozen) plus a FC-Head 512-&gt;256-&gt;128 deepimage = DeepImage(pretrained=True, head_hidden_dims=[512, 256, 128]) . ...and, as we build the model, add a fully connected head via the input parameters (could also be used via the additional component/parameter deephead) . model = WideDeep( wide=wide, deeptabular=deeptabular, deeptext=deeptext, deepimage=deepimage, head_hidden_dims=[128, 64] ) . Let&#39;s have a look to the model . model . WideDeep( (wide): Wide( (wide_linear): Embedding(357, 1, padding_idx=0) ) (deeptabular): TabMlp( (embed_layers): ModuleDict( (emb_layer_accommodates_catg): Embedding(4, 16, padding_idx=0) (emb_layer_bathrooms_catg): Embedding(4, 16, padding_idx=0) (emb_layer_bedrooms_catg): Embedding(5, 16, padding_idx=0) (emb_layer_beds_catg): Embedding(5, 16, padding_idx=0) (emb_layer_cancellation_policy): Embedding(6, 16, padding_idx=0) (emb_layer_guests_included_catg): Embedding(4, 16, padding_idx=0) (emb_layer_host_listings_count_catg): Embedding(5, 16, padding_idx=0) (emb_layer_minimum_nights_catg): Embedding(4, 16, padding_idx=0) (emb_layer_neighbourhood_cleansed): Embedding(33, 64, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (norm): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (tab_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): BatchNorm1d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.1, inplace=False) (2): Linear(in_features=196, out_features=128, bias=False) (3): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=128, out_features=64, bias=True) (2): ReLU(inplace=True) ) ) ) ) (deeptext): DeepText( (word_embed): Embedding(2192, 100, padding_idx=1) (rnn): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.5) ) (deepimage): DeepImage( (backbone): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (8): AdaptiveAvgPool2d(output_size=(1, 1)) ) (imagehead): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=512, out_features=256, bias=True) (2): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=256, out_features=128, bias=True) (2): ReLU(inplace=True) ) ) ) ) (deephead): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=256, out_features=128, bias=True) (2): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=128, out_features=64, bias=True) (2): ReLU(inplace=True) ) ) (head_out): Linear(in_features=64, out_features=1, bias=True) ) ) . This is a big model, so let me go component by component. . wide: simple linear model implemented via an Embedding layer . | deeptabular: embeddings concatenated to categorical columns that are then passed through two dense layers with the following sizes [196 $ rightarrow$ 128 $ rightarrow$ 64]. . | deeptext: two stacked LTSMs that will received the pre-trained glove wordvectors and output a last hidden state of dim 64 (this would be 128 if we had used bidirectional = True) . | deepimage: a pre-trained ResNet 18 model where only the last Sequential block (7) will be trained. The rest will remain &quot;frozen&quot;. on top of it we have imagehead which is just a Sequential model comprised of two dense layers with the following sizes [512 $ rightarrow$ 256 $ rightarrow$ 128] . | deephead: on top of the 3 deep components we have a final component referred as deephead. This component will receive the concatenated output from all the deep components, and pass it through a further collection of dense layers. In this case the sizes are [256 $ rightarrow$ 64 $ rightarrow$ 1]. We input 256 because the output dim from deeptabular is 64, the output dim from deeptext is 64 and the output dim from deepimage is 128. The final deephead output dim is 1 because we are performing a regression, i.e. one output neuron with no activation function. . | Let&#39;s go even a step further and use different optimizers, initializers and schedulers for different components. Moreover, let&#39;s use a different learning rate for different parameter groups in the case of the deeptabular, remember, this is Kaggle mode. . #¬†Optimizers. Different parameter groups for the deeptabular component will use different lr tab_params = [] for childname, child in model.named_children(): if childname == &#39;deeptabular&#39;: for n,p in child.named_parameters(): if &quot;emb_layer&quot; in n: tab_params.append({&#39;params&#39;: p, &#39;lr&#39;: 0.01}) else: tab_params.append({&#39;params&#39;: p, &#39;lr&#39;: 0.03}) wide_opt = torch.optim.Adam(model.wide.parameters(), lr=0.03) tab_opt = torch.optim.Adam(tab_params) text_opt = RAdam(model.deeptext.parameters()) img_opt = RAdam(model.deepimage.parameters()) head_opt = torch.optim.AdamW(model.deephead.parameters()) optimizers = {&#39;wide&#39;: wide_opt, &#39;deeptabular&#39;:tab_opt, &#39;deeptext&#39;:text_opt, &#39;deepimage&#39;: img_opt, &#39;deephead&#39;: head_opt} # schedulers wide_sch = torch.optim.lr_scheduler.StepLR(wide_opt, step_size=5) deep_sch = torch.optim.lr_scheduler.MultiStepLR(tab_opt, milestones=[3,8]) text_sch = torch.optim.lr_scheduler.StepLR(text_opt, step_size=5) img_sch = torch.optim.lr_scheduler.MultiStepLR(tab_opt, milestones=[3,8]) head_sch = torch.optim.lr_scheduler.StepLR(head_opt, step_size=5) schedulers = {&#39;wide&#39;: wide_sch, &#39;deeptabular&#39;:deep_sch, &#39;deeptext&#39;:text_sch, &#39;deepimage&#39;: img_sch, &#39;deephead&#39;: head_sch} # initializers initializers = {&#39;wide&#39;: KaimingNormal, &#39;deeptabular&#39;:KaimingNormal, &#39;deeptext&#39;:KaimingNormal(pattern=r&quot;^(?!.*word_embed).*$&quot;), # do not initialize the pre-trained word-vectors! &#39;deepimage&#39;:KaimingNormal} # transforms and callbacks mean = [0.406, 0.456, 0.485] #BGR std = [0.225, 0.224, 0.229] #BGR transforms = [ToTensor, Normalize(mean=mean, std=std)] callbacks = [LRHistory(n_epochs=10), EarlyStopping, ModelCheckpoint(filepath=&#39;model_weights/wd_out&#39;)] . Note that, since we will use pre-trained word embeddings, we do not want to initialize these embeddings. However you might still want to initialize the other layers in the deeptext component. This is not a problem, you can do that with the parameter pattern and your knowledge on regular expressions. In the deeptext initializer definition above: . KaimingNormal(pattern=r&quot;^(?!.*word_embed).*$&quot;) . I am NOT initializing parameters whose name contains the string word_embed. . So...let&#39;s compile and run, which is as easy as: . trainer = Trainer(model, objective=&quot;regression&quot;, initializers=initializers, optimizers=optimizers, lr_schedulers=schedulers, callbacks=callbacks, transforms=transforms) . trainer.fit(X_wide=X_wide, X_tab=X_tab, X_text=X_text, X_img=X_images, target=target, n_epochs=1, batch_size=32, val_split=0.2) . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [02:11&lt;00:00, 5.28s/it, loss=1.27e+4] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:15&lt;00:00, 2.25s/it, loss=9.2e+3] . As I mentioned early in the post, please, do not focus on the success metric/loss (mse in this case). I am just using a very small sample of the dataset and some &quot;random&quot; set up. I just want to illustrate usability. A benchmark post will come in the &quot;no-so-distant future&quot;. . 4. Finetune/Warmup routines . Let&#39;s place ourselves in two possible scenarios. . Let&#39;s assume we have run a model and we want to just transfer the learnings (you know...transfer-learning) to another dataset, or simply we have received new data and we do not want to start the training of each component from scratch. Simply, we want to load the pre-trained weights and fine-tune. . | Or, we just want to &quot;warm up&quot; individual model components individually before the joined training begins. . | This can be done with the finetune set of parameters (aliased all as warmup parameters if you wanted). There are 3 fine-tuning routines: . Fine-tune all trainable layers at once with a triangular one-cycle learning rate (referred as slanted triangular learning rates in Howard &amp; Ruder 2018) . | Gradual fine-tuning inspired by the work of Felbo et al., 2017 [2] . | Gradual fine-tuning based on the work of Howard &amp; Ruder 2018 [3] . | Currently fine-tunning is only supported without a fully connected head, i.e. if deephead=None. In addition, Felbo and Howard routines apply only, of course, to the deeptabular, deeptext and deepimagemodels. The wide component can also be fine-tuned, but only in an &quot;all at once&quot; mode. . Let me briefly describe the &quot;Felbo&quot; and &quot;Howard&quot; routines before showing how to use them. . 4.1 The Felbo finetune routine . The Felbo fine-tune routine can be illustrated by the following figure: . . Figure 1. The figure can be described as follows: fine-tune (or train) the last layer for one epoch using a one cycle triangular learning rate. Then fine-tune the next deeper layer for one epoch, with a learning rate that is a factor of 2.5 lower than the previous learning rate (the 2.5 factor is fixed) while freezing the already warmed up layer(s). Repeat untill all individual layers are warmed. Then warm one last epoch with all warmed layers trainable. The vanishing color gradient in the figure attempts to illustrate the decreasing learning rate. . Note that this is not identical to the Fine-Tunning routine described in Felbo et al, 2017, this is why I used the word &#39;inspired&#39;. . 4.2 The Howard finetune routine . The Howard routine can be illustrated by the following figure: . . Figure 2. The figure can be described as follows: fine-tune (or train) the last layer for one epoch using a one cycle triangular learning rate. Then fine-tune the next deeper layer for one epoch, with a learning rate that is a factor of 2.5 lower than the previous learning rate (the 2.5 factor is fixed) while keeping the already warmed up layer(s) trainable. Repeat. The vanishing color gradient in the figure attempts to illustrate the decreasing learning rate. . Note that I write &quot;fine-tune (or train) the last layer for one epoch [...]&quot;. However, in practice the user will have to specify the order of the layers to be fine-tuned. This is another reason why I wrote that the fine-tune routines I have implemented are inspired by the work of Felbo and Howard and not identical to their implemenations. . The felbo and howard routines can be accessed with via the finetune parameters (aliased as warmup parameters in case the user wants to use consistent naming). Let me go back to the adult dataset and let&#39;s have a look: . wide_cols = [&#39;education&#39;, &#39;relationship&#39;,&#39;workclass&#39;,&#39;occupation&#39;,&#39;native_country&#39;,&#39;gender&#39;] crossed_cols = [(&#39;education&#39;, &#39;occupation&#39;), (&#39;native_country&#39;, &#39;occupation&#39;)] cat_embed_cols = [(&#39;education&#39;,32), (&#39;relationship&#39;,32), (&#39;workclass&#39;,32), (&#39;occupation&#39;,32),(&#39;native_country&#39;,32)] continuous_cols = [&quot;age&quot;,&quot;hours_per_week&quot;] target_col = &#39;income_label&#39; # TARGET target = adult[target_col].values # WIDE wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols) X_wide = wide_preprocessor.fit_transform(adult) # DEEP tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols) X_tab = tab_preprocessor.fit_transform(adult) . wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1) deeptabular = TabResnet( blocks_dims=[128, 64, 32], column_idx=tab_preprocessor.column_idx, embed_input=tab_preprocessor.embeddings_input, continuous_cols=continuous_cols) model = WideDeep(wide=wide, deeptabular=deeptabular) . model . WideDeep( (wide): Wide( (wide_linear): Embedding(797, 1, padding_idx=0) ) (deeptabular): Sequential( (0): TabResnet( (embed_layers): ModuleDict( (emb_layer_education): Embedding(17, 32, padding_idx=0) (emb_layer_native_country): Embedding(43, 32, padding_idx=0) (emb_layer_occupation): Embedding(16, 32, padding_idx=0) (emb_layer_relationship): Embedding(7, 32, padding_idx=0) (emb_layer_workclass): Embedding(10, 32, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (tab_resnet): DenseResnet( (dense_resnet): Sequential( (lin1): Linear(in_features=162, out_features=128, bias=True) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (block_0): BasicBlock( (lin1): Linear(in_features=128, out_features=64, bias=True) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True) (dp): Dropout(p=0.1, inplace=False) (lin2): Linear(in_features=64, out_features=64, bias=True) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (resize): Sequential( (0): Linear(in_features=128, out_features=64, bias=True) (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (block_1): BasicBlock( (lin1): Linear(in_features=64, out_features=32, bias=True) (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True) (dp): Dropout(p=0.1, inplace=False) (lin2): Linear(in_features=32, out_features=32, bias=True) (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (resize): Sequential( (0): Linear(in_features=64, out_features=32, bias=True) (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) ) ) (1): Linear(in_features=32, out_features=1, bias=True) ) ) . trainer = Trainer(model, objective=&quot;binary&quot;, metrics=[Accuracy]) . trainer.fit(X_wide=X_wide, X_tab=X_tab, target=target, val_split=0.1, n_epochs=2, batch_size=256) . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:06&lt;00:00, 26.32it/s, loss=0.415, metrics={&#39;acc&#39;: 0.8016}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00&lt;00:00, 74.72it/s, loss=0.364, metrics={&#39;acc&#39;: 0.8044}] epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 172/172 [00:06&lt;00:00, 26.31it/s, loss=0.372, metrics={&#39;acc&#39;: 0.8249}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00&lt;00:00, 76.28it/s, loss=0.356, metrics={&#39;acc&#39;: 0.8256}] . trainer.save_model(&quot;models_dir/model.t&quot;) . Now we are going to fine-tune the model components, and in the case of the deeptabular component, we will fine-tune the resnet-blocks and the linear layer but NOT the embeddings. . For this, we need to access the model component&#39;s children: deeptabular $ rightarrow$ tab_resnet $ rightarrow$ dense_resnet $ rightarrow$ blocks . # you can just load the model as any pytorch model or use the Trainer&#39;s staticmethod `load_model` model = Trainer.load_model(&quot;models_dir/model.t&quot;) . tab_lin_layers = list(model.deeptabular.children())[1] . tab_deep_layers = list( list(list(list(model.deeptabular.children())[0].children())[2].children())[ 0 ].children() )[::-1][:2] . tab_layers = [tab_lin_layers] + tab_deep_layers . tab_layers . [Linear(in_features=32, out_features=1, bias=True), BasicBlock( (lin1): Linear(in_features=64, out_features=32, bias=True) (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True) (dp): Dropout(p=0.1, inplace=False) (lin2): Linear(in_features=32, out_features=32, bias=True) (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (resize): Sequential( (0): Linear(in_features=64, out_features=32, bias=True) (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ), BasicBlock( (lin1): Linear(in_features=128, out_features=64, bias=True) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True) (dp): Dropout(p=0.1, inplace=False) (lin2): Linear(in_features=64, out_features=64, bias=True) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (resize): Sequential( (0): Linear(in_features=128, out_features=64, bias=True) (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) )] . new_trainer = Trainer(model, objective=&quot;binary&quot;, metrics=[Accuracy]) . new_trainer.fit( X_wide=X_wide, X_tab=X_tab, target=target, val_split=0.1, finetune=True, finetune_epochs=2, finetune_deeptabular_gradual=True, finetune_deeptabular_layers = tab_layers, finetune_deeptabular_max_lr = 0.01, n_epochs=2) . 0%| | 0/1374 [00:00&lt;?, ?it/s] . Training wide for 2 epochs . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:09&lt;00:00, 150.31it/s, loss=0.421, metrics={&#39;acc&#39;: 0.7995}] epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:08&lt;00:00, 160.97it/s, loss=0.361, metrics={&#39;acc&#39;: 0.8158}] 0%| | 0/1374 [00:00&lt;?, ?it/s] . Training deeptabular, layer 1 of 3 . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:23&lt;00:00, 58.62it/s, loss=0.385, metrics={&#39;acc&#39;: 0.8172}] 0%| | 0/1374 [00:00&lt;?, ?it/s] . Training deeptabular, layer 2 of 3 . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:26&lt;00:00, 51.08it/s, loss=0.373, metrics={&#39;acc&#39;: 0.8193}] 0%| | 0/1374 [00:00&lt;?, ?it/s] . Training deeptabular, layer 3 of 3 . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:24&lt;00:00, 55.97it/s, loss=0.368, metrics={&#39;acc&#39;: 0.8207}] 0%| | 0/1374 [00:00&lt;?, ?it/s] . Fine-tuning of individual components completed. Training the whole model for 2 epochs . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:33&lt;00:00, 41.35it/s, loss=0.352, metrics={&#39;acc&#39;: 0.8373}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153/153 [00:01&lt;00:00, 113.01it/s, loss=0.35, metrics={&#39;acc&#39;: 0.8368}] epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1374/1374 [00:31&lt;00:00, 43.85it/s, loss=0.344, metrics={&#39;acc&#39;: 0.8398}] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153/153 [00:01&lt;00:00, 129.62it/s, loss=0.348, metrics={&#39;acc&#39;: 0.8395}] . 5. Custom model . So far we have used the components that come with pytorch-widedee. However, as I mentioned in the first post, it is very likely that the user wants to use custom models for the deeptext and deepimage components. This is easily attainable by...well...simply passing your own model. . You should just remember that the model must return the last layer of activations (and NOT the predictions) and must contained an attribute called output_dim with the output dimension of that last layer. . For example, let&#39;s say we want to use as deeptext a very simple stack of 2 bidirectional GRUs. Let&#39;s see how to do such a thing with the airbnb dataset . crossed_cols = [(&#39;property_type&#39;, &#39;room_type&#39;)] already_dummies = [c for c in airbnb.columns if &#39;amenity&#39; in c] + [&#39;has_house_rules&#39;] wide_cols = [&#39;is_location_exact&#39;, &#39;property_type&#39;, &#39;room_type&#39;, &#39;host_gender&#39;, &#39;instant_bookable&#39;] + already_dummies cat_embed_cols = [(c, 16) for c in airbnb.columns if &#39;catg&#39; in c] + [(&#39;neighbourhood_cleansed&#39;, 64), (&#39;cancellation_policy&#39;, 16)] continuous_cols = [&#39;latitude&#39;, &#39;longitude&#39;, &#39;security_deposit&#39;, &#39;extra_people&#39;] already_standard = [&#39;latitude&#39;, &#39;longitude&#39;] text_col = &#39;description&#39; img_col = &#39;id&#39; word_vectors_path = &#39;data/glove.6B/glove.6B.100d.txt&#39; img_path = &#39;data/airbnb/property_picture&#39; target_col = &#39;yield&#39; target = airbnb[target_col].values wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols) X_wide = wide_preprocessor.fit_transform(airbnb) tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols) X_tab = tab_preprocessor.fit_transform(airbnb) text_preprocessor = TextPreprocessor(word_vectors_path=word_vectors_path, text_col=text_col) X_text = text_preprocessor.fit_transform(airbnb) image_processor = ImagePreprocessor(img_col = img_col, img_path = img_path) X_images = image_processor.fit_transform(airbnb) . The vocabulary contains 2192 tokens Indexing word vectors... Loaded 400000 word vectors Preparing embeddings matrix... 2175 words in the vocabulary had data/glove.6B/glove.6B.100d.txt vectors and appear more than 5 times Reading Images from data/airbnb/property_picture . 4%|‚ñç | 39/1001 [00:00&lt;00:02, 389.27it/s] . Resizing . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1001/1001 [00:02&lt;00:00, 381.95it/s] . Computing normalisation metrics . from torch import nn class MyDeepText(nn.Module): def __init__(self, vocab_size, padding_idx=1, embed_dim=100, hidden_dim=64): super(MyDeepText, self).__init__() # word/token embeddings self.word_embed = nn.Embedding( vocab_size, embed_dim, padding_idx=padding_idx ) # stack of RNNs self.rnn = nn.GRU( embed_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True, ) # Remember, this must be defined. If not WideDeep will through an error self.output_dim = hidden_dim * 2 def forward(self, X): embed = self.word_embed(X.long()) o, h = self.rnn(embed) return torch.cat((h[-2], h[-1]), dim=1) . And from here, &quot;proceed as usual&quot; . wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1) deeptabular = TabMlp( mlp_hidden_dims=[64,32], column_idx=tab_preprocessor.column_idx, embed_input=tab_preprocessor.embeddings_input, continuous_cols=continuous_cols ) mydeeptext = MyDeepText(vocab_size=len(text_preprocessor.vocab.itos)) model = WideDeep(wide=wide, deeptabular=deeptabular, deeptext=mydeeptext) . model . WideDeep( (wide): Wide( (wide_linear): Embedding(357, 1, padding_idx=0) ) (deeptabular): Sequential( (0): TabMlp( (embed_layers): ModuleDict( (emb_layer_accommodates_catg): Embedding(4, 16, padding_idx=0) (emb_layer_bathrooms_catg): Embedding(4, 16, padding_idx=0) (emb_layer_bedrooms_catg): Embedding(5, 16, padding_idx=0) (emb_layer_beds_catg): Embedding(5, 16, padding_idx=0) (emb_layer_cancellation_policy): Embedding(6, 16, padding_idx=0) (emb_layer_guests_included_catg): Embedding(4, 16, padding_idx=0) (emb_layer_host_listings_count_catg): Embedding(5, 16, padding_idx=0) (emb_layer_minimum_nights_catg): Embedding(4, 16, padding_idx=0) (emb_layer_neighbourhood_cleansed): Embedding(33, 64, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (tab_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=196, out_features=64, bias=True) (2): ReLU(inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=64, out_features=32, bias=True) (2): ReLU(inplace=True) ) ) ) ) (1): Linear(in_features=32, out_features=1, bias=True) ) (deeptext): Sequential( (0): MyDeepText( (word_embed): Embedding(2192, 100, padding_idx=1) (rnn): GRU(100, 64, num_layers=2, batch_first=True, bidirectional=True) ) (1): Linear(in_features=128, out_features=1, bias=True) ) ) . trainer = Trainer(model, objective=&quot;regression&quot;) . trainer.fit(X_wide=X_wide, X_tab=X_tab, X_text=X_text, target=target, n_epochs=1, batch_size=64, val_split=0.2) . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:03&lt;00:00, 3.77it/s, loss=1.79e+4] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;00:00, 13.34it/s, loss=1.49e+4] . 6. Conclusion . In this second post I tried to illustrate in detail the different functionalities of the pytorch-widedeep package, and how these can be used to customize each of the four potential components of the WideDeep model that can be built with pytorch-widedeep. I have also describe the warm-up routines that can be used to &quot;warm-up&quot; each individual component before the joined training and finally, how custom models, &quot;external&quot; to pytorch-widedeep can be used in combination with the package. . However, this is not the end of the journey. As you will have seen, there is an &quot;imbalance in the pytorch-widedeep force&quot;, in the sense that while fully pre-trained models are incorporated for the deepimage component, this is not the case for the deeptext component, where only pre-trained word embeddings are considered. Of course, as illustrated in Section 4, you could build your own pre-trained deeptext component and pass it to the WideDeep constructor class, but eventually, I want to allow that option within the package. . This means that eventually I will need to integrate the library with some of the pre-trained Language models available or simply code a custom version for pytorch-widedeep. . One the other hand, I want to bring more DL models for the deeptabular components, such as TabNet. There is already a fantastic Pytorch implementation which I highly recommend. . If you made it this far, thanks for reading! And if you use the package, let me know your thoughts! . References . [1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, et al., 2018: Focal Loss for Dense Object Detection. arXiv:1708.02002v2 . [3] Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm. Bjarke Felbo, Alan Mislove, Anders S√∏gaard, et al., 2017. arXiv:1708.00524 . [3] Universal Language Model Fine-tuning for Text Classification. Jeremy Howard, Sebastian Ruder, 2018 arXiv:1801.06146v5 .",
            "url": "https://jrzaurin.github.io/infinitoml/2020/12/11/pytorch-widedeep_ii.html",
            "relUrl": "/2020/12/11/pytorch-widedeep_ii.html",
            "date": " ‚Ä¢ Dec 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "pytorch-widedeep, deep learning for tabular data I: data preprocessing, model components and basic use",
            "content": "This is the first of a series of posts introducing pytorch-widedeep, which is intended to be a flexible package to use Deep Learning (hereafter DL) with tabular data and combine it with text and images via wide and deep models. pytorch-widedeep is partially based on Heng-Tze Cheng et al., 2016 paper [1]. . in this post I describe the data preprocessing functionalities of the library, the main components of the model, and the basic use of the library. In a separate post I will show a more advance use of pytorch-widedeep. . Before I move any further I just want to emphasize that there are a number of libraries that implement functionalities to use DL on tabular data. To cite a few, the ubiquitous and fantastic FastAI (and their tabular api), NVIDIA&#39;s NVTabular, the powerful pytorch-tabnet based on work of Sercan O. Arik and Tomas Pfisterfrom [2], which is starting to take victories in Kaggle competitions, and perhaps my favourite AutoGluon Tabular [3]. . It is not my intention to &quot;compete&quot; against these libraries. pytorch-widedeep started as an attempt to package and automate an algorithm I had to use a couple of times at work and ended up becoming the entertaining process that is building a library. Needless to say that if you wanted to apply DL to tabular data you should go and check all the libraries I mentioned before (as well as this one üôÇ. You can find the source code here)). . 1. Installation . To install the package simply use pip: . pip install pytorch-widedeep . or directly from github . pip install git+https://github.com/jrzaurin/pytorch-widedeep.git . Important note for Mac Users . Note that the following comments are not directly related to the package, but to the interplay between pytorch and OSX (more precisely pytorch&#39;s dependency on OpenMP I believe) and in general parallel processing in Mac. . In the first place, at the time of writing the latest pytorch version is 1.7. This version is known to have some issues when running on Mac and the data-loaders might not run in parallel. . On the other hand, since Python 3.8 the multiprocessing library start method changed from &#39;fork&#39; to &#39;spawn&#39;. This also affects the data-loaders (for any torch version) and they will not run in parallel. . Therefore, for Mac users I suggest using python 3.7 and torch &lt;= 1.6 (with its corresponding torchvision version, i.e. &lt;= 0.7.0). I could have enforced this versioning via the setup.py file. However, there are a number of unknowns and I preferred to leave it as it is. For example I developed the package using macOS Catalina and maybe some of this issues are not present in the new release Big Sur. Also, I hope that they release soon a patch for pytorch 1.7 and some, if not all these problems disappear. . Installing pytorch-widedeep via pip will install the latest version. Therefore, if these problems are present and the dataloaders do not run in parallel, one can easily downgrade manually: . pip install torch==1.6.0 torchvision==0.7.0 . None of these issues affect Linux users . 2. pytorch-widedeep architectures . In general terms, pytorch-widedeep is a package to use deep learning with tabular data. In particular, is intended to facilitate the combination of text and images with corresponding tabular data using wide and deep models. With that in mind there are a number of architectures that can be implemented with just a few lines of code. The main components of those architectures are shown in the Figure below: . . The dashed boxes in the figure represent optional, overall components, and the dashed lines/arrows indicate the corresponding connections, depending on whether or not certain components are present. For example, the dashed, blue-arrows indicate that the deeptabular, deeptext and deepimage components are connected directly to the output neuron or neurons (depending on whether we are performing a binary classification or regression, or a multi-class classification) if the optional deephead is not present. Finally, the components within the faded-pink rectangle are concatenated. . Note that it is not possible to illustrate the number of architectures and components available in pytorch-widedeep in one Figure. This is why I wrote before &quot;overall components&quot;, because within the components represented by the boxes, there are a number of options as well. Therefore, for more details on possible architectures (and more) please, see the documentation, or the Examples folders and the notebooks in the repo. . In math terms, and following the notation in the paper, the expression for the architecture without a deephead component can be formulated as: . $$ preds = sigma(W^{T}_{wide}[x, phi(x)] + W^{T}_{deeptabular}a^{(l_f)}_{dense} + W^{T}_{deeptext}a^{(l_f)}_{text} + W^{T}_{deepimage}a^{(l_f)}_{image} + b) $$Where $W$ are the weight matrices applied to the wide model and to the final activations of the deep models, $a$ are these final activations, and $ phi(x)$ are the cross product transformations of the original features $x$. In case you are wondering what are &quot;cross product transformations&quot;, here is a quote taken directly from the paper: &quot;For binary features, a cross-product transformation (e.g., ‚ÄúAND(gender=female, language=en)‚Äù) is 1 if and only if the constituent features (‚Äúgender=female‚Äù and ‚Äúlanguage=en‚Äù) are all 1, and 0 otherwise&quot;. . While if there is a deephead component, the previous expression turns into: . $$ preds = sigma(W^{T}_{wide}[x, phi(x)] + W^{T}_{deephead}a^{(l_f)}_{deephead} + b) $$It is important to emphasize that each individual component, wide, deeptabular, deeptext and deepimage, can be used independently and in isolation. For example, one could use only wide, which is in simply a linear model. In fact, one of the most interesting offerings in pytorch-widedeep is the deeptabular component, and I intend to write a dedicated post focused on that component alone. . Finally, while I recommend using the wide and deeptabular models in pytorch-widedeep it is very likely that users will want to use their own models for the deeptext and deepimage components. That is perfectly possible as long as the the custom models have an attribute called output_dim with the size of the last layer of activations, so that WideDeep can be constructed. Again, examples on how to use custom components can be found in the Examples folder in the repo. Just in case pytorch-widedeep includes standard text (stack of LSTMs) and image (pre-trained ResNets or stack of CNNs) models. . 3. Quick start (TL;DR) . Maybe I should have started with this section, but I thought that knowing at least the architectures one can build with pytorch-widedeep was &quot;kind-off&quot; necessary. In any case and before diving into the details of the library, let&#39;s just say that you just want to quickly run one example and get the feel of how pytorch-widedeep works. Let&#39;s do so using the adult census dataset. . In this example we will be fitting a model comprised by two components: wide and deeptabular. . #collapse-hide import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score . . #collapse-hide adult = pd.read_csv(&quot;data/adult/adult.csv.zip&quot;) adult.columns = [c.replace(&quot;-&quot;, &quot;_&quot;) for c in adult.columns] adult[&quot;income_label&quot;] = (adult[&quot;income&quot;].apply(lambda x: &quot;&gt;50K&quot; in x)).astype(int) adult.drop(&quot;income&quot;, axis=1, inplace=True) for c in adult.columns: if adult[c].dtype == &#39;O&#39;: adult[c] = adult[c].apply(lambda x: &quot;unknown&quot; if x == &quot;?&quot; else x) adult[c] = adult[c].str.lower() . . adult_train, adult_test = train_test_split(adult, test_size=0.2, stratify=adult.income_label) adult.head() . age workclass fnlwgt education educational_num marital_status occupation relationship race gender capital_gain capital_loss hours_per_week native_country income_label . 0 25 | private | 226802 | 11th | 7 | never-married | machine-op-inspct | own-child | black | male | 0 | 0 | 40 | united-states | 0 | . 1 38 | private | 89814 | hs-grad | 9 | married-civ-spouse | farming-fishing | husband | white | male | 0 | 0 | 50 | united-states | 0 | . 2 28 | local-gov | 336951 | assoc-acdm | 12 | married-civ-spouse | protective-serv | husband | white | male | 0 | 0 | 40 | united-states | 1 | . 3 44 | private | 160323 | some-college | 10 | married-civ-spouse | machine-op-inspct | husband | black | male | 7688 | 0 | 40 | united-states | 1 | . 4 18 | unknown | 103497 | some-college | 10 | never-married | unknown | own-child | white | female | 0 | 0 | 30 | united-states | 0 | . The following lines below is all you need . from pytorch_widedeep import Trainer from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor from pytorch_widedeep.models import Wide, TabMlp, WideDeep from pytorch_widedeep.metrics import Accuracy # define wide, crossed, embedding and continuous columns, and target wide_cols = [&quot;education&quot;, &quot;relationship&quot;, &quot;workclass&quot;, &quot;occupation&quot;, &quot;native_country&quot;, &quot;gender&quot;] cross_cols = [(&quot;education&quot;, &quot;occupation&quot;), (&quot;native_country&quot;, &quot;occupation&quot;)] embed_cols = [(&quot;education&quot;, 32), (&quot;workclass&quot;, 32), (&quot;occupation&quot;, 32), (&quot;native_country&quot;, 32)] cont_cols = [&quot;age&quot;, &quot;hours_per_week&quot;] target = adult_train[&quot;income_label&quot;].values # prepare wide component wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=cross_cols) X_wide = wide_preprocessor.fit_transform(adult_train) wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1) # prepare deeptabular component tab_preprocessor = TabPreprocessor(embed_cols=embed_cols, continuous_cols=cont_cols) X_tab = tab_preprocessor.fit_transform(adult_train) deeptabular = TabMlp( mlp_hidden_dims=[200, 100], column_idx=tab_preprocessor.column_idx, embed_input=tab_preprocessor.embeddings_input, continuous_cols=cont_cols, ) # build, compile and fit model = WideDeep(wide=wide, deeptabular=deeptabular) # Train trainer = Trainer(model, objective=&quot;binary&quot;, metrics=[(Accuracy)]) trainer.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=2, batch_size=256) # predict X_wide_te = wide_preprocessor.transform(adult_test) X_tab_te = tab_preprocessor.transform(adult_test) preds = trainer.predict(X_wide=X_wide_te, X_tab=X_tab_te) . epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153/153 [00:03&lt;00:00, 43.06it/s, loss=0.428, metrics={&#39;acc&#39;: 0.802}] epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153/153 [00:03&lt;00:00, 44.41it/s, loss=0.389, metrics={&#39;acc&#39;: 0.8217}] predict: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:00&lt;00:00, 149.41it/s] . 4. Preprocessors . As you can see in Section 3, and as with any ML algorithm, the data need to be prepared/preprocessed before going through the model. This is handled by the pytorch-widedeep preprocessors. There is one preprocessor per WideDeep model component: . WidePreprocessor TabPreprocessor TextPreprocessor ImagePreprocessor . &quot;Behind the scenes&quot;, these preprocessors use a series of helper functions and classes that are in the utils module. Initially I did not intend to &quot;expose&quot; them to the user, but I believe they can be useful for all sorts of preprocessing tasks, even if they are not related to pytorch-widedeep, so I made them available. The utils tools are: . deep_utils.LabelEncoder text_utils.simple_preprocess text_utils.get_texts text_utils.pad_sequences text_utils.build_embeddings_matrix fastai_transforms.Tokenizer fastai_transforms.Vocab image_utils.SimplePreprocessor image_utils.AspectAwarePreprocessor . They are accessible directly from utils, e.g.: . from pytorch_widedeep.utils import LabelEncoder . Note that here I will be concentrating directly on the preprocessors. If you want more details on the utils tools, have a look to the source code or read the documentation. . 4.1. WidePreprocessor . The Wide component of the model is a linear model that in principle, could be implemented as a linear layer receiving the result of on one-hot encoded categorical columns. However, this is not memory efficient (at all). Therefore, we implement a liner layer as an Embedding layer plus a bias. I will explain it in a bit more detail later. For now, just know that WidePreprocessor simply encodes the categories numerically so that they are the indexes of the lookup table that is an Embedding layer. . from pytorch_widedeep.preprocessing import WidePreprocessor wide_cols = [&#39;education&#39;, &#39;relationship&#39;,&#39;workclass&#39;,&#39;occupation&#39;,&#39;native_country&#39;,&#39;gender&#39;] crossed_cols = [(&#39;education&#39;, &#39;occupation&#39;), (&#39;native_country&#39;, &#39;occupation&#39;)] wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols) X_wide = wide_preprocessor.fit_transform(adult) # From here on, any new observation can be prepared by simply running `.transform` # new_X_wide = wide_preprocessor.transform(new_df) . X_wide . array([[ 1, 17, 23, ..., 89, 91, 316], [ 2, 18, 23, ..., 89, 92, 317], [ 3, 18, 24, ..., 89, 93, 318], ..., [ 2, 20, 23, ..., 90, 103, 323], [ 2, 17, 23, ..., 89, 103, 323], [ 2, 21, 29, ..., 90, 115, 324]]) . X_wide[0] . array([ 1, 17, 23, 32, 47, 89, 91, 316]) . Note that the label encoding starts from 1. This is because it is convenient to leave 0 for padding, i.e. unknown categories. Let&#39;s take from example the first entry . wide_preprocessor.inverse_transform(X_wide[:1]) . education relationship workclass occupation native_country gender education_occupation native_country_occupation . 0 11th | own-child | private | machine-op-inspct | united-states | male | 11th-machine-op-inspct | united-states-machine-op-inspct | . As we can see, wide_preprocessor numerically encodes the wide_cols and the crossed_cols, which can be recovered using the method inverse_transform. . 4.2 TabPreprocessor . Simply, TabPreprocessor label-encodes the categorical columns and normalizes the numerical ones (unless otherwise specified). . from pytorch_widedeep.preprocessing import TabPreprocessor # cat_embed_cols = [(column_name, embed_dim), ...] cat_embed_cols = [(&#39;education&#39;,10), (&#39;relationship&#39;,8), (&#39;workclass&#39;,10), (&#39;occupation&#39;,10),(&#39;native_country&#39;,10)] continuous_cols = [&quot;age&quot;,&quot;hours_per_week&quot;] tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols) X_tab = tab_preprocessor.fit_transform(adult) # From here on, any new observation can be prepared by simply running `.transform` # new_X_deep = deep_preprocessor.transform(new_df) . print(X_tab[:5]) . [[ 1. 1. 1. 1. 1. -0.99512893 -0.03408696] [ 2. 2. 1. 2. 1. -0.04694151 0.77292975] [ 3. 2. 2. 3. 1. -0.77631645 -0.03408696] [ 4. 2. 1. 1. 1. 0.39068346 -0.03408696] [ 4. 1. 3. 4. 1. -1.50569139 -0.84110367]] . Note that the label encoding starts from 1. This is because it is convenient to leave 0 for padding, i.e. unknown categories. Let&#39;s take from example the first entry . Behind the scenes, TabPreprocessor uses LabelEncoder, simply a custom numerical encoder for categorical features, available via . from pytorch_widedeep.utils import LabelEncoder . 4.3. TextPreprocessor . This preprocessor returns the tokenized, padded sequences that will be directly &quot;fed&quot; to the deeptext component. . To illustrate the text and image preprocessors I will use a small sample of the Airbnb listing dataset, which you can get here. . airbnb=pd.read_csv(&quot;data/airbnb/airbnb_sample.csv&quot;) . texts = airbnb.description.tolist() texts[0] . &#34;My bright double bedroom with a large window has a relaxed feeling! It comfortably fits one or two and is centrally located just two blocks from Finsbury Park. Enjoy great restaurants in the area and easy access to easy transport tubes, trains and buses. Babies and children of all ages are welcome. Hello Everyone, I&#39;m offering my lovely double bedroom in Finsbury Park area (zone 2) for let in a shared apartment. You will share the apartment with me and it is fully furnished with a self catering kitchen. Two people can easily sleep well as the room has a queen size bed. I also have a travel cot for a baby for guest with small children. I will require a deposit up front as a security gesture on both our parts and will be given back to you when you return the keys. I trust anyone who will be responding to this add would treat my home with care and respect . Best Wishes Alina Guest will have access to the self catering kitchen and bathroom. There is the flat is equipped wifi internet,&#34; . from pytorch_widedeep.preprocessing import TextPreprocessor text_preprocessor = TextPreprocessor(text_col=&#39;description&#39;) X_text = text_preprocessor.fit_transform(airbnb) # From here on, any new observation can be prepared by simply running `.transform` # new_X_text = text_preprocessor.transform(new_df) . The vocabulary contains 2192 tokens . print(X_text[0]) . [ 29 48 37 367 818 17 910 17 177 15 122 349 53 879 1174 126 393 40 911 0 23 228 71 819 9 53 55 1380 225 11 18 308 18 1564 10 755 0 942 239 53 55 0 11 36 1013 277 1974 70 62 15 1475 9 943 5 251 5 0 5 0 5 177 53 37 75 11 10 294 726 32 9 42 5 25 12 10 22 12 136 100 145] . TextPreprocessor uses the utilities within the text_utils and the fastai_transforms modules. Again, all the utilities within those modules are are directly accessible from utils, e.g.: . from pytorch_widedeep.utils import simple_preprocess, pad_sequences, build_embeddings_matrix, Tokenizer, Vocab . 4.4 ImagePreprocessor . Finally, ImagePreprocessor simply resizes the images, being aware of the aspect ratio. By default they will be resized to (224, 224, ...). This is because the default deepdense component of the model is a pre-trained ResNet model, which requires inputs of height and width of 224. . Let&#39;s have a look . from pytorch_widedeep.preprocessing import ImagePreprocessor image_preprocessor = ImagePreprocessor(img_col=&#39;id&#39;, img_path=&quot;data/airbnb/property_picture/&quot;) X_images = image_preprocessor.fit_transform(airbnb) # From here on, any new observation can be prepared by simply running `.transform` # new_X_images = image_preprocessor.transform(new_df) . Reading Images from data/airbnb/property_picture/ . 4%|‚ñç | 41/1001 [00:00&lt;00:02, 396.72it/s] . Resizing . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1001/1001 [00:02&lt;00:00, 354.70it/s] . Computing normalisation metrics . X_images[0].shape . (224, 224, 3) . ImagePreprocessor uses two helpers: SimplePreprocessor and AspectAwarePreprocessor, available from the utils module, e.g.: . from pytorch_widedeep.utils import SimplePreprocessor, AspectAwarePreprocessor . These two classes are directly taken from Adrian Rosebrock&#39;s fantastic book &quot;Deep Learning for Computer Vision&quot;. Therefore, all credit to Adrian. . 5. Model Components . Let&#39;s now have a look to the components that can be used to build a wide and deep model. The 5 main components of WideDeep are: . wide deeptabular deeptext deepimage deephead . The first 4 will be collected and combined by the WideDeep class, while the 5th one can be optionally added to the WideDeep model through its corresponding parameters: deephead or alternatively head_layers, head_dropout and head_batchnorm. . 5.1. wide . The wide component is a Linear layer &quot;plugged&quot; into the output neuron(s) . The only particularity of our implementation is that we have implemented the linear layer via an Embedding layer plus a bias. While the implementations are equivalent, the latter is faster and far more memory efficient, since we do not need to one hot encode the categorical features. . Let&#39;s have a look: . import torch import pandas as pd import numpy as np from torch import nn . df = pd.DataFrame({&#39;color&#39;: [&#39;r&#39;, &#39;b&#39;, &#39;g&#39;], &#39;size&#39;: [&#39;s&#39;, &#39;n&#39;, &#39;l&#39;]}) df.head() . color size . 0 r | s | . 1 b | n | . 2 g | l | . one hot encoded, the first observation (color: r, size: s) would be . obs_0_oh = (np.array([1., 0., 0., 1., 0., 0.])).astype(&#39;float32&#39;) . if we simply numerically encode (or label encode) the values: . obs_0_le = (np.array([0, 3])).astype(&#39;int64&#39;) . Note that in the implementation of the package we start from 1, saving 0 for padding, i.e. unseen values. . Now, let&#39;s see if the two implementations are equivalent . # we have 6 different values. Let&#39;s assume we are performing a regression, so pred_dim = 1 lin = nn.Linear(6, 1) . emb = nn.Embedding(6, 1) emb.weight = nn.Parameter(lin.weight.reshape_as(emb.weight)) . lin(torch.tensor(obs_0_oh)) . tensor([0.0656], grad_fn=&lt;AddBackward0&gt;) . emb(torch.tensor(obs_0_le)).sum() + lin.bias . tensor([0.0656], grad_fn=&lt;AddBackward0&gt;) . And this is precisely how the linear component Wide is implemented . from pytorch_widedeep.models import Wide wide = Wide(wide_dim=10, pred_dim=1) wide . Wide( (wide_linear): Embedding(11, 1, padding_idx=0) ) . Again, let me emphasize that even though the input dim is 10, the Embedding layer has 11 weights. This is because we save 0 for padding, which is used for unseen values during the encoding process . 5.2. deeptabular . There are 3 alternatives for the so called deepdense component of the model: TabMlp and TabResnet and the TabTransformer: . TabMlp: this is almost identical to the tabular model in the fantastic fastai library, and consists simply in embeddings representing the categorical features, concatenated with the continuous features, and passed then through a MLP. . | TabRenset: This is similar to the previous model but the embeddings are passed through a series of ResNet blocks built with dense layers. . | TabTransformer: Details on the TabTransformer can be found in: TabTransformer: Tabular Data Modeling Using Contextual Embeddings . | For details on these 3 models and their options please see the examples in the Examples folder and the documentation. . Through the development of the package, the deeptabular component became one of the core values of the package. The possibilities are numerous, and therefore, I will further describe this component in detail in a separate post. . For now let&#39;s have a quick look: . Let&#39;s have a look first to TabMlp: . from pytorch_widedeep.models import TabMlp # fake dataset X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1) colnames = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] embed_input = [(u,i,j) for u,i,j in zip(colnames[:4], [4]*4, [8]*4)] column_idx = {k:v for v,k in enumerate(colnames)} continuous_cols = [&#39;e&#39;] # my advice would be to not use dropout in the last layer, but I add the option because you never # know..there is crazy people everywhere. tabmlp = TabMlp( mlp_hidden_dims=[16,8], mlp_dropout=[0.5, 0.], mlp_batchnorm=True, mlp_activation=&quot;leaky_relu&quot;, column_idx=column_idx, embed_input=embed_input, continuous_cols=continuous_cols) tabmlp . TabMlp( (embed_layers): ModuleDict( (emb_layer_a): Embedding(5, 8, padding_idx=0) (emb_layer_b): Embedding(5, 8, padding_idx=0) (emb_layer_c): Embedding(5, 8, padding_idx=0) (emb_layer_d): Embedding(5, 8, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (tab_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.5, inplace=False) (2): Linear(in_features=33, out_features=16, bias=False) (3): LeakyReLU(negative_slope=0.01, inplace=True) ) (dense_layer_1): Sequential( (0): Linear(in_features=16, out_features=8, bias=True) (1): LeakyReLU(negative_slope=0.01, inplace=True) ) ) ) ) . tabmlp(X_tab) . tensor([[-2.0658e-03, 5.0888e-01, 2.1883e-01, -3.1523e-03, -3.2836e-03, 8.3450e-02, -3.4315e-03, -8.6029e-04], [-2.8116e-03, 2.1922e-01, 5.0364e-01, -1.3522e-03, -9.8741e-04, -1.2356e-03, -1.4323e-03, 2.7542e-03], [ 1.1020e-01, 4.0867e-01, 4.3776e-01, 3.1146e-03, 2.7392e-01, -1.2640e-02, 1.2793e-02, 5.7851e-01], [-4.4498e-03, 2.0174e-01, 1.1082e+00, 2.3353e-01, -1.9922e-05, -4.9581e-03, 6.1367e-01, 9.4608e-01], [-5.7167e-03, 2.7813e-01, 7.8706e-01, -3.6171e-03, 1.5563e-01, -1.1303e-02, -7.6483e-04, 5.0236e-01]], grad_fn=&lt;LeakyReluBackward1&gt;) . Let&#39;s now have a look to TabResnet: . from pytorch_widedeep.models import TabResnet tabresnet = TabResnet( blocks_dims=[16, 8], blocks_dropout=0.1, column_idx=column_idx, embed_input=embed_input, continuous_cols=continuous_cols, ) tabresnet . TabResnet( (embed_layers): ModuleDict( (emb_layer_a): Embedding(5, 8, padding_idx=0) (emb_layer_b): Embedding(5, 8, padding_idx=0) (emb_layer_c): Embedding(5, 8, padding_idx=0) (emb_layer_d): Embedding(5, 8, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (tab_resnet): DenseResnet( (dense_resnet): Sequential( (lin1): Linear(in_features=33, out_features=16, bias=True) (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (block_0): BasicBlock( (lin1): Linear(in_features=16, out_features=8, bias=True) (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True) (dp): Dropout(p=0.1, inplace=False) (lin2): Linear(in_features=8, out_features=8, bias=True) (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (resize): Sequential( (0): Linear(in_features=16, out_features=8, bias=True) (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) ) ) . tabresnet(X_tab) . tensor([[-1.7038e-02, -2.2898e-03, 6.7239e-01, -1.1374e-02, -1.4843e-03, -1.0570e-02, 5.0264e-01, -1.3277e-02], [ 2.2679e+00, -5.1538e-04, -2.6135e-02, -2.9038e-02, -2.2504e-02, 5.5052e-01, 1.0497e+00, 1.3348e+00], [ 2.5005e-01, 7.7862e-01, 4.0052e-01, 7.6070e-01, 5.2203e-01, 6.5057e-01, -2.3226e-02, -4.0509e-04], [-1.3928e-02, -6.9325e-03, 1.6976e-01, 1.3968e+00, 5.9813e-01, -9.4279e-03, -9.0917e-03, 7.7908e-01], [ 5.7862e-01, 1.9515e-01, 1.3709e+00, 1.8836e+00, 1.2787e+00, 7.9873e-01, 1.6794e+00, -7.4565e-03]], grad_fn=&lt;LeakyReluBackward1&gt;) . and finally, the TabTransformer: . from pytorch_widedeep.models import TabTransformer embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)] tabtransformer = TabTransformer( column_idx=column_idx, embed_input=embed_input, continuous_cols=continuous_cols ) tabtransformer . TabTransformer( (embed_layers): ModuleDict( (emb_layer_a): Embedding(5, 32, padding_idx=0) (emb_layer_b): Embedding(5, 32, padding_idx=0) (emb_layer_c): Embedding(5, 32, padding_idx=0) (emb_layer_d): Embedding(5, 32, padding_idx=0) ) (embedding_dropout): Dropout(p=0.1, inplace=False) (blks): Sequential( (block0): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) (block1): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) (block2): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) (block3): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) (block4): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) (block5): TransformerEncoder( (self_attn): MultiHeadedAttention( (dropout): Dropout(p=0.1, inplace=False) (inp_proj): Linear(in_features=32, out_features=96, bias=True) (out_proj): Linear(in_features=32, out_features=32, bias=True) ) (feed_forward): PositionwiseFF( (w_1): Linear(in_features=32, out_features=128, bias=True) (w_2): Linear(in_features=128, out_features=32, bias=True) (dropout): Dropout(p=0.1, inplace=False) (activation): GELU() ) (attn_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) (ff_addnorm): AddNorm( (dropout): Dropout(p=0.1, inplace=False) (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True) ) ) ) (tab_transformer_mlp): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Linear(in_features=129, out_features=516, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.1, inplace=False) ) (dense_layer_1): Sequential( (0): Linear(in_features=516, out_features=258, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.1, inplace=False) ) ) ) ) . tabtransformer(X_tab) . tensor([[0.0000, 0.0000, 0.0000, ..., 0.0399, 0.2358, 0.3762], [0.1373, 0.0000, 0.0000, ..., 0.0550, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, ..., 0.0000, 0.0212, 0.0000], [0.3322, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000], [0.2914, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.6590]], grad_fn=&lt;MulBackward0&gt;) . 5.3. deeptext . pytorch-widedeep offers one model that can be passed to WideDeep as the deeptext component, DeepText, which is a standard and simple stack of LSTMs on top of word embeddings. You could also add a FC-Head on top of the LSTMs. The word embeddings can be pre-trained. In the future I aim to include some simple pre-trained models so that the combination between text and images is fair. . On the other hand, while I recommend using the wide and deeptabular models within this package when building the corresponding wide and deep model components, it is very likely that the user will want to use custom text and image models. That is perfectly possible. Simply, build them and pass them as the corresponding parameters. Note that the custom models MUST return a last layer of activations (i.e. not the final prediction) so that these activations are collected by WideDeep and combined accordingly. In addition, the models MUST also contain an attribute output_dim with the size of these last layers of activations. . I will illustrate all of the above more in detail in the second post of these series. . Let&#39;s have a look to DeepText . import torch from pytorch_widedeep.models import DeepText . X_text = torch.cat((torch.zeros([5,1]), torch.empty(5, 4).random_(1,4)), axis=1) deeptext = DeepText(vocab_size=4, hidden_dim=4, n_layers=1, padding_idx=0, embed_dim=4) deeptext . /Users/javier/.pyenv/versions/3.7.9/envs/wdposts/lib/python3.7/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1 &#34;num_layers={}&#34;.format(dropout, num_layers)) . DeepText( (word_embed): Embedding(4, 4, padding_idx=0) (rnn): LSTM(4, 4, batch_first=True, dropout=0.1) ) . deeptext(X_text) . tensor([[ 0.1727, -0.0800, -0.2599, -0.1245], [ 0.1530, -0.2874, -0.2385, -0.1379], [-0.0747, -0.1666, -0.0124, -0.1875], [-0.0382, -0.1085, -0.0167, -0.1702], [-0.0393, -0.0926, -0.0141, -0.1371]], grad_fn=&lt;SelectBackward&gt;) . You could, if you wanted, add a Fully Connected Head (FC-Head) on top of it . deeptext = DeepText(vocab_size=4, hidden_dim=8, n_layers=3, padding_idx=0, embed_dim=4, head_hidden_dims=[8,4], head_batchnorm=True, head_dropout=[0.5, 0.5]) . deeptext . DeepText( (word_embed): Embedding(4, 4, padding_idx=0) (rnn): LSTM(4, 8, num_layers=3, batch_first=True, dropout=0.1) (texthead): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=8, out_features=4, bias=True) (2): ReLU(inplace=True) ) ) ) ) . deeptext(X_text) . tensor([[0.4726, 0.0555, 0.0000, 0.1431], [0.4907, 0.1357, 0.0000, 0.2591], [0.4019, 0.0831, 0.0000, 0.1308], [0.3942, 0.1759, 0.0000, 0.2517], [0.3184, 0.0902, 0.0000, 0.1955]], grad_fn=&lt;ReluBackward1&gt;) . 5.4. deepimage . Similarly to deeptext, pytorch-widedeep offers one model that can be passed to WideDeep as the deepimage component, DeepImage, which is either a pre-trained ResNet (18, 34, or 50. Default is 18) or a stack of CNNs, to which one can add a FC-Head. If is a pre-trained ResNet, you can chose how many layers you want to defrost deep into the network with the parameter freeze_n . from pytorch_widedeep.models import DeepImage X_img = torch.rand((2,3,224,224)) deepimage = DeepImage(head_hidden_dims=[512, 64, 8], head_activation=&quot;leaky_relu&quot;) deepimage . DeepImage( (backbone): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (8): AdaptiveAvgPool2d(output_size=(1, 1)) ) (imagehead): MLP( (mlp): Sequential( (dense_layer_0): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=512, out_features=64, bias=True) (2): LeakyReLU(negative_slope=0.01, inplace=True) ) (dense_layer_1): Sequential( (0): Dropout(p=0.1, inplace=False) (1): Linear(in_features=64, out_features=8, bias=True) (2): LeakyReLU(negative_slope=0.01, inplace=True) ) ) ) ) . deepimage(X_img) . tensor([[ 0.0965, 0.0056, 0.1143, -0.0007, 0.3860, -0.0050, -0.0023, -0.0011], [ 0.2437, -0.0020, -0.0021, 0.2480, 0.6217, -0.0033, -0.0030, 0.0566]], grad_fn=&lt;LeakyReluBackward1&gt;) . 5.5. deephead . The are two possibilities when defining the so-called deephead component. . When defining the WideDeep model there is a parameter called head_hidden_dims (and the corresponding related parameters. See the package documentation) that define the FC-head on top of the deeptabular, deeptext and deepimage components. . | Of course, you could also chose to define it yourself externally and pass it using the parameter deephead. Have a look at the documentation. . | 6. Conclusion . This is the first of a series of posts introducing the python library pytorch-widedeep. This library is intended to be a flexible frame to combine tabular data with text and images via wide and deep models. Of course, it can also be used directly on &quot;traditional&quot; tabular data, without text and/or images. . In this post I have shown how to quickly start using the library (Section 3) and explained the utilities available in the preprocessing module (Section 4) and and model component definitions (Section 5), available in the models module. . In the next post I will show more advance uses that hopefully will illustrate pytorch-widedeep&#39;s flexibility to build wide and deep models. . References . [1] Wide &amp; Deep Learning for Recommender Systems. Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, et al. 2016. arXiv:1606.07792 . [2] TabNet: Attentive Interpretable Tabular Learning. Sercan O. Arik, Tomas Pfister, 2020. arXiv:1908.07442 . [3] AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data Nick Erickson, Jonas Mueller, Alexander Shirkov, et al., 2020. arXiv:2003.06505 . [4] Universal Language Model Fine-tuning for Text Classification. Jeremy Howard, Sebastian Ruder, 2018 arXiv:1801.06146v5 . [5] Single Headed Attention RNN: Stop Thinking With Your Head. Stephen Merity, 2019 arXiv:1801.06146v5 .",
            "url": "https://jrzaurin.github.io/infinitoml/2020/12/06/pytorch-widedeep.html",
            "relUrl": "/2020/12/06/pytorch-widedeep.html",
            "date": " ‚Ä¢ Dec 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "RecoTour III: Variational Autoencoders for Collaborative Filtering with Mxnet and Pytorch",
            "content": "This post and the code here are part of a larger repo called RecoTour, where I normally explore and implement some recommendation algorithms that I consider interesting and/or useful (see RecoTour and RecoTourII). In every directory, I have included a README file and a series of explanatory notebooks that I hope help explaining the code. I keep adding algorithms from time to time, so stay tunned if you are interested. . As always, let me first acknowledge the relevant people that did the hard work. This post and the companion repo are based on the papers ‚ÄúVariational Autoencoders for Collaborative Filtering‚Äù [1] and &quot;Auto-Encoding Variational Bayes&quot; [2]. The code here and in that repo is partially inspired by the implementation from Younggyo Seo. I have adapted the code to my coding preferences and added a number of options and flexibility to run multiple experiment. . The reason to take a deep dive into variational autoencoders for collaborative filtering is because they seem to be one of the few Deep Learning based algorithm (if not the only one) that is obtains better results that those using non-Deep Learning techniques [3]. . All the experiments in this post were run using a p2.xlarge EC2 instance on AWS. . 1. Variational Autoencoders for collaborative filtering . I must admit that when it comes to variational autoencoders (VAEs) I find that there is a &quot;notable&quot; difference between the complexity of the math and that of the code (or maybe is just me that I am not a mathematician). Nonetheless, I think that speaking about VAEs and not mentioning log likelihoods, Evidence Lower Bound (EBLO) or the Kullback‚ÄìLeibler divergence ($ text{D}_{ text{KL}}$) is almost like &quot;cheating&quot;. With that in mind I will try to give some mathematical context to &quot;Partially Regularized Multinomial Variational Autoencoder&quot; ($ text{Mult-VAE}^{ text{PR}}$) for collaborative filtering and then move to the code. The whole purpose of the math below is to ultimately justify the loss function we will be using when training the $ text{Mult-VAE}^{ text{PR}}$ as well as the architecture of the algorithm. . Before diving into the problem scenario and the mathematical formulation, let me describe the notational convention. Following Liang et al., 2018, I will use $u in {1, dots,U }$ to index users and $i in {1, dots,I }$ to index items. The user-by-item binary interaction matrix (i.e. the click matrix) is $ mathbf{X} in mathbb{N}^{U times I}$ and I will use lower case $ mathbf{x}_u =[X_{u1}, dots,X_{uI}]^ top in mathbb{N}^I$ to refer to the click history of an individual user $u$. . 1.1 Problem scenario . We are given a dataset $ mathbf{X} = { { mathbf{x}_u} }^{U}_{u=1}$ of user clicks (a more general scenario is described in &quot;Auto-Encoding Variational Bayes&quot; [2]). Our job is to estimate the parameters of the underlying probability distribution so that we can do inference. In other words, we need to find a statistical model of the data (like in any other ML problem). To do this, we need to maximize the likelihood function $p_{ theta}( mathbf{X})$ so that under the assumed statistical model the observed data is most probable. . To find the maximum likelihood we could assume that the statistical model of the data involves some latent variable $ bf{z}$, so that the marginal likelihood can be written as: . $$ p_{ theta}( mathbf{x}_u) = int {p_{ theta}( mathbf{z}_u)p_{ theta}( mathbf{x}_u vert mathbf{z}_u) d mathbf{z}_u} hspace{1cm} (1) $$where $ theta$ are the parameters of the distribution. Eq (1) is solvable if we assume that both the prior $p_{ theta}( mathbf{z}_u)$ and the conditional probability $p_{ theta}( mathbf{x}_u vert mathbf{z}_u)$ come from parametric families of distributions and that their PDFs are differentiable almost everywhere w.r.t. both $ theta$ and $ mathbf{z}_u$. However, for &quot;moderately&quot; complicated likelihood functions $p_{ theta}( mathbf{x}_u vert mathbf{z}_u)$, such as a neural network with a nonlinear layer, Eq (1) is intractable (it is not possible to evaluate of differentiate the marginal likelihood). Furthermore, the true posterior $p_{ theta}( mathbf{z}_u vert mathbf{x}_u) = p_{ theta}( mathbf{x}_u vert mathbf{z}_u)p_{ theta}( mathbf{z}_u)/p_{ theta}( mathbf{x}_u)$ is also intractable, and therefore we cannot use an EM algorithm (since the E-step involves the computation of the true posterior at a given iteration). . To address these and some other limitations, and find a general solution to this problem, Kingma and Welling 2014 proposed a flexible neural network based approach. . 1.2 Auto-Encoding Variational Bayes . The following Section is both a summary and my understanding of the paper &quot;Auto-Encoding Variational Bayes&quot; to which I keep referring and that I strongly recommend reading. . Let me remind you: our goal is to maximize the likelihood, or more conveniently the log likelihood $ log p_{ theta}( mathbf{X})$, where: . $$ log p_{ theta}( mathbf{X}) = sum_u log p_{ theta}( mathbf{x}_u) hspace{1cm} (2) $$Each term in the summation can be re-written as: . $$ log p_{ theta}( mathbf{x}_u) = D_{KL} left(q_ phi( textbf{z}_u vert textbf{x}_u) | p_ theta( textbf{z}_u vert textbf{x}_u) right) + underbrace{ mathbb{E} small{ q_{ phi}( mathbf{z}_u vert mathbf{x}_u) } left[ - log q_{ phi}( mathbf{z}_u vert mathbf{x_u}) + log p_{ theta}( mathbf{x}_u, mathbf{z}_u) right]}_{ELBO mathcal L( textbf{x}_u, phi, theta)} hspace{1cm} (3) $$Where the first elements in the right hand side is the Kullback‚ÄìLeibler divergence ($ text{D}_{ text{KL}}$) and $q_ phi( textbf{z}_u vert textbf{x}_u)$ is the approximate posterior of the true posterior $p_ theta( textbf{z}_u vert textbf{x}_u)$. Eq (3) is our &quot;point of entry&quot; from which we will derive the remaining equations. If you want proof of Eq (3) I would recommend reading this tutorial or this &quot;crazy&quot; post. . Moving on, given that $ text{D}_{ text{KL}}$ is non-negative, $ log p_{ theta}( mathbf{x}_u) geq mathcal L( textbf{x}_u, phi, theta)$ and therefore $ mathcal L$ is referred as Evidence Lower Bound (ELBO). It is straightforward to understand from Eq (3) that maximizing $ log p_{ theta}( mathbf{x}_u)$ implies maximizing ELBO $ mathcal L$. If we re-order the terms in that equation, we could also think of the problem as follows: maximizing ELBO $ mathcal L$ implies minimizing $ text{D}_{ text{KL}}$, which makes sense, since $D_{KL}$ measures the dissimilarity between the approximate posterior $q_ phi( textbf{z}_u vert textbf{x}_u)$ and the true posterior $p_{ theta}( textbf{z}_u vert textbf{x}_u)$. . ELBO $ mathcal L$ in Eq (3) can also be re-written as: . $$ mathcal L( textbf{x}_u, phi, theta) = - D_{KL} left(q_ phi( textbf{z}_u vert textbf{x}_u) | p_ theta( textbf{z}_u right) + mathbb{E} small{ q_{ phi}( mathbf{z}_u vert mathbf{x}_u) } left[ log p_{ theta}( textbf{x}_u vert textbf{z}_u) right] hspace{1cm} (4) $$We can see that Eq (4) involves sampling $ tilde{ mathbf{z}_u} sim q_{ phi}( mathbf{z}_u vert mathbf{x}_u)$. When sampling is involved, backpropagation is not trivial (how one would take gradients with respect to $ phi$?). To remedy this situation Kingma &amp; Welling introduced the so called &quot;reparameterization trick&quot;. Instead of sampling from the approximate postertior $q_{ phi}( mathbf{z}_u vert mathbf{x}_u)$, the authors used a differentiable transformation $g_{ phi}( mathbf{ epsilon}, mathbf{x}_u)$ of a noise variable $ epsilon$, such that: . $$ tilde{ mathbf{z}_u} = g_{ phi}( mathbf{ epsilon}, mathbf{x}_u) hspace{1cm} with hspace{1cm} mathbf{ epsilon} sim p( epsilon) hspace{1cm} (5) $$where $p( epsilon)$ can be, for example, a variable sampled from a random normal distribution (see Section 1.3 for the selection of $g_{ phi}$ in the particular case of the $ text{Mult-VAE}^{ text{PR}}$). With these formulation, one can use Monte Carlo estimates of expectations of some function $f( mathbf{z})$ with respect to $q_{ phi}( mathbf{z}_u vert mathbf{x}_u)$ such that: . $$ mathbb{E} small{ q_{ phi}( mathbf{z}_u vert mathbf{x}_u) } left[ f( mathbf{z}_u) right] = mathbb{E} small{ q_{ phi}( mathbf{z}_u vert mathbf{x}_u) } left[ f(g_{ phi}( mathbf{ epsilon}, mathbf{x}_u)) right] simeq frac{1}{L} sum_{l=1}^{L} f(g_{ phi}( mathbf{ epsilon}^l), mathbf{x}_u) text{where} hspace{1cm} mathbf{ epsilon}^l sim p( epsilon) hspace{1cm} (6) $$Replacing the second term in Eq (4) with the result in Eq (6), we see that the ELBO $ mathcal L$ can be approximated by what Kingma and Welling called &quot;Generic Stochastic Gradient Variational Bayes&quot; (SGVB) estimator $ tilde{ mathcal L}( textbf{x}_u, phi, theta) simeq mathcal L( textbf{x}_u, phi, theta)$: . $$ tilde{ mathcal L}( mathbf{x}_u, phi, theta) = - D_{KL} left(q_ phi( textbf{z}_u vert textbf{x}_u) | p_ theta( textbf{z}_u right) + frac{1}{L} sum_{l=1}^{L} log p_{ theta}( mathbf{x}_u vert mathbf{z}^l_u) text{where} hspace{1cm} mathbf{z}^l_u = g_{ phi}( epsilon^l_u, mathbf{x}_u) hspace{1cm} text{and} hspace{1cm} epsilon^l sim p( epsilon) hspace{1cm} (7) $$Of course, when running a practical application, we will be using minibatches. With that in mind, we can re-write ELBO $ mathcal{L}$ in &quot;minibatch form&quot; as: . $$ mathcal L( mathbf{ text{X}}^M, phi, theta) simeq tilde{ mathcal L}^{M}( mathbf{ text{X}}^M, phi, theta) = frac{1}{M} sum_{u=1}^{M} tilde{ mathcal L}( mathbf{x}_u, phi, theta) hspace{1cm} (8) $$where $ mathbf{X}^M = { mathbf{x}_u }_{u=1}^M$ is a minibatch of M users. In their experiments the authors found that the number of samples $L$ can be set to 1 as long as the minibatch size was large enough, e.g. $M$ = 100. Therefore, as long as our batch sizes are of 100 or more, Eq (7) can be re-written as: . $$ mathcal L( mathbf{ text{X}}^M, phi, theta) simeq frac{1}{M} sum_{u=1}^{M} - D_{KL} left(q_ phi( textbf{z}_u vert textbf{x}_u) | p_ theta( textbf{z}_u right) + log p_{ theta}( mathbf{x}_u vert mathbf{z}^s_u) hspace{1cm} (9) $$Note that $ mathbf{z}^s_u$ signifies that $ mathbf{z}_u$ still needs to be sampled once from $q_ phi( textbf{z}_u vert textbf{x}_u)$, but using the reparameterization trick this will be rather easy, as we will see in the next section. Finally, now that we have a &quot;nice looking&quot; mathematical expression, this is how Auto-Encoding Variational Bayes works: . Select a prior for latent representation of $ textbf{x}_u$, $p_{ theta}( textbf{z}_u)$ | Use a neural network to parameterize the distribution $p_{ theta}( textbf{x}_u vert textbf{z}_u)$. Because this part of the model maps the latent variable/representation $ textbf{z}_u$ to the observed data $ textbf{x}_u$, it is referred as a &quot;decoder&quot; network. | Rather than explicitly calculating the intractable posterior $p_{ theta}( textbf{z}_u vert textbf{x}_u)$, use another another neural network to parameterize the distribution $q_ phi( textbf{z}_u vert textbf{x}_u)$ as the approximate posterior. Since $q_ phi$ maps the observed data $ textbf{x}_u$ to the latent space of $ textbf{z}_u$&#39;s, is referred as the &quot;encoder&quot; network. | maxmize ELBO $ mathcal{L}$ in Eq (9) using Stochastic Gradient Descent or any of its cousins | 1.3 Partially Regularized Autoencoder for Collaborative Filtering . or $ text{Mult-VAE}^{ text{PR}}$... . In the previous Section we obtained Eq (9), which is a generic form of the function we need to maximize to solve the problem described in Section 1.1. Now let&#39;s see a particular case of that equation for the set up used by Liang and co-authors in their paper. Such set up is described as follows: for each user $u$, the latent representation $ textbf{z}_u$ is assumed to be drawn from a standard Gaussian prior $p( textbf{z}_u) sim mathcal N(0, I)$. Such representation is then transformed by a multi-layer perceptron (MLP), and the output is normalized via a Softmax function to produce a probability distribution over all items $I$, $ pi( mathbf{z}_u) = Softmax(MLP( mathbf{z}_u))$. Then, the click history of user $u$ is assumed to be drawn from a Multinomial distribution with probability $ pi( mathbf{z}_u)$: . $$ textbf{x}_u sim text{Mult}(N_u, pi( mathbf{z}_u)) hspace{1cm} (10) $$where $N_u = sum_i x_{ui}$ is the total number of clicks for user $u$. In this set up, the log-likelihood of the click history $ mathbf{x}_u$ conditioned to the latent representation $ mathbf{z}_u$ is simply: . $$ begin{equation*} log(p_{ theta}( textbf{x}_u vert textbf{z}_u)) = mathbf{x}_u log( pi( mathbf{z}_u)) hspace{1cm} (11) end{equation*} $$The posterior $q_ phi( textbf{z}_u vert textbf{x}_u)$ is also chosen to be a standard Gaussian $q_ phi( textbf{z}_u vert textbf{x}_u) sim mathcal N( mu_ phi( textbf{x}_u), sigma_ phi( textbf{x}_u) I)$ where $ mu_ phi( textbf{x}_u)$ and $ sigma_ phi( textbf{x}_u)$ are functions implemented as neural networks. Then, we use the reparameterization trick and chose $g_{ phi}( mathbf{ epsilon}, mathbf{x}_u) = mu( textbf{x}_u) + sigma( textbf{x}_u) cdot epsilon$, where $ epsilon sim mathcal{N}(0,I)$. This way $ mathbf{z}^s_u = mu( textbf{x}_u) + sigma( textbf{x}_u) cdot epsilon$ where we sample directly $ epsilon$. . At this stage we have defined the Gaussian prior, the Gaussian approximate posterior and our sampled latent representation. We are finally ready to &quot;plug the terms&quot; into Eq (9) and write the loss function that we will minimize when training the Mult-VAE: . $$ Loss = - frac{1}{M} sum_{u=1}^{M} left[ mathbf{x}_u log( pi( mathbf{z}_u)) + frac{ beta}{2} sum_j ( 1 + log( sigma_{uj}^2) - mu_{uj}^2 - sigma_{uj}^2 ) right] hspace{1cm} (12) $$Note that the expression above is the negative ELBO $ mathcal L$ (maximizing $ mathcal L$ is equivalent to minimize -$ mathcal L$) with a multiplicative factor $ beta$ applied to the $D_{KL}$. For the math behind the $D_{KL}$ expression given this set up have a look here. . Let me just comment on that $ beta$. Looking at the loss function in Eq (12) within the context of VAEs, we can see that the first term is the reconstruction loss, while the $D_{KL}$ act as a regularizer. With that in mind, Liang et al add a factor $ beta$ to control the strength of the regularization, and propose $ beta &lt; 1$. . Let&#39;s pause for one second and think on what this means. First of all, we are no longer optimizing a lower bound for a given log likelihood. In addition, remember that the $D_{KL}$ divergence measures the similarity between the approximate posterior $q_ phi( textbf{z}_u vert textbf{x}_u)$ and the prior $p_ theta( textbf{z}_u)$. Therefore, by using $ beta &lt; 1$ we are weakening the influence of the prior constrain $q_ phi( textbf{z}_u vert textbf{x}_u) approx p_ theta( textbf{z}_u)$ on the loss. This means that we are less able to generalize to novel user clicks from historical data. However, when building recommendation systems we are often not interested in reproducing precisely click histories (i.e. achieving the best loss) but in making good recommendations (i.e. achieving the best ranking metrics). As the authors show in the paper (and we will see here later), the best ranking metrics are obtained when using $ beta &lt; 1$ and in consequence they name the algorithm Partially Regularized Multinomial Autoencoder or $ text{Mult-VAE}^{ text{PR}}$. . 2. Preparing the data . Throughout this exercise I will use two dataset. The Amazon Movies and TV dataset [4] [5] and the Movilens dataset. The later is used so I can make sure I am obtaining consistent results to those obtained in the paper. As we will see through the notebook, the Amazon dataset is significantly more challenging that the Movielens dataset. . The data preparation is fairly simple, and is identical for both datasets. Therefore, I will focus here only on the Amazon dataset. . #collapse-hide import os import sys import pandas as pd import numpy as np import pickle from tqdm import trange from typing import Tuple, Dict, Union from pathlib import Path sys.path.append(os.path.abspath(&#39;/Users/javier/ml_experiments_python/RecoTour/Amazon/mult-vae/&#39;)) rootpath = Path(&quot;/Users/javier/ml_experiments_python/RecoTour/Amazon/mult-vae/&quot;) . . DATA_DIR = Path(rootpath / &#39;data&#39;) new_colnames = [&quot;user&quot;, &quot;item&quot;, &quot;rating&quot;, &quot;timestamp&quot;] inp_path = DATA_DIR / &quot;amazon-movies&quot; filename = &quot;reviews_Movies_and_TV_5.p&quot; raw_data = pd.read_pickle(inp_path / filename) keep_cols = [&quot;reviewerID&quot;, &quot;asin&quot;, &quot;overall&quot;, &quot;unixReviewTime&quot;] raw_data = raw_data[keep_cols] raw_data.columns = new_colnames . print(raw_data.shape) raw_data.head() . (1697533, 4) . user item rating timestamp . 0 ADZPIG9QOCDG5 | 0005019281 | 4 | 1203984000 | . 1 A35947ZP82G7JH | 0005019281 | 3 | 1388361600 | . 2 A3UORV8A9D5L2E | 0005019281 | 3 | 1388361600 | . 3 A1VKW06X1O2X7V | 0005019281 | 5 | 1202860800 | . 4 A3R27T4HADWFFJ | 0005019281 | 4 | 1387670400 | . 2.1 Filter triples (user, item, score) . The first thing that the we do is to &quot;filter triples&quot; (hereafter refereed as tp) based on the number of times a user interacted with items (min_user_click) or items that where &quot;interacted with&quot; by a user a given number of times (min_item_click). . def get_count(tp: pd.DataFrame, id: str) -&gt; pd.Index: &quot;&quot;&quot; Returns `tp` groupby+count by `id` &quot;&quot;&quot; playcount_groupbyid = tp[[id]].groupby(id, as_index=False) count = playcount_groupbyid.size() return count def filter_triplets( tp: pd.DataFrame, min_user_click, min_item_click ) -&gt; Tuple[pd.DataFrame, pd.Index, pd.Index]: &quot;&quot;&quot; Returns triplets (`tp`) of user-item-rating for users/items with more than min_user_click/min_item_click counts &quot;&quot;&quot; if min_item_click &gt; 0: itemcount = get_count(tp, &quot;item&quot;) tp = tp[tp[&quot;item&quot;].isin(itemcount.index[itemcount &gt;= min_item_click])] if min_user_click &gt; 0: usercount = get_count(tp, &quot;user&quot;) tp = tp[tp[&quot;user&quot;].isin(usercount.index[usercount &gt;= min_user_click])] usercount, itemcount = get_count(tp, &quot;user&quot;), get_count(tp, &quot;item&quot;) return tp, usercount, itemcount . filtered_raw_data, user_activity, item_popularity = filter_triplets( raw_data, min_user_click=5, min_item_click=0 ) . Note that, since I am using the &quot;reviews_Movies_and_TV_5&quot; (i.e. the 5-core dataset, where users and items have at least 5 reviews each) filtered_raw_data has no effect on the Amazon dataset. It does however filter some users/items in the case of the Movilens dataset. . Let&#39;s now have a look to the sparsity of the dataset: . sparsity = ( 1.0 * filtered_raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0]) ) print( &quot;After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)&quot; % ( filtered_raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100, ) ) . After filtering, there are 1697533 watching events from 123960 users and 50052 movies (sparsity: 0.027%) . Comparing these numbers to those of the Movilens dataset (9990682 watching events from 136677 users and 20720 movies: sparsity: 0.353%. see the notebook corresponding to the original publication, or the original publication itself) one can see that the Amazon dataset is $ sim$13 times more sparse than the Movielens dataset. In consequence, I one would expect that the algorithm finds it more challenging, resulting in lower ranking metrics. . 2.2 Train, validation and test split . Once the raw data is filtered, we follow the same procedure than that of the original paper to split the users into training, validation and test users. . def split_users( unique_uid: pd.Index, test_users_size: Union[float, int] ) -&gt; Tuple[pd.Index, pd.Index, pd.Index]: n_users = unique_uid.size if isinstance(test_users_size, int): n_heldout_users = test_users_size else: n_heldout_users = int(test_users_size * n_users) tr_users = unique_uid[: (n_users - n_heldout_users * 2)] vd_users = unique_uid[(n_users - n_heldout_users * 2) : (n_users - n_heldout_users)] te_users = unique_uid[(n_users - n_heldout_users) :] return tr_users, vd_users, te_users . unique_uid = user_activity.index np.random.seed(98765) idx_perm = np.random.permutation(unique_uid.size) unique_uid = unique_uid[idx_perm] tr_users, vd_users, te_users = split_users( unique_uid, test_users_size=0.1 ) print(tr_users.shape, vd_users.shape, te_users.shape) . (99168,) (12396,) (12396,) . And this is how the authors set up the experiment: for validation and test they consider &quot;only&quot; items that have been seen during training . # Select the training observations raw data tr_obsrv = filtered_raw_data.loc[filtered_raw_data[&quot;user&quot;].isin(tr_users)] tr_items = pd.unique(tr_obsrv[&quot;item&quot;]) #¬†Save index dictionaries to &quot;numerise&quot; later one item2id = dict((sid, i) for (i, sid) in enumerate(tr_items)) user2id = dict((pid, i) for (i, pid) in enumerate(unique_uid)) vd_obsrv = filtered_raw_data[ filtered_raw_data[&quot;user&quot;].isin(vd_users) &amp; filtered_raw_data[&quot;item&quot;].isin(tr_items) ] te_obsrv = filtered_raw_data[ filtered_raw_data[&quot;user&quot;].isin(te_users) &amp; filtered_raw_data[&quot;item&quot;].isin(tr_items) ] . Now that we have the validation and test users and their interactions, we will split such interactions into so-called &quot;validation and test train and test sets&quot;. . I know that this sounds convoluted, but is not that complex. The &quot;validation_train and test_train sets&quot;, comprising here 80% of the total validation and test sets, will be used to build what we could think as an input binary &quot;image&quot; (i.e. the binary matrix of clicks) to be &quot;encoded -&gt; decoded&quot; by the trained auto-encoder. On the other hand the &quot;validation_test and test_test sets&quot;, comprising here 20% of the total validation and test sets, will be used to compute the ranking metrics at validation/test time. If you want more details along with a toy example please go to the corresponding notebook in the repo. I will discuss this further in Section 4. . def split_train_test( data: pd.DataFrame, test_size: float ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]: data_grouped_by_user = data.groupby(&quot;user&quot;) tr_list, te_list = list(), list() np.random.seed(98765) for i, (nm, group) in enumerate(data_grouped_by_user): n_items_u = len(group) if n_items_u &gt;= 5: idx = np.zeros(n_items_u, dtype=&quot;bool&quot;) idx[ np.random.choice( n_items_u, size=int(test_size * n_items_u), replace=False ).astype(&quot;int64&quot;) ] = True tr_list.append(group[np.logical_not(idx)]) te_list.append(group[idx]) else: tr_list.append(group) data_tr = pd.concat(tr_list) data_te = pd.concat(te_list) return data_tr, data_te def numerize(tp: pd.DataFrame, user2id: Dict, item2id: Dict) -&gt; pd.DataFrame: user = [user2id[x] for x in tp[&quot;user&quot;]] item = [item2id[x] for x in tp[&quot;item&quot;]] return pd.DataFrame(data={&quot;user&quot;: user, &quot;item&quot;: item}, columns=[&quot;user&quot;, &quot;item&quot;]) . vd_items_tr, vd_items_te = split_train_test(vd_obsrv, test_size=0.2) te_items_tr, te_items_te = split_train_test(te_obsrv, test_size=0.2) . And that&#39;s it regarding to data preparation. We can now move to the model itself. . 3 $ text{Mult-VAE}^{ text{PR}}$: the code . After the explanation in Section 1 you might expect the code to look rather complex. However, you might feel disappointed/pleased when you see how simple it really is. . In the original publications the authors used a one hidden layer MLP as generative model. There they say that deeper architectures do not improve the results, which I find it to be true after having run over 60 experiments. With that it mind, let&#39;s first have a look the model $ I rightarrow 600 rightarrow 200 rightarrow 600 rightarrow I$, where $I$ is the total number of items: . . Figure 1. $ text{Mult-VAE}^{ text{PR}}$ architecture. The colors in the Figure are my attempt to emphasize the reparameterization trick. . In code, the figure above is: . #collapse-hide from typing import List import numpy as np import mxnet as mx from mxnet import autograd, gluon, nd from mxnet.gluon import nn, HybridBlock . . /usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters . 3.1 Encoder . class VAEEncoder(HybridBlock): def __init__(self, q_dims: List[int], dropout: List[float]): super().__init__() # last dim multiplied by two for the reparameterization trick q_dims_ = q_dims[:-1] + [q_dims[-1] * 2] with self.name_scope(): self.q_layers = nn.HybridSequential(prefix=&quot;q_net&quot;) for p, inp, out in zip(dropout, q_dims_[:-1], q_dims_[1:]): self.q_layers.add(nn.Dropout(p)) self.q_layers.add(nn.Dense(in_units=inp, units=out)) def hybrid_forward(self, F, X): h = F.L2Normalization(X) for i, layer in enumerate(self.q_layers): h = layer(h) if i != len(self.q_layers) - 1: h = F.tanh(h) else: mu, logvar = F.split(h, axis=1, num_outputs=2) return mu, logvar . 3.2 Decoder . class Decoder(HybridBlock): def __init__(self, p_dims: List[int], dropout: List[float]): super().__init__() with self.name_scope(): self.p_layers = nn.HybridSequential(prefix=&quot;p_net&quot;) for p, inp, out in zip(dropout, p_dims[:-1], p_dims[1:]): self.p_layers.add(nn.Dropout(p)) self.p_layers.add(nn.Dense(in_units=inp, units=out)) def hybrid_forward(self, F, X): h = X for i, layer in enumerate(self.p_layers): h = layer(h) if i != len(self.p_layers) - 1: h = F.tanh(h) return h . 3.3 The model . class MultiVAE(HybridBlock): def __init__( self, p_dims: List[int], dropout_enc: List[float], dropout_dec: List[float], q_dims: List[int] = None, ): super().__init__() self.encode = VAEEncoder(q_dims, dropout_enc) self.decode = Decoder(p_dims, dropout_dec) def hybrid_forward(self, F, X): mu, logvar = self.encode(X) std = F.exp(0.5 * logvar) eps = F.random.normal_like(std) sampled_z = mu + float(autograd.is_training()) * eps * std return self.decode(sampled_z), mu, logvar . Before I move on, let me mention (and appreciate) one of the many nice &quot;little&quot; things that Mxnet&#39;s Gluon has to offer. You will notice the use of HybridBlock and the use of the input F (the backend) when we define the forward pass, or more precisely, the hybrid_forward pass. One could write a full post on the joys of HybridBlocks and how nicely and easily the guys that developed Gluon brought together the flexibility of imperative frameworks (e.g. Pytorch) and the speed of declarative frameworks (e.g. Tensorflow) together. If you want to learn the details go here, but believe me, this is FAST. . Having said that, there is only one more piece that we need to have the complete model, the loss function in Eq (12). . def vae_loss_fn(inp, out, mu, logvar, anneal): # firt term neg_ll = -nd.mean(nd.sum(nd.log_softmax(out) * inp, -1)) # second term without beta KLD = -0.5 * nd.mean(nd.sum(1 + logvar - nd.power(mu, 2) - nd.exp(logvar), axis=1)) # &quot;full&quot; loss (anneal is beta in the expressions above) return neg_ll + anneal * KLD . In the paper the authors also use a Multinomial Denoising Autoencoder (Mult-DAE). The architecture is identical to that of the $ text{Mult-VAE}^{ text{PR}}$ apart from the fact that there is no variational aspect. I have implemented the Mult-DAE and run multiple experiments with it. However, given its simplicity and an already lengthy post, I will not discuss the corresponding code here. . Let&#39;s have a look to the MultiVAE . I = 50000 q_dims = [I] + [600, 200] p_dims = [200, 600] + [I] dropout_enc = [0.5, 0.] dropout_dec = [0., 0.] . vae_model = MultiVAE( p_dims=p_dims, q_dims=q_dims, dropout_enc=dropout_enc, dropout_dec=dropout_dec, ) vae_model . MultiVAE( (encode): VAEEncoder( (q_layers): HybridSequential( (0): Dropout(p = 0.5, axes=()) (1): Dense(50000 -&gt; 600, linear) (2): Dropout(p = 0.0, axes=()) (3): Dense(600 -&gt; 400, linear) ) ) (decode): Decoder( (p_layers): HybridSequential( (0): Dropout(p = 0.0, axes=()) (1): Dense(200 -&gt; 600, linear) (2): Dropout(p = 0.0, axes=()) (3): Dense(600 -&gt; 50000, linear) ) ) ) . Note that following the original implementation, I apply dropout at the input layer for both $ text{Mult-VAE}^{ text{PR}}$ and $ text{Mult-DAE}$ to avoid overfitting. I also include the option for applying dropout throughout the network. . Note that even though I have explored different dropouts, the best way of addressing the interplay between dropout, weight decay, $ beta$, etc, and the architecture is using &quot;proper&quot; hyperparamter optimization. . 4. Train, validate and test model . So far we have explained (a bit) the math behind the model, prepared the data and build the (relatively) simple model. Now is time to train it. . If you go to prepare_data.py in the repo, you will see that the results of the data preparation in Section 2 are saved into pickle files and are later loader in main_mxnet.py by a class called Dataloader in the module utils. This is of course inspired by the original implementation and the already mentioned Pytorch implementation. . Let&#39;s have a look using this time the Movilens dataset . #collapse-hide from utils.data_loader import DataLoader from utils.metrics import NDCG_binary_at_k_batch, Recall_at_k_batch . . data_loader = DataLoader(DATA_DIR / &quot;movielens_processed&quot;) n_items = data_loader.n_items train_data = data_loader.load_data(&quot;train&quot;) valid_data_tr, valid_data_te = data_loader.load_data(&quot;validation&quot;) test_data_tr, test_data_te = data_loader.load_data(&quot;test&quot;) train_data . &lt;116677x20108 sparse matrix of type &#39;&lt;class &#39;numpy.float32&#39;&gt;&#39; with 8538846 stored elements in Compressed Sparse Row format&gt; . As you can see, the training data (same applies to validation and test) is the binary sparse matrix of interactions. Have a look to the class DataLoader if you want a few more details on how it is built. . 4.1 Annealing schedule . As mentioned before, we can interpret the Kullback-Leiber divergence as a regularization term. With that in mind, in a procedure inspired by Samuel R. Bowman et al, 2016 [6], Liang and co-authors linearly annealed the KL term (i.e. increase $ beta$) slowly over a large number of training steps. . More specifically, the authors anneal the KL divergence all the way to $ beta$ = 1, reaching that value at around 80% of the total number of epochs used during the process. They then identify the best performing $ beta$ based on the peak validation metric, and retrain the model with the same annealing schedule, but stop increasing $ beta$ after reaching that value. . If we go to their implementation, these are the specifics of the process: using a batch size of 500 they set the total number of annealing steps to 200000. Given that the training dataset has a size of 116677, every epoch has 234 training steps. Their anneal_cap value, i.e. the maximum annealing reached during training, is set to 0.2, and during training they use the following approach: . if total_anneal_steps &gt; 0: anneal = min(anneal_cap, 1. * update_count / total_anneal_steps) else: anneal = anneal_cap . where update_count will increase by 1 every training step/batch. They use 200 epochs, therefore, if we do the math, the anneal_cap value will stop increasing when update_count / total_anneal_steps = 0.2, i.e. after 40000 training steps, or in other words, after around 170 epochs, i.e. $ sim$80% of the total number of epochs. . Whit that in mind my implementation looks like this: . batch_size = 500 anneal_epochs = None anneal_cap = 0.2 constant_anneal = False n_epochs = 200 . training_steps = len(range(0, train_data.shape[0], batch_size)) try: total_anneal_steps = ( training_steps * (n_epochs - int(n_epochs * 0.2)) ) / anneal_cap except ZeroDivisionError: assert ( constant_anneal ), &quot;if &#39;anneal_cap&#39; is set to 0.0 &#39;constant_anneal&#39; must be set to &#39;True&quot; . once the total_anneal_steps is set, the only thing left is to define the training and validation steps. If you are familiar with Pytorch, the next two functions will be look very familiar to you: . 4.2 Train Step . def train_step(model, optimizer, data, epoch): running_loss = 0.0 global update_count N = data.shape[0] idxlist = list(range(N)) np.random.shuffle(idxlist) training_steps = len(range(0, N, batch_size)) with trange(training_steps) as t: for batch_idx, start_idx in zip(t, range(0, N, batch_size)): t.set_description(&quot;epoch: {}&quot;.format(epoch + 1)) end_idx = min(start_idx + batch_size, N) X_inp = data[idxlist[start_idx:end_idx]] X_inp = nd.array(X_inp.toarray()).as_in_context(ctx) if constant_anneal: anneal = anneal_cap else: anneal = min(anneal_cap, update_count / total_anneal_steps) update_count += 1 with autograd.record(): if model.__class__.__name__ == &quot;MultiVAE&quot;: X_out, mu, logvar = model(X_inp) loss = vae_loss_fn(X_inp, X_out, mu, logvar, anneal) train_step.anneal = anneal elif model.__class__.__name__ == &quot;MultiDAE&quot;: X_out = model(X_inp) loss = -nd.mean(nd.sum(nd.log_softmax(X_out) * X_inp, -1)) loss.backward() trainer.step(X_inp.shape[0]) running_loss += loss.asscalar() avg_loss = running_loss / (batch_idx + 1) t.set_postfix(loss=avg_loss) . 4.3 Validation step . def eval_step(data_tr, data_te, data_type=&quot;valid&quot;): running_loss = 0.0 eval_idxlist = list(range(data_tr.shape[0])) eval_N = data_tr.shape[0] eval_steps = len(range(0, eval_N, batch_size)) n100_list, r20_list, r50_list = [], [], [] with trange(eval_steps) as t: for batch_idx, start_idx in zip(t, range(0, eval_N, batch_size)): t.set_description(data_type) end_idx = min(start_idx + batch_size, eval_N) X_tr = data_tr[eval_idxlist[start_idx:end_idx]] X_te = data_te[eval_idxlist[start_idx:end_idx]] X_tr_inp = nd.array(X_tr.toarray()).as_in_context(ctx) with autograd.predict_mode(): if model.__class__.__name__ == &quot;MultiVAE&quot;: X_out, mu, logvar = model(X_tr_inp) loss = vae_loss_fn(X_tr_inp, X_out, mu, logvar, train_step.anneal) elif model.__class__.__name__ == &quot;MultiDAE&quot;: X_out = model(X_tr_inp) loss = -nd.mean(nd.sum(nd.log_softmax(X_out) * X_tr_inp, -1)) running_loss += loss.asscalar() avg_loss = running_loss / (batch_idx + 1) # Exclude examples from training set X_out = X_out.asnumpy() X_out[X_tr.nonzero()] = -np.inf n100 = NDCG_binary_at_k_batch(X_out, X_te, k=100) r20 = Recall_at_k_batch(X_out, X_te, k=20) r50 = Recall_at_k_batch(X_out, X_te, k=50) n100_list.append(n100) r20_list.append(r20) r50_list.append(r50) t.set_postfix(loss=avg_loss) n100_list = np.concatenate(n100_list) r20_list = np.concatenate(r20_list) r50_list = np.concatenate(r50_list) return avg_loss, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list) . I have widely discussed the evaluation metrics (NDCG@k and Recall@k) in a number of notebooks in this repo (and corresponding posts). Therefore, with that in mind and with the aim of not making this a more &quot;infinite notebook&quot;, I will not describe the corresponding implementation here. If you want details on those evaluation metrics, please go the metrics.py module in utils. The code there is a very small adaptation to the one in the original implementation. . 4.4 Running the process . Let&#39;s define the model, prepare the set up and run a small sample (of course, ignore the results printed. I only want to illustrate how to run the model) . model = MultiVAE( p_dims=[200, 600, n_items], q_dims=[n_items, 600, 200], dropout_enc=[0.5, 0.0], dropout_dec=[0.0, 0.0], ) . model . MultiVAE( (encode): VAEEncoder( (q_layers): HybridSequential( (0): Dropout(p = 0.5, axes=()) (1): Dense(20108 -&gt; 600, linear) (2): Dropout(p = 0.0, axes=()) (3): Dense(600 -&gt; 400, linear) ) ) (decode): Decoder( (p_layers): HybridSequential( (0): Dropout(p = 0.0, axes=()) (1): Dense(200 -&gt; 600, linear) (2): Dropout(p = 0.0, axes=()) (3): Dense(600 -&gt; 20108, linear) ) ) ) . ctx = mx.gpu() if mx.context.num_gpus() else mx.cpu() model.initialize(mx.init.Xavier(), ctx=ctx) model.hybridize() optimizer = mx.optimizer.Adam(learning_rate=0.001, wd=0.) trainer = gluon.Trainer(model.collect_params(), optimizer=optimizer) . stop_step = 0 update_count = 0 eval_every = 1 stop = False for epoch in range(1): train_step(model, optimizer, train_data[:2000], epoch) if epoch % eval_every == (eval_every - 1): val_loss, n100, r20, r50 = eval_step(valid_data_tr[:1000], valid_data_te[:1000]) print(&quot;=&quot; * 80) print( &quot;| valid loss {:4.3f} | n100 {:4.3f} | r20 {:4.3f} | &quot; &quot;r50 {:4.3f}&quot;.format(val_loss, n100, r20, r50) ) print(&quot;=&quot; * 80) . epoch: 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04&lt;00:00, 1.25s/it, loss=737] valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01&lt;00:00, 1.12it/s, loss=562] . ================================================================================ | valid loss 561.928 | n100 0.006 | r20 0.003 | r50 0.006 ================================================================================ . . And with a few more rings and bells (e.g. optional learning rate scheduler, early stopping, etc...) this is exactly the code that you will find in main_mxnet.py. . Before I move to the next, final section, just a quick comment about something I normally find in these scientific publications. Normally, once they have found the best hyperparameters on the validation set, they test the model on the test set. . # Run on test data with best model model.load_parameters(str(model_weights / (model_name + &quot;.params&quot;)), ctx=ctx) test_loss, n100, r20, r50 = eval_step( test_data_tr, test_data_te, data_type=&quot;test&quot; ) . In &quot;real-life&quot; scenarios, there would be one additional step, the one merging the train and validation sets, re-training the model with the best hyperparameters and then testing on the test set. In any case, since here my goal is not to build a real-life system, I will follow the same procedure to that found in the original implementation. . Time now to have a look to the results obtained with both Pytorch and Mxnet. . 5. Summary of the results . let me remind again the annealing schedule described in Section 4.1. Basically, we gradually anneal to $ beta = 1$, which is reached at around 80% of the total number of epochs, and record the best anneal parameter ($ beta_{best}$). Then we apply the same annealing schedule but with $ beta_{best}$, i.e. we anneal to $ beta_{best}$ reaching that value at around 80% of the total number of epochs. . #collapse-hide from utils.plot_utils import plot_anneal_schedule, find_best, plot_metric_vs_loss, plot_ndcg_vs_pdims plot_anneal_schedule() . . Figure 2. Same annealing schedule as on Liang et al 2018 for 3 different architectures and for the Movielens and the Amazon datasets using Pytorch and Mxnet. During the annealing schedule, $ beta=1$ is reached at 170 epochs (out of 200, i.e. 85%) . #collapse-hide best_results = pd.concat([ find_best(dl_frame=&#39;pt&#39;, model=&#39;vae&#39;), find_best(dl_frame=&#39;pt&#39;, model=&#39;dae&#39;), find_best(dl_frame=&#39;mx&#39;, model=&#39;vae&#39;), find_best(dl_frame=&#39;mx&#39;, model=&#39;dae&#39;), ]).reset_index(drop=True) best_results.sort_values([&quot;dataset&quot;, &quot;model&quot;]).reset_index(drop=True) . . dataset dl_frame model p_dims weight_decay lr lr_scheduler anneal_cap best_epoch loss n100 r20 r50 . 0 amazon | Pytorch | dae | [50,150] | 0.0 | 0.001 | False | NA | 28 | 87.588 | 0.091 | 0.120 | 0.182 | . 1 amazon | Mxnet | dae | [100,300] | 0.0 | 0.001 | False | NA | 18 | 85.985 | 0.090 | 0.119 | 0.182 | . 2 amazon | Pytorch | vae | [300,900] | 0.0 | 0.001 | False | 0.7 | 170 | 92.263 | 0.101 | 0.137 | 0.204 | . 3 amazon | Mxnet | vae | [200,600] | 0.0 | 0.001 | False | 0 | 8 | 85.310 | 0.090 | 0.118 | 0.179 | . 4 movielens | Pytorch | dae | [200,600] | 0.0 | 0.001 | False | NA | 136 | 349.714 | 0.418 | 0.386 | 0.530 | . 5 movielens | Mxnet | dae | [200, 600] | 0.0 | 0.005 | True | NA | 184 | 348.841 | 0.424 | 0.393 | 0.536 | . 6 movielens | Pytorch | vae | [200, 600] | 0.0 | 0.005 | True | 0.2 | 155 | 365.372 | 0.427 | 0.398 | 0.538 | . 7 movielens | Mxnet | vae | [200,600] | 0.0 | 0.001 | False | 0 | 101 | 350.479 | 0.417 | 0.388 | 0.531 | . Table 1. Best performing experiments (in terms of NDCG@10) among all the experiments I run (which can be found in the run_experiment.sh file in the repo). A csv file with the results for all experiments run can be found in the all_results.csv file in the repo. . Figure 1 reproduces the same annealing schedule for 3 different architectures and for the Movielens and the Amazon datasets using Pytorch and Mxnet. During the annealing schedule, $ beta=1$ is reached at 170 epochs (out of 200, i.e. 85%). In addition, I have also used early stopping with a &quot;patience&quot; of 20 epochs, which is why none of the experiments reaches the 200 epochs. The vertical lines in the figure indicate the epoch at which the best NDGC@100 is reached, and the corresponding $ beta$ value is indicated in the top x-axis. . On the other hand, Table 1 shows the best results I obtained for all the experiments I run, which you can find in this repo in the file run_experiments.sh. . At first sight it is apparent how different the two deep learning frames behave. I find Pytorch to perform a bit better than Mxnet and to be more stable across experiments. This is something that I keep finding every time I use these two frames for the same exercise. For example, here, using Hierarchical Attention networks. I actually believe this is due to the fact that I know (or have used) more Pytorch than Mxnet. Nonetheless, at this stage it is clear for me that I need to do a proper benchmark exercise between these two deep learning libraries. . Focusing on the results shown in Figure 1, the first apparent result is that the Mxnet implementation performs better with little or no regularization. In fact, I have run over 60 experiments and, as shown in Table 1, the best results when using Mult-VAE and Mxnet are obtained with no regularization, i.e. a denoising autoencoder with the reparametrization trick. Furthermore, the best overall metrics with Mxnet are obtained using the Mult-DAE (NDCG@100 = 0.424). . If we also focus in the differences between datasets, it is first apparent that the metrics are significantly smaller for the Amazon dataset relative to those obtained with the Movielens dataset. This was of course expected since as I mentioned in Section 2.1 the Amazon dataset is 13 times more sparse that the Movielens dataset, i.e. significantly more challenging. In addition, we see that the Pytorch implementation shows a very stable behavior for both datasets and architectures, reaching the best NDCG@10 later in the training epochs in the case of the Amazon dataset. Again this is different in the case of the Mxnet implementation, where we see less consistency and the maximum NDCG@10 being reached very early during training for both datasets. . #collapse-hide plot_ndcg_vs_pdims() . . Figure 3 NDCG100 (aka n100) plotted against the first dimension of the decoder for the following architectures: $ small{ i) space I rightarrow 150 rightarrow 50 rightarrow 150 rightarrow I}$, $ small{ ii) space I rightarrow 300 rightarrow 100 rightarrow 300 rightarrow I}$, $ small{ iii) space I rightarrow 600 rightarrow 200 rightarrow 600 rightarrow I}$ and $ small{ iv) space I rightarrow 900 rightarrow 300 rightarrow 900 rightarrow I}$ . On the other hand, Liang et al mentioned in their paper that deeper architectures did not lead to any improvement. This is consistent with the results I found in my experiments. In fact, Figure 3 shows the NDCG100 (refereed in the figure as n100) vs the first dimension of the decoder for four different architectures. As we can see in the figure, even concentrating in architectures with the same number of layers, adding neurons per layer was not particularly helpful beyond a certain number (50 and 200 for the Movilens and the Amazon dataset respectively). . #collapse-hide plot_metric_vs_loss() . . Figure 4. NDGC@100 (n100), and Recall@20 (r20) and Recall@50 (r50) plotted against the loss for all experiments I run . Before I end up this exercise I wanted to emphasize a result I have already discussed in the past (see here) and that is illustrated in Fig 4. . Fig 2 shows that, in general, the best ranking metrics do not correspond to the best loss values. Even though the the reconstruction of the input matrix of clicks might be worse, the ranking metrics might still improve. This is an important and not uncommon results, and something one has to bear in mind when building real world recommendation systems. When building recommendation algorithms we are not interested in achieving the best classification/regression loss, but in producing the best recommendations, which is more related to information retrieval effectiveness, and therefore ranking metrics. For more information on this and many other aspects of recommendation systems, I recommend this fantastic book. Chapter 7 in that book focuses on evaluation metrics. . And with this, I conclude my experimentation around the Mult-VAE with Pytorch and Mxnet . The next, most immediate projects I want to add to the repo are: . Sequential Variational Autoencoders for Collaborative Filtering [7] | LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation [8] | But before that, I will be re-visiting the [nlp-stuff repo] and also updating the pytorch-widedeep package. . If you manage to read all that, I hope you found it useful. . References: . [1] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara, 2018. Variational Autoencoders for Collaborative Filtering: arXiv:1802.05814v1 . [2] Diederik P Kingma, Max Welling, 2014. Auto-Encoding Variational Bayes: arXiv:1312.6114v10 . [3] Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach. Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches: arXiv:1907.06902v3 . [4] J. McAuley, C. Targett, J. Shi, A. van den Hengel. 2015. Image-based recommendations on styles and substitutes. arXiv:1506.04757v1 . [5] R. He, J. McAuley, 2016. Modeling the visual evolution of fashion trends with one-class collaborative filtering. arXiv:1602.01585v1 . [6] Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio, 2016. Generating Sentences from a Continuous Space: arXiv:1511.06349v4 . [7] Noveen Sachdeva, Giuseppe Manco, Ettore Ritacco, Vikram Pudi, 2018. Sequential Variational Autoencoders for Collaborative Filtering: arXiv:1811.09975v1 . [8] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation arXiv:2002.02126v2 .",
            "url": "https://jrzaurin.github.io/infinitoml/2020/05/15/mult-vae.html",
            "relUrl": "/2020/05/15/mult-vae.html",
            "date": " ‚Ä¢ May 15, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Coming soon. Don‚Äôt expect much though‚Ä¶ .",
          "url": "https://jrzaurin.github.io/infinitoml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page8": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://jrzaurin.github.io/infinitoml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}